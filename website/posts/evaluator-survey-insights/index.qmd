---
title: "Inside The Unjournal: What Evaluators Tell Us"
author: "David Reinstein with Claude Code assistance"
date: "2025-11-12"
execute:
  echo: false
  code-fold: true
  code-summary: "Show code"
  freeze: auto
  warning: false
  message: false
editor_options:
  chunk_output_type: console
categories: [evaluators, process, survey]
description: "Analyzing 100+ evaluator survey responses about time spent, experience levels, and feedback on The Unjournal process."
---

```{r}
#| label: setup
#| include: false

library(dplyr)
library(ggplot2)
library(readr)
library(tidyr)
library(stringr)
library(plotly)
library(here)
library(DT)
library(janitor)

# Load evaluator survey data (excludes confidential comments, COI info, evaluator codes)
# Combines both academic and applied stream evaluations
survey <- read_csv(here("data/evaluator_survey_responses.csv"),
                   show_col_types = FALSE) %>%
  clean_names()
```

## Overview

We've collected feedback from evaluators through survey questions in our evaluation forms. With over 100 completed evaluations from both academic and applied streams, we can now share insights about time commitments, evaluator experience, and process feedback.

## Time Commitment

```{r}
#| label: time-stats

hours_data <- survey %>%
  filter(!is.na(hours_spent_manual_impute))

time_stats <- hours_data %>%
  summarise(
    Mean = round(mean(hours_spent_manual_impute), 1),
    Median = median(hours_spent_manual_impute),
    Min = min(hours_spent_manual_impute),
    Max = max(hours_spent_manual_impute),
    N = n()
  )
```

Evaluators spend an average of **`r time_stats$Mean` hours** (median: `r time_stats$Median` hours) on an evaluation, ranging from `r time_stats$Min` to `r time_stats$Max` hours. This reflects differences in paper complexity and evaluator approaches.

```{r}
#| label: time-viz
#| fig-cap: "Distribution of evaluation time"
#| fig-height: 4

p <- ggplot(hours_data, aes(x = hours_spent_manual_impute)) +
  geom_histogram(bins = 15, fill = "#2E86AB", alpha = 0.7, color = "white") +
  geom_vline(xintercept = time_stats$Mean,
             color = "#A23B72", linetype = "dashed", linewidth = 1) +
  geom_vline(xintercept = time_stats$Median,
             color = "#F18F01", linetype = "dashed", linewidth = 1) +
  labs(
    title = "Hours Spent on Evaluations",
    subtitle = paste0("Mean: ", time_stats$Mean, " hrs (purple) | Median: ",
                     time_stats$Median, " hrs (orange) | N=", time_stats$N),
    x = "Hours", y = "Count"
  ) +
  theme_minimal()

ggplotly(p)
```

## Evaluator Experience

```{r}
#| label: experience-parsing

# Parse years in field from text responses
years_data <- survey %>%
  filter(!is.na(how_long_have_you_been_in_this_field) &
         how_long_have_you_been_in_this_field != "") %>%
  mutate(
    # Extract first number from text
    years_numeric = str_extract(how_long_have_you_been_in_this_field, "\\d+"),
    years_numeric = as.numeric(years_numeric)
  ) %>%
  filter(!is.na(years_numeric))

# Parse number of papers reviewed
reviews_data <- survey %>%
  filter(!is.na(how_many_proposals_papers_and_projects_have_you_evaluated_reviewed_for_journals_grants_or_other_peer_review) &
         how_many_proposals_papers_and_projects_have_you_evaluated_reviewed_for_journals_grants_or_other_peer_review != "") %>%
  mutate(
    # Extract numbers, handle "100+", "25+", ">100" etc
    reviews_text = how_many_proposals_papers_and_projects_have_you_evaluated_reviewed_for_journals_grants_or_other_peer_review,
    # Clean and extract number
    reviews_numeric = str_extract(reviews_text, "\\d+"),
    reviews_numeric = as.numeric(reviews_numeric)
  ) %>%
  filter(!is.na(reviews_numeric))

# Calculate statistics
exp_stats <- list(
  years_n = nrow(years_data),
  years_median = median(years_data$years_numeric),
  years_mean = round(mean(years_data$years_numeric), 1),
  years_10plus = sum(years_data$years_numeric >= 10),
  reviews_n = nrow(reviews_data),
  reviews_median = median(reviews_data$reviews_numeric),
  reviews_mean = round(mean(reviews_data$reviews_numeric), 1),
  reviews_100plus = sum(reviews_data$reviews_numeric >= 100)
)
```

Our evaluators are highly experienced. From **`r exp_stats$years_n` respondents** who reported years in field, the median is **`r exp_stats$years_median` years** (mean: `r exp_stats$years_mean` years), with **`r exp_stats$years_10plus` (`r round(100*exp_stats$years_10plus/exp_stats$years_n)`%)** reporting 10+ years.

For review experience, **`r exp_stats$reviews_n` respondents** provided counts, with a median of **`r exp_stats$reviews_median` papers reviewed** (mean: `r exp_stats$reviews_mean`). **`r exp_stats$reviews_100plus` evaluators (`r round(100*exp_stats$reviews_100plus/exp_stats$reviews_n)`%)** have reviewed 100+ papers.

```{r}
#| label: exp-viz
#| fig-cap: "Evaluator experience distribution"
#| fig-height: 4

p1 <- ggplot(years_data, aes(x = years_numeric)) +
  geom_histogram(bins = 10, fill = "#2E86AB", alpha = 0.7, color = "white") +
  geom_vline(xintercept = exp_stats$years_median,
             color = "#F18F01", linetype = "dashed", linewidth = 1) +
  labs(
    title = "Years in Field",
    subtitle = paste0("Median: ", exp_stats$years_median, " years | N=", exp_stats$years_n),
    x = "Years", y = "Count"
  ) +
  theme_minimal()

ggplotly(p1)
```

```{r}
#| label: exp-viz2
#| fig-cap: "Papers reviewed (log scale)"
#| fig-height: 4

p2 <- ggplot(reviews_data, aes(x = reviews_numeric)) +
  geom_histogram(bins = 12, fill = "#A23B72", alpha = 0.7, color = "white") +
  geom_vline(xintercept = exp_stats$reviews_median,
             color = "#F18F01", linetype = "dashed", linewidth = 1) +
  scale_x_continuous(trans = "log10", breaks = c(10, 25, 50, 100, 200)) +
  labs(
    title = "Papers Reviewed",
    subtitle = paste0("Median: ", exp_stats$reviews_median, " papers | N=", exp_stats$reviews_n),
    x = "Number of Papers", y = "Count"
  ) +
  theme_minimal()

ggplotly(p2)
```

::: {.callout-note collapse="true"}
## Sample text responses

```{r}
#| label: exp-samples

field_sample <- survey %>%
  filter(!is.na(how_long_have_you_been_in_this_field) &
         how_long_have_you_been_in_this_field != "") %>%
  select(how_long_have_you_been_in_this_field) %>%
  slice_head(n = 5)

review_sample <- survey %>%
  filter(!is.na(how_many_proposals_papers_and_projects_have_you_evaluated_reviewed_for_journals_grants_or_other_peer_review) &
         how_many_proposals_papers_and_projects_have_you_evaluated_reviewed_for_journals_grants_or_other_peer_review != "") %>%
  select(how_many_proposals_papers_and_projects_have_you_evaluated_reviewed_for_journals_grants_or_other_peer_review) %>%
  slice_head(n = 5)

knitr::kable(
  data.frame(
    `Years in Field` = field_sample[[1]],
    `Papers Reviewed` = review_sample[[1]]
  ),
  caption = "Sample evaluator experience"
)
```
:::

## Process Feedback

```{r}
#| label: process-stats

process_ratings <- survey %>%
  filter(!is.na(how_would_you_rate_this_template_and_process) &
         how_would_you_rate_this_template_and_process != "")

n_ratings <- nrow(process_ratings)
```

**`r n_ratings` evaluators** provided feedback on our template and process.

**Highlights of positive feedback:**

- "two enthusiastic thumbs up" • "9.5/10"
- "Process was clear, template was helpful"
- "Very thorough evaluation... enjoyed providing point estimates alongside text-based review"

**Key areas for improvement:**

1. **Questionnaire length**: "Too many questions," "annoying after spending time on referee report"
2. **Usability**: "Sliders are confusing/wonky," "metrics with intervals is confusing"
3. **Clarity**: "Would be good to have worked examples," "process not intuitive"

::: {.callout-note collapse="true"}
## View all process feedback

```{r}
#| label: all-feedback

datatable(
  process_ratings %>% select(how_would_you_rate_this_template_and_process),
  colnames = "Feedback",
  options = list(pageLength = 5, scrollX = TRUE)
)
```
:::

## Willingness to Re-evaluate

```{r}
#| label: reevaluate-stats

reevaluate <- survey %>%
  filter(!is.na(would_you_be_willing_to_consider_evaluating_a_revised_version_of_this_work) &
         would_you_be_willing_to_consider_evaluating_a_revised_version_of_this_work != "")

yes_count <- sum(str_detect(
  str_to_lower(reevaluate$would_you_be_willing_to_consider_evaluating_a_revised_version_of_this_work),
  "yes"
))

pct_yes <- round(100 * yes_count / nrow(reevaluate))
```

Out of **`r nrow(reevaluate)` responses**, **`r yes_count` included "yes" (~`r pct_yes`%)** when asked about evaluating revised versions. This high engagement suggests evaluators find the process worthwhile despite friction points.

## Fields Represented

```{r}
#| label: fields

expertise <- survey %>%
  filter(!is.na(field_expertise) & field_expertise != "") %>%
  pull(field_expertise)

n_fields <- length(expertise)
```

Our **`r n_fields` evaluators** come from diverse backgrounds including development economics, experimental economics, environmental economics, political science, AI safety, meta-analysis, and more.

::: {.callout-note collapse="true"}
## View all fields of expertise

```{r}
#| label: all-fields

knitr::kable(
  data.frame(`Field/Expertise` = expertise),
  caption = paste0(n_fields, " evaluator fields")
)
```
:::

## What We're Learning

1. **Time commitment is substantial but manageable**: Median of 8 hours for high-quality peer review
2. **Evaluators are highly qualified**: 10+ years experience, 100+ prior reviews
3. **Process needs refinement**: Appreciate thoroughness but find sliders/questions cumbersome
4. **Strong engagement**: High re-evaluation willingness indicates value despite friction

## Next Steps

We're working on:

- Streamlining the evaluation template
- Creating guides with worked examples
- Improving rating interface usability
- Reducing redundancy between written reviews and structured metrics

## Conclusion

Analysis of **`r nrow(survey)` evaluations** reveals The Unjournal successfully engages experienced researchers in substantive reviews averaging ~10 hours. While room for improvement exists—particularly around usability—evaluators remain willing to participate, suggesting we provide value justifying the time investment.

---

::: {.callout-tip}
## Data & Privacy

Analysis code and data (excluding confidential information) available in our [GitHub repository](https://github.com/unjournal/unjournaldata).

**Privacy**: This dataset excludes confidential comments, COI information, and evaluator pseudonyms.
:::
