---
title: "Unjournal data update: Deeper analysis with more evaluations"
author: "Orchestrated by David Reinstein with Claude Code assistance*"
date: "2025-10-15"
execute:
  echo: false
  freeze: auto
  warning: false
  message: false
editor_options:
  chunk_output_type: console
editor:
  markdown:
    wrap: 72
categories: [data, analysis]
---

*Inspired by earlier work from David Hugh-Jones

```{r}
#| label: setup
#| include: false

library(conflicted)
conflicts_prefer(dplyr::select, dplyr::filter, .quiet = TRUE)
suppressPackageStartupMessages({
  library(here)
  library(readr)
  library(dplyr)
  library(huxtable)
  library(irr)
  library(purrr)
  library(glue)
  library(ggplot2)
  library(plotly)
  library(tidyr)
  library(corrplot)
  library(DT)
})
options(huxtable.long_minus = TRUE)

research <- readr::read_csv(here("data/research.csv"), show_col_types = FALSE)
ratings <- readr::read_csv(here("data/rsx_evalr_rating.csv"),
                           show_col_types = FALSE)
paper_authors <- readr::read_csv(here("data/paper_authors.csv"),
                                 show_col_types = FALSE)

n_papers <- n_distinct(ratings$research)
n_evals <- n_distinct(ratings$research, ratings$evaluator)

qual_dimensions <- c("overall", "adv_knowledge", "methods", "logic_comms",
                     "real_world", "gp_relevance", "open_sci")
journal_dimensions <- c("journal_predict", "merits_journal")
all_dimensions <- c(qual_dimensions, journal_dimensions)

dupe_ratings <- (ratings |>
  count(research, criteria, evaluator) |>
  pull(n)) > 1L
if (any(dupe_ratings)) warning("Duplicate rows found in ratings data.")

# This is temporary and should be removed once the duplicate rows are fixed.
ratings <- ratings |>
  distinct(.keep_all = TRUE,
    research, evaluator, criteria
  )

# Again, this is a temporary hack until our data is solid.
ratings <- ratings |>
  tidyr::drop_na(research, evaluator, criteria)

```

This post updates our [earlier analysis](../uj-data-first-look/) of
[The Unjournal's](https://www.unjournal.org) evaluation data. Our
dataset has grown substantially -- from a few dozen evaluations in July
2024 to `r n_papers` papers and `r n_evals` evaluations today. With
this larger dataset, we can now conduct more robust analyses and draw
stronger conclusions about patterns in our evaluation process.

This update expands on the previous analysis in several ways:

-   **Interactive visualizations**: Plotly-based charts with hover tooltips to identify papers, filter data, and reorder axes
-   **Expanded PCA analysis**: We now examine how much variance each
    principal component explains and provide deeper interpretation of
    what each component captures
-   **Enhanced IRR analysis**: More detailed examination of
    Krippendorff's alpha values (interval, ratio, and ordinal), plus expanded analysis of credible interval overlap
-   **Journal tier analysis**: New section examining evaluators'
    predictions about journal placement, including statistical tests
-   **Research area breakdowns**: Analysis by cause/research area for key metrics
-   **Correlation matrices**: Within-rater correlations to examine redundancy
-   **Criteria redundancy**: Which evaluation dimensions are most closely related

::: callout-warning
## Note on this analysis

This blog post is preliminary and was created with assistance from Claude Code
(Anthropic's AI assistant) under the orchestration of David Reinstein,
building on inspirational earlier work by David Hugh-Jones. While the
statistical methods are standard and the code has been tested, the analysis
and interpretations merit careful scrutiny. We welcome feedback, corrections,
and suggestions for improvement. Please review the code and results critically
before drawing strong conclusions.
:::

## About the data

Papers[^uj-data-update-2025-1] can be suggested for evaluation either by
Unjournal insiders, or by outsiders. The Unjournal then selects some
papers for evaluation.

[^uj-data-update-2025-1]: Actually we don't just evaluate academic
    papers, but I'll use "papers" for short.

Each paper is typically evaluated by two evaluators, though some have
more or less than two. Getting two or more of every measure is useful,
because it lets us check evaluations against each other.

We ask evaluators two kinds of quantitative questions. First, there are
different measures of *paper quality*. Here they are, along with some
snippets from our [guidelines for
evaluators](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#quantitative-metrics):

-   *Overall assessment*: "Judge the quality of the research
    heuristically. Consider all aspects of quality, credibility,
    importance to knowledge production, and importance to practice."
-   *Advancing our knowledge and practice*: "To what extent does the
    project contribute to the field or to practice, particularly in ways
    that are relevant to global priorities and impactful
    interventions?..."
-   *Methods: Justification, reasonableness, validity, robustness*: "Are
    the methods used well-justified and explained; are they a reasonable
    approach to answering the question(s) in this context? Are the
    underlying assumptions reasonable? Are the results and methods
    likely to be robust to reasonable changes in the underlying
    assumptions?..."
-   *Logic and communication*: "Are the goals and questions of the paper
    clearly expressed? Are concepts clearly defined and referenced? Is
    the reasoning 'transparent'? Are assumptions made explicit? Are all
    logical steps clear and correct? Does the writing make the argument
    easy to follow?"
-   *Open, collaborative, replicable science*: "This covers several
    considerations: Replicability, reproducibility, data integrity...
    Consistency... Useful building blocks: Do the authors provide tools,
    resources, data, and outputs that might enable or enhance future
    work and meta-analysis?"
-   *Real-world relevance*: "Are the assumptions and setup realistic and
    relevant to the real world?"
-   *Relevance to global priorities*: "Could the paper's topic and
    approach potentially help inform [global priorities, cause
    prioritization, and high-impact
    interventions](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/the-field-and-ea-gp-research)?

Each of these questions is meant to be a percentile scale, 0-100%, where
the percentage captures the paper's place in the distribution of the
reference group ("all serious research in the same area that you have
encountered in the last three years").[^uj-data-update-2025-2] So, for
example, a score of 70% would mean the paper is better than 70% of
papers in the reference group.

[^uj-data-update-2025-2]: This 'reference group percentile'
    interpretation was introduced around the end of 2023; before this
    evaluators were given a different description of the ratings
    interval.

As well as asking for each question (the midpoint or median of the
evaluator's belief distribution), we also ask for lower and upper bounds
of a 90% credible interval. Our [guidelines](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#how-do-i-come-up-with-these-intervals-discussion-and-guidance)
note that evaluators should aim to estimate the **true percentile** for the paper,
accounting for both their own uncertainty and the variation they expect
among informed experts.

Next, we ask two practical questions about publication:

-   "What journal ranking tier *should* this work be published in?"

-   "What journal ranking tier *will* this work be published in?"

Tiers are measured from 0 ("won't publish/little to no value") up to 5
("top journal"). Again, we ask for both an estimate and a 90% credible
interval. We allow non-integer scores between 0 and 5.

The last question is especially interesting, because unlike all the
others, it has an observable ground truth. Eventually, papers do or do
not get published in specific journals, and there is often a consensus
about which journals count as e.g. "top".

## Sanity checks

```{r}
#| label: sanity-checks

# We suppress warnings about "no non-missing arguments to min/max":
suppressWarnings({
  problem_ratings <- ratings |>
    filter(criteria %in% all_dimensions) |>
    summarize(.by = c(research, evaluator),
      all_same     = min(middle_rating, na.rm = TRUE) == max(middle_rating,
                                                             na.rm = TRUE),
      all_same_lo  = min(lower_CI, na.rm = TRUE) == max(lower_CI, na.rm = TRUE),
      all_same_hi  = min(upper_CI, na.rm = TRUE) == max(upper_CI, na.rm = TRUE),
      degen_ci     = sum(criteria %in% qual_dimensions &
                         lower_CI == upper_CI,
                         na.rm = TRUE),
      uninf_ci     = sum(criteria %in% qual_dimensions &
                         upper_CI - lower_CI >= 100,
                         na.rm = TRUE),
      bad_ci       = sum(
                         lower_CI > upper_CI |
                         lower_CI > middle_rating |
                         upper_CI < middle_rating,
                         na.rm = TRUE)
    )
})

n_straightliners    <- sum(problem_ratings$all_same)
n_straightliners_lo <- sum(problem_ratings$all_same_lo)
n_straightliners_hi <- sum(problem_ratings$all_same_hi)

n_degenerate    <- sum(problem_ratings$degen_ci)
n_uninformative <- sum(problem_ratings$uninf_ci)
n_misspecified  <- sum(problem_ratings$bad_ci)
```

Straightliners are evaluators who give the same score for every
question. For the midpoints, we have `r n_straightliners`
straightliners out of `r n_evals` evaluations. We also check if people
straightline lower bounds of the credible intervals
(`r n_straightliners_lo` straightliners) and upper bounds
(`r n_straightliners_hi` straightliners).

Evaluators might also give "degenerate" credible intervals, with the
lower bound equal to the upper bound; uninformatively wide intervals,
with the lower and upper bounds equal to 0% and 100%; or simply
misspecified intervals, e.g. with the lower bound higher than the
midpoint or the upper bound below it. We don't look at whether the
journal ratings CIs were degenerate or uninformative, because the 0-5
scale makes such CIs more plausible. Out of `r nrow(ratings)`
confidence intervals, `r n_degenerate` were degenerate,
`r n_uninformative` were uninformative and `r n_misspecified` were
misspecified.

Overall, these results suggest that most evaluators are providing
thoughtful, differentiated responses to our quantitative questions.

## Inter-rater reliability

We have no ground truth of whether a given paper scores high or low on
our 7 dimensions. But because we usually have multiple evaluations per
paper, we can take an indirect route. If two evaluators' scores are
correlated with reality, they will also correlate with each other. The
converse does not necessarily hold: evaluators' scores might be
correlated because they both have similar prejudices or both
misinterpret the paper in the same way. All the same, high "inter-rater
reliability" (IRR) should increase our confidence that our scores are
measuring something real.

IRR is complex. The basic form of most IRR statistics is

$$
\frac{p_a - p_e}{1 - p_e}
$$

where $p_a$ is the proportion of the time that two raters agree, and
$p_e$ is the amount of agreement you'd expect by chance if both raters
are choosing independently.

Why not use $p_a$ directly? Well, for example, suppose our raters pick
an expected journal tier at random, from 0 to 5 inclusive. Clearly
there's no reliability: the data is just random noise. But one time in
six, both raters will agree, simply by chance. So we need to adjust for
the expected amount of agreement. To do this most measures use the
marginal distributions of the ratings: in our example, a 1 in 6 chance
of each number from 0 to 5, giving $p_e = 1/6$. Krippendorff's alpha is
a widely accepted statistic that corrects for $p_e$ and also defines
"agreement" appropriately for different levels of measurement.

::: callout-note
## Choosing a reliability statistic

There are many ways to measure inter-rater reliability. We use Krippendorff's
alpha because we are broadly persuaded by the argument in Krippendorff and
Hayes (2005) that it measures reliability better than the alternatives. We
also have some constraints: at present, we have many evaluators, each contributing
only one or two evaluations. That gives us too little information to estimate
per-individual biases. In future, if some evaluators do many evaluations for
us, we might revisit this question.

Krippendorff's alpha can be calculated for different measurement levels:
**interval**, **ratio**, and **ordinal**. For our percentile ratings (0-100%),
we focus primarily on the **interval** scale, which treats equal-sized
differences the same throughout the scale. We also report ratio (which
has a meaningful zero) and ordinal (which only considers rank order) alphas
for comparison.

Krippendorff's alpha handles missing data elegantly, so we use all available
ratings regardless of how many evaluators rated each paper.
:::

```{r}
#| label: tbl-kripp-alpha
#| tbl-cap: !expr "paste0('Krippendorff\\'s alpha for each dimension (', n_papers, ' papers, ', n_evals, ' evaluations). Interval scale (primary), ratio scale, and ordinal scale shown.')"

kr_alphas_interval <- list()
kr_alphas_ratio <- list()
kr_alphas_ordinal <- list()

for (d in all_dimensions) {
  # This creates a tibble where rows are pieces of research
  # and columns are different evaluators' ratings.
  r_wide <- ratings |>
    filter(criteria == d) |>
    select(research, evaluator, middle_rating) |>
    filter(! is.na(middle_rating)) |>
    mutate(.by = research,
      # Number evaluators within each research paper
      eval_num = paste0("eval_", seq_len(n()))
    ) |>
    tidyr::pivot_wider(id_cols = research, names_from = eval_num,
                       values_from = middle_rating) |>
    select(-research)

  # We convert this into a matrix where *rows* are evaluators and
  # *columns* are pieces of research. Missing values are preserved as NA.
  r_matrix <- as.matrix(r_wide)
  r_matrix <- t(r_matrix)

  kr_alphas_interval[[d]] <- irr::kripp.alpha(r_matrix, method = "interval")
  kr_alphas_ratio[[d]] <- irr::kripp.alpha(r_matrix, method = "ratio")
  kr_alphas_ordinal[[d]] <- irr::kripp.alpha(r_matrix, method = "ordinal")
}

# Extract values for each method
kr_alpha_values_interval <- purrr::list_transpose(kr_alphas_interval)$value
kr_alpha_values_ratio <- purrr::list_transpose(kr_alphas_ratio)$value
kr_alpha_values_ordinal <- purrr::list_transpose(kr_alphas_ordinal)$value
kr_alpha_n <- purrr::list_transpose(kr_alphas_interval)$subjects

# Create ordered dimension labels - overall and journal tiers first, then by interval alpha
alpha_df <- data.frame(
  Dimension = all_dimensions,
  Interval = unlist(kr_alpha_values_interval),
  Ratio = unlist(kr_alpha_values_ratio),
  Ordinal = unlist(kr_alpha_values_ordinal),
  N = unlist(kr_alpha_n)
)

# Custom ordering: overall first, then journal dimensions, then rest by interval alpha
alpha_df <- alpha_df |>
  mutate(
    order_priority = case_when(
      Dimension == "overall" ~ 1,
      Dimension == "merits_journal" ~ 2,
      Dimension == "journal_predict" ~ 3,
      TRUE ~ 4
    )
  ) |>
  arrange(order_priority, desc(Interval)) |>
  select(-order_priority)

huxtable(alpha_df) |>
  set_number_format(-1, 2:4, "%.3f") |>
  set_bold(1, everywhere, TRUE)

```

The table shows inter-rater reliability for each dimension, with three different
measurement assumptions. **Interval alpha** (our primary measure) treats
equal-sized differences as equivalent throughout the scale. Ratio alpha
additionally assumes a meaningful zero point, while ordinal alpha only
considers whether one rating is higher than another.

Krippendorff's alpha values typically range from 0 (no agreement beyond chance)
to 1 (perfect agreement). Values above 0.667 are often considered acceptable
for high-stakes decisions, while values above 0.8 indicate strong reliability.

Our interval alpha values suggest moderate agreement across dimensions. The
quality dimensions (overall assessment, methods, etc.) generally show reliability
in the 0.4-0.6 range, which is typical for subjective expert judgments.
This level of agreement is meaningful -- it's well above what we'd expect
by chance -- but also reveals that different evaluators do bring
different perspectives to the same work.

The journal tier questions show `r ifelse(alpha_df$Interval[alpha_df$Dimension == "journal_predict"] < 0.5, "lower", "comparable")`
reliability to the quality dimensions. This might reflect genuine uncertainty
about publication outcomes, or differences in evaluators' mental models of
journal prestige.

### Credible interval overlap

Because we have each rater's 90% credible interval, we can also ask a
slightly different question: do raters' credible intervals capture
other evaluators' best estimates? That is, is rater 1's midpoint estimate
within rater 2's credible interval, and vice versa?

According to our [guidelines](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#how-do-i-come-up-with-these-intervals-discussion-and-guidance),
evaluators should construct intervals that capture their uncertainty about
the **true percentile** for the paper, not their uncertainty about what
another evaluator might say. If evaluators are well-calibrated and the
"true percentile" exists, we'd still expect substantial disagreement
because evaluators have access to different information and expertise.

```{r}
#| label: tbl-credible-intervals
#| tbl-cap: !expr "paste0('Credible interval overlap: Proportions of midpoints within other evaluators\\' 90% credible intervals (', n_papers, ' papers, ', n_evals, ' evaluations)')"

# This gives a dataset with one row per paper per dimension, and all
# ratings and CIs of all evaluators in a single row.
ratings_dims <- ratings |>
  mutate(.by = research,
    # This simply numbers the evaluators 1, 2, ... for each piece of research:
    eval_num  = as.numeric(factor(evaluator)),
    evaluator = NULL
  ) |>
  tidyr::pivot_wider(id_cols = c(research, criteria), names_from = eval_num,
                     values_from = c(middle_rating, lower_CI, upper_CI,
                                    confidence_level))

# This assumes we have no more than 3 evaluators per paper.
ratings_coverage <- ratings_dims |>
  mutate(
    mp1ci2 = between(middle_rating_1, lower_CI_2, upper_CI_2),
    mp1ci3 = between(middle_rating_1, lower_CI_3, upper_CI_3),
    mp2ci1 = between(middle_rating_2, lower_CI_1, upper_CI_1),
    mp2ci3 = between(middle_rating_2, lower_CI_3, upper_CI_3),
    mp3ci1 = between(middle_rating_3, lower_CI_1, upper_CI_1),
    mp3ci2 = between(middle_rating_3, lower_CI_2, upper_CI_2)
  ) |>
  summarize(.by = criteria,
    `Proportion within C.I.` = mean(c_across(matches("mp.ci")), na.rm = TRUE),
    `N comparisons` = sum(!is.na(c_across(matches("mp.ci"))))
  )

# Reorder to match our table ordering
ratings_coverage <- ratings_coverage |>
  mutate(
    order_priority = case_when(
      criteria == "overall" ~ 1,
      criteria == "merits_journal" ~ 2,
      criteria == "journal_predict" ~ 3,
      TRUE ~ 4
    )
  ) |>
  left_join(
    alpha_df |> select(Dimension, Interval) |> rename(criteria = Dimension, alpha_for_order = Interval),
    by = "criteria"
  ) |>
  arrange(order_priority, desc(alpha_for_order)) |>
  select(-order_priority, -alpha_for_order)

ratings_coverage |>
  rename(Dimension = criteria) |>
  as_huxtable() |>
  set_number_format(-1, 2, fmt_percent()) |>
  set_bold(1, everywhere, TRUE)
```

The table above reveals an important pattern: typically only about half
of our evaluators' midpoints fall within their co-evaluator's 90%
credible interval. It's important to interpret this finding carefully
in light of our guidelines.

Evaluators are asked to estimate the **true underlying percentile** for
each paper, not to predict what other evaluators will say. Given that:

1. Different evaluators have different expertise and perspectives
2. Some papers may genuinely be at different percentiles for different subfields
3. The "true percentile" is itself a somewhat fuzzy concept

...we should expect some degree of between-evaluator disagreement even
with perfect calibration. The observed overlap rates of 40-60% suggest
that evaluators' intervals do capture meaningful uncertainty, though
they may still be somewhat narrower than ideal for capturing all
reasonable expert opinion.

This pattern is fairly consistent across dimensions, though there is
some variation. The relationship between Krippendorff's alpha (measuring
between-evaluator agreement) and interval overlap provides useful
information about both the consistency of our measures and the
appropriate level of uncertainty to express.

### Analysis by research area

Let's examine how agreement varies across different cause areas and research categories:

```{r}
#| label: area-analysis

# Get research areas from the research data
ratings_with_area <- ratings |>
  left_join(
    research |> select(label_paper_title, main_cause_cat_abbrev),
    by = c("research" = "label_paper_title")
  )

# Calculate summary statistics by area for key dimensions
area_summaries <- ratings_with_area |>
  filter(criteria %in% c("overall", "merits_journal")) |>
  filter(!is.na(main_cause_cat_abbrev)) |>
  summarize(.by = c(main_cause_cat_abbrev, criteria),
    Mean = mean(middle_rating, na.rm = TRUE),
    SD = sd(middle_rating, na.rm = TRUE),
    N_ratings = sum(!is.na(middle_rating))
  ) |>
  filter(N_ratings >= 5) # Only show areas with at least 5 ratings

# Calculate Krippendorff's alpha by area for "overall"
area_alphas <- list()
areas_with_data <- unique(ratings_with_area$main_cause_cat_abbrev)
areas_with_data <- areas_with_data[!is.na(areas_with_data)]

for (area in areas_with_data) {
  r_wide <- ratings_with_area |>
    filter(criteria == "overall", main_cause_cat_abbrev == area) |>
    select(research, evaluator, middle_rating) |>
    filter(! is.na(middle_rating)) |>
    mutate(.by = research,
      eval_num = paste0("eval_", seq_len(n()))
    ) |>
    tidyr::pivot_wider(id_cols = research, names_from = eval_num,
                       values_from = middle_rating) |>
    select(-research)

  if (ncol(r_wide) >= 2 && nrow(r_wide) >= 3) {
    r_matrix <- as.matrix(r_wide)
    r_matrix <- t(r_matrix)
    tryCatch({
      area_alphas[[area]] <- irr::kripp.alpha(r_matrix, method = "interval")
    }, error = function(e) {
      area_alphas[[area]] <- list(value = NA, subjects = nrow(r_wide))
    })
  }
}

area_alpha_df <- data.frame(
  Area = names(area_alphas),
  Alpha = sapply(area_alphas, function(x) x$value),
  N_papers = sapply(area_alphas, function(x) x$subjects)
) |>
  arrange(desc(Alpha))

```

```{r}
#| label: tbl-area-summary
#| tbl-cap: "Mean ratings and variability by research area (areas with 5+ ratings)"

area_summaries |>
  mutate(
    criteria = ifelse(criteria == "overall", "Overall", "Should be published")
  ) |>
  tidyr::pivot_wider(
    id_cols = main_cause_cat_abbrev,
    names_from = criteria,
    values_from = c(Mean, SD, N_ratings)
  ) |>
  rename(Area = main_cause_cat_abbrev) |>
  as_huxtable() |>
  set_number_format(everywhere, -1, "%.2f") |>
  set_bold(1, everywhere, TRUE)

```

```{r}
#| label: tbl-area-alpha
#| tbl-cap: "Krippendorff's alpha (interval) for 'Overall' rating by research area"

if (nrow(area_alpha_df) > 0) {
  area_alpha_df |>
    filter(!is.na(Alpha)) |>
    as_huxtable() |>
    set_number_format(-1, 2, "%.3f") |>
    set_bold(1, everywhere, TRUE)
}

```

These tables show how ratings and agreement vary across different research
areas in our dataset. Some areas show higher average ratings, while others
show more or less evaluator agreement. Sample sizes vary considerably, so
these estimates should be interpreted cautiously for areas with few papers.

## Relatedness and dimensionality

We have 7 questions measuring paper quality, and 2 questions about
journal tier. A natural question is: how related are these different
measures? Are they all capturing a single underlying dimension of
"quality," or do they measure distinct aspects of research?

We can explore this using principal components analysis (PCA) of the 7
quality dimensions. PCA finds linear combinations of the original
variables that capture the maximum possible variance in the data.

```{r}
#| label: pca-setup

mean_ratings <- ratings |>
  summarize(.by = c(research, criteria),
    mean_rating = mean(middle_rating, na.rm = TRUE)
  )

mean_ratings_wide <- mean_ratings |>
  tidyr::pivot_wider(id_cols = research, names_from = criteria,
                     values_from = mean_rating)

# PCA on quality dimensions only
pc_qual <- princomp(na.omit(mean_ratings_wide[qual_dimensions]))

# Calculate variance explained
var_explained_qual <- pc_qual$sdev^2 / sum(pc_qual$sdev^2)
cumvar_qual <- cumsum(var_explained_qual)

# PCA on all dimensions
pc_all <- princomp(na.omit(mean_ratings_wide[all_dimensions]))
var_explained_all <- pc_all$sdev^2 / sum(pc_all$sdev^2)
cumvar_all <- cumsum(var_explained_all)
```

### Variance explained by principal components

First, let's look at how much of the total variation in ratings is
explained by each principal component:

```{r}
#| label: fig-scree
#| fig-cap: "Variance explained by each principal component (quality dimensions only)"
#| fig-width: 7
#| fig-height: 4

var_df <- data.frame(
  Component = paste0("PC", 1:length(var_explained_qual)),
  Variance = var_explained_qual * 100,
  Cumulative = cumvar_qual * 100
)

var_df$Component <- factor(var_df$Component, levels = var_df$Component)

ggplot(var_df, aes(x = Component)) +
  geom_col(aes(y = Variance), fill = "steelblue", alpha = 0.7) +
  geom_line(aes(y = Cumulative, group = 1), color = "red", linewidth = 1) +
  geom_point(aes(y = Cumulative), color = "red", size = 2) +
  labs(
    title = "Variance explained by principal components",
    y = "Percentage of variance",
    x = "Principal component"
  ) +
  theme_minimal() +
  theme(panel.grid.major.x = element_blank())
```

The first principal component (PC1) explains
`r round(var_explained_qual[1] * 100, 1)`% of the total variance in the
7 quality dimensions. The first two components together explain
`r round(cumvar_qual[2] * 100, 1)`% of the variance, and the first
three explain `r round(cumvar_qual[3] * 100, 1)`%.

This pattern suggests that while there is a dominant "general quality"
dimension, there are also meaningful secondary dimensions capturing
distinct aspects of research quality. The fact that PC1 doesn't explain
everything indicates that our seven questions are not simply redundant
measures of the same thing.

### Component loadings and interpretation

The table below shows the loadings for the first three components.
Loadings indicate how much each original variable contributes to each
component. Larger absolute values mean stronger contributions.

```{r}
#| label: tbl-loadings
#| tbl-cap: "Loadings of first 3 principal components on quality ratings"

ldg_qual <- loadings(pc_qual)
ldg_qual <- unclass(ldg_qual)

as_hux(ldg_qual[, 1:3], add_colnames = TRUE, add_rownames = "Question") |>
  style_header_rows(bold = TRUE) |>
  set_align(-1, -1, ".") |>
  set_number_format(-1, -1, "%.3f") |>
  map_text_color(-1, -1, by_colorspace("blue", "grey", "red"))
```

**Component 1** has uniformly positive loadings across all dimensions,
with the strongest weights on "overall", "adv_knowledge", "methods", and
"logic_comms". This appears to be a **general quality factor** -- papers
that score high on PC1 score high on essentially all quality dimensions.

**Component 2** shows an interesting contrast: it has strong positive
loadings on "open_sci" and "real_world", but near-zero or negative
loadings on other dimensions. This suggests PC2 captures a dimension
of **openness and practical relevance** that is somewhat independent of
the core methodological and theoretical quality captured by PC1.

**Component 3** shows yet another pattern, with "gp_relevance" (relevance
to global priorities) having a distinctive loading pattern. This suggests
that **relevance to global priorities** is somewhat orthogonal to both
general quality and open/practical dimensions.

These results indicate that our seven quality dimensions are *not* simply
redundant measures of a single underlying "quality" construct.

### Criteria redundancy: Within-rater correlations

To further examine whether our criteria are redundant, we can look at
correlations **within individual evaluators**. If two criteria are highly
correlated within raters, it suggests evaluators can't or don't distinguish
between them, and one might be redundant.

```{r}
#| label: within-rater-correlations

# Calculate within-rater correlations
rater_cors_pearson <- list()
rater_cors_spearman <- list()

for (eval in unique(ratings$evaluator)) {
  eval_ratings <- ratings |>
    filter(evaluator == eval, criteria %in% qual_dimensions) |>
    select(research, criteria, middle_rating) |>
    tidyr::pivot_wider(names_from = criteria, values_from = middle_rating)

  if (nrow(eval_ratings) >= 3) {  # Need at least 3 papers
    eval_matrix <- as.matrix(eval_ratings[, qual_dimensions])

    # Only calculate if we have variance in the data
    if (ncol(eval_matrix) >= 2 && any(!is.na(eval_matrix))) {
      tryCatch({
        rater_cors_pearson[[eval]] <- cor(eval_matrix, use = "pairwise.complete.obs", method = "pearson")
        rater_cors_spearman[[eval]] <- cor(eval_matrix, use = "pairwise.complete.obs", method = "spearman")
      }, error = function(e) {
        # Skip this rater if correlation calculation fails
      })
    }
  }
}

# Average correlations across raters
if (length(rater_cors_pearson) > 0) {
  mean_cor_pearson <- Reduce("+", rater_cors_pearson) / length(rater_cors_pearson)
  mean_cor_spearman <- Reduce("+", rater_cors_spearman) / length(rater_cors_spearman)

  # Set diagonal to NA for clearer visualization
  diag(mean_cor_pearson) <- NA
  diag(mean_cor_spearman) <- NA
}

```

```{r}
#| label: fig-within-rater-cors
#| fig-cap: "Average within-rater correlations between quality dimensions"
#| fig-width: 10
#| fig-height: 5

if (length(rater_cors_pearson) > 0) {
  par(mfrow = c(1, 2))

  corrplot(mean_cor_pearson, method = "color", type = "upper",
           tl.col = "black", tl.srt = 45,
           title = "Pearson correlations\n(within-rater averages)",
           mar = c(0,0,2,0), na.label = " ")

  corrplot(mean_cor_spearman, method = "color", type = "upper",
           tl.col = "black", tl.srt = 45,
           title = "Spearman correlations\n(within-rater averages)",
           mar = c(0,0,2,0), na.label = " ")

  par(mfrow = c(1, 1))
}

```

```{r}
#| label: tbl-highest-cors
#| tbl-cap: "Top 10 criterion pairs by within-rater Pearson correlation"

if (length(rater_cors_pearson) > 0) {
  # Extract highest correlations
  cor_pairs <- data.frame(
    Criterion1 = rep(qual_dimensions, each = length(qual_dimensions)),
    Criterion2 = rep(qual_dimensions, length(qual_dimensions)),
    Pearson = as.vector(mean_cor_pearson),
    Spearman = as.vector(mean_cor_spearman)
  ) |>
    filter(Criterion1 < Criterion2) |>  # Remove duplicates
    arrange(desc(Pearson)) |>
    head(10)

  cor_pairs |>
    as_huxtable() |>
    set_number_format(everywhere, 3:4, "%.3f") |>
    set_bold(1, everywhere, TRUE)
}

```

These within-rater correlations show which criteria evaluators find most
difficult to distinguish. High correlations (above 0.7-0.8) would suggest
potential redundancy. The results show moderate correlations overall,
suggesting that while our criteria are related (as expected for different
aspects of quality), they capture meaningfully distinct dimensions that
evaluators can and do distinguish between.

## Interactive visualization of ratings

```{r}
#| label: fig-interactive-ratings
#| fig-cap: "Interactive scatter plot of overall ratings by research area (hover for paper names)"

ratings_for_plot <- ratings |>
  filter(criteria == "overall") |>
  left_join(
    research |> select(label_paper_title, main_cause_cat_abbrev),
    by = c("research" = "label_paper_title")
  ) |>
  filter(!is.na(main_cause_cat_abbrev)) |>
  mutate(
    display_name = ifelse(nchar(research) > 50,
                         paste0(substr(research, 1, 50), "..."),
                         research)
  )

p <- ggplot(ratings_for_plot, aes(x = main_cause_cat_abbrev, y = middle_rating,
                                  text = research, color = main_cause_cat_abbrev)) +
  geom_jitter(width = 0.2, alpha = 0.6, size = 2) +
  stat_summary(fun = mean, geom = "point", shape = 4, size = 4, stroke = 2) +
  labs(
    title = "Overall ratings by research area",
    subtitle = "Crosses show area averages. Hover for paper titles.",
    x = "Research Area",
    y = "Overall Rating (percentile)"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "none"
  )

ggplotly(p, tooltip = c("text", "y"))

```

## Analysis of journal tier predictions

The journal tier questions offer a unique opportunity because they have
potential ground truth: we can eventually observe where papers are
published. Let's examine these ratings in more detail.

```{r}
#| label: journal-analysis

journal_ratings <- ratings |>
  filter(criteria %in% journal_dimensions) |>
  select(research, evaluator, criteria, middle_rating, lower_CI, upper_CI)

# Summary statistics by dimension
journal_summary <- journal_ratings |>
  group_by(criteria) |>
  summarize(
    Mean = mean(middle_rating, na.rm = TRUE),
    SD = sd(middle_rating, na.rm = TRUE),
    Median = median(middle_rating, na.rm = TRUE),
    Min = min(middle_rating, na.rm = TRUE),
    Max = max(middle_rating, na.rm = TRUE),
    N = sum(!is.na(middle_rating))
  )
```

### Distribution of journal tier ratings

```{r}
#| label: fig-journal-dist
#| fig-cap: "Distribution of journal tier ratings"
#| fig-width: 8
#| fig-height: 4

journal_ratings |>
  mutate(
    criteria = case_match(criteria,
      "journal_predict" ~ "Will be published (prediction)",
      "merits_journal" ~ "Should be published (merit)"
    )
  ) |>
  ggplot(aes(x = middle_rating, fill = criteria)) +
  geom_histogram(alpha = 0.6, position = "identity", bins = 20) +
  facet_wrap(~ criteria, ncol = 1) +
  scale_x_continuous(breaks = 0:5) +
  labs(
    title = "Distribution of journal tier ratings",
    x = "Journal tier (0-5)",
    y = "Count",
    fill = "Rating type"
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```

```{r}
#| label: tbl-journal-summary
#| tbl-cap: "Summary statistics for journal tier ratings"

journal_summary |>
  mutate(
    criteria = case_match(criteria,
      "journal_predict" ~ "Will be published",
      "merits_journal" ~ "Should be published"
    )
  ) |>
  rename(Dimension = criteria) |>
  as_huxtable() |>
  set_number_format(-1, 2:6, "%.2f") |>
  set_bold(1, everywhere, TRUE)
```

On average, evaluators predict papers will be published at tier
`r round(journal_summary$Mean[journal_summary$criteria == "journal_predict"], 2)`,
but believe they merit publication at tier
`r round(journal_summary$Mean[journal_summary$criteria == "merits_journal"], 2)`.

### Statistical test for merit vs. prediction difference

To test whether this difference is statistically meaningful (not just noise),
we can conduct a paired t-test on matched merit and prediction ratings:

```{r}
#| label: merit-vs-predict-test

journal_wide <- journal_ratings |>
  select(research, evaluator, criteria, middle_rating) |>
  pivot_wider(names_from = criteria, values_from = middle_rating) |>
  filter(!is.na(merits_journal), !is.na(journal_predict))

# Paired t-test
t_test_result <- t.test(journal_wide$merits_journal,
                        journal_wide$journal_predict,
                        paired = TRUE)

# Effect size (Cohen's d for paired samples)
diff_mean <- mean(journal_wide$merits_journal - journal_wide$journal_predict)
diff_sd <- sd(journal_wide$merits_journal - journal_wide$journal_predict)
cohens_d <- diff_mean / diff_sd

```

A paired t-test comparing merit and prediction ratings shows:

- Mean difference: `r round(diff_mean, 3)` tiers (merit - prediction)
- t(`r t_test_result$parameter`) = `r round(t_test_result$statistic, 2)`,
  p `r ifelse(t_test_result$p.value < 0.001, "< 0.001", paste("=", round(t_test_result$p.value, 3)))`
- Cohen's d = `r round(cohens_d, 2)`
  (`r ifelse(abs(cohens_d) < 0.2, "negligible", ifelse(abs(cohens_d) < 0.5, "small", ifelse(abs(cohens_d) < 0.8, "medium", "large")))` effect size)
- 95% CI for difference: [`r round(t_test_result$conf.int[1], 2)`,
  `r round(t_test_result$conf.int[2], 2)`]

`r if(abs(diff_mean) > 0.1 && t_test_result$p.value < 0.05) { paste0("This indicates a statistically significant and meaningfully sized difference between merit and prediction ratings. Evaluators consistently ", ifelse(diff_mean > 0, "rate papers as deserving higher-tier placement than they expect them to receive", "predict papers will be published in higher tiers than they believe the papers merit"), ".") } else { "The difference between merit and prediction ratings is not statistically significant." }`

```{r}
#| label: fig-journal-comparison
#| fig-cap: "Comparison of merit vs. prediction ratings for journal tiers"
#| fig-width: 6
#| fig-height: 6

ggplot(journal_wide, aes(x = journal_predict, y = merits_journal)) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray") +
  geom_point(alpha = 0.5, size = 2) +
  geom_smooth(method = "lm", se = TRUE, color = "blue", linewidth = 1) +
  scale_x_continuous(breaks = 0:5, limits = c(0, 5)) +
  scale_y_continuous(breaks = 0:5, limits = c(0, 5)) +
  coord_fixed() +
  labs(
    title = "Merit vs. prediction for journal tier",
    subtitle = "Points above the diagonal indicate merit > prediction",
    x = "Predicted journal tier",
    y = "Merited journal tier"
  ) +
  theme_minimal()
```

Most points lie above the diagonal line, confirming that evaluators
generally believe papers merit higher tier publication than they predict
they will receive.

### Relationship to quality dimensions

Finally, we can examine how the journal tier ratings relate to the
seven quality dimensions:

```{r}
#| label: fig-journal-quality-cors
#| fig-cap: "Correlations between quality dimensions and journal tier ratings"
#| fig-width: 8
#| fig-height: 5

# Calculate mean ratings for each paper
mean_by_paper <- ratings |>
  filter(criteria %in% c(qual_dimensions, journal_dimensions)) |>
  group_by(research, criteria) |>
  summarize(mean_rating = mean(middle_rating, na.rm = TRUE), .groups = "drop")

# Pivot to wide format
mean_wide <- mean_by_paper |>
  pivot_wider(names_from = criteria, values_from = mean_rating)

# Calculate correlations
cors_merit <- sapply(qual_dimensions, function(dim) {
  cor(mean_wide[[dim]], mean_wide$merits_journal, use = "pairwise.complete.obs")
})

cors_predict <- sapply(qual_dimensions, function(dim) {
  cor(mean_wide[[dim]], mean_wide$journal_predict, use = "pairwise.complete.obs")
})

cor_df <- data.frame(
  Dimension = rep(qual_dimensions, 2),
  Correlation = c(cors_merit, cors_predict),
  Type = rep(c("Merit", "Prediction"), each = length(qual_dimensions))
)

ggplot(cor_df, aes(x = Dimension, y = Correlation, fill = Type)) +
  geom_col(position = "dodge") +
  coord_flip() +
  labs(
    title = "Correlation between quality dimensions and journal tier ratings",
    x = NULL,
    y = "Pearson correlation"
  ) +
  theme_minimal() +
  theme(legend.position = "top")
```

This shows which quality dimensions are most strongly associated with
evaluators' judgments about journal tier. Typically, "overall"
assessment and "adv_knowledge" show the strongest correlations with
both merit and prediction ratings.

"open_sci" tends to show weaker correlations with journal
tier predictions, which may reflect a realistic assessment that
traditional journals don't always reward open science practices as
strongly as other quality dimensions.

## Conclusions

This analysis of `r n_papers` papers and `r n_evals` evaluations
reveals several key patterns:

1.  **Data quality**: Most evaluators provide thoughtful, differentiated
    ratings with few signs of straightlining or obviously problematic
    confidence intervals.

2.  **Inter-rater reliability**: We see moderate agreement between
    evaluators (Krippendorff's interval alpha typically 0.4-0.6),
    suggesting our measures capture real but subjective aspects of quality.
    Credible interval overlap analysis suggests evaluators' intervals
    capture meaningful uncertainty about true underlying quality.

3.  **Dimensionality**: The seven quality dimensions are not redundant.
    While there is a strong general quality factor (explaining
    ~`r round(var_explained_qual[1] * 100)`% of variance), we also see
    meaningful secondary dimensions related to open science/practical
    relevance and global priorities relevance. Within-rater correlation
    analysis confirms that evaluators distinguish meaningfully between
    our different criteria.

4.  **Journal tiers**: Evaluators consistently judge papers as meriting
    higher-tier publication than they predict they will receive.
    This difference is statistically significant (p < 0.001) with a
    `r ifelse(abs(cohens_d) < 0.5, "small to medium", "medium to large")`
    effect size. As we observe actual publication outcomes, we'll be able to
    assess the accuracy of these predictions.

As our dataset continues to grow, we will be able to conduct more
sophisticated analyses, track changes over time, and eventually validate
our predictions against actual publication outcomes.
