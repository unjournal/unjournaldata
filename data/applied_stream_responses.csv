Title of the paper or project,Link,About our evaluation process,Identification as an author,Code,Summary of Eval,Evaluation,"Claim identification, assessment, and implications",Overall assessment ranking,Lower Cl Overall assessment ranking,Upper Cl - Overall assessment ranking,Claims - midpoint ranking,Lower Cl - Claims,Upper Cl - Claims,Methods:  - midpoint ranking,Upper Cl - Methods,Lower Cl - Methods,Advancing Knowledge ranking,Lower Cl - Advancing Knowledge,Upper Cl - Advancing Knowledge,midpoint_logic_comms,lower_ci_logic_comms,upper_ci_logic_comms,midpoint_ci_replicable,lower_ci_replicable,upper_ci_replicable,midpoint_relevance,lower_ci_relevance,upper_ci_relevance,Very optional: Journal ranking tiers,midpoint_normative_journal,midpoint_predicted_journal,lower_ci_normative_journal,upper_ci_normative_journal,lower_ci_predicted_journal,upper_ci_predicted_journal,Confidential comments,Confidential comments section,Survey questions,How long have you been in this field?,"How many proposals, papers, and projects have you evaluated/reviewed (for journals, grants, or other peer-review)?",Feedback,Approximately how long did you spend completing this evaluation?,How would you rate this template and process?,Would you be willing to consider evaluating a revised version of this work?,"Do you have any other suggestions or questions about this process or The Unjournal? (We will try to respond, and incorporate your suggestions.)",Metrics,Main claim,eval collab,Confidence in claim,"What additional information, evidence, replication, or robustness check would make you substantially more (or less) confident in this claim?",ratings and CIs,Overall assessment: rating,Overall assessment,"Methods: Justification, reasonableness, validity, robustness",Advancing knowledge and practice,Logic & communication,"Open, collaborative, replicable","Relevance to global priorities, usefulness for practitioners",journal_rank_normative_applied,Final section,Comments on Overall Assessment rating,_______________________,"Comments on ""Claims..."" rating (optional)","Comments on ""Methods..."" rating (optional) 2","Comments on ""Advancing knowledge & practice"" rating (optional) 3","Comments on ""Logic & Comms"" rating (optional)","Comments on ""Open... replicable"" rating (optional)",comments_relevance_rating,"Comments on ""Journal ranking tiers"" ratings and predictions (optional)",If relevant … “what journal ranking tier *will* this work be published in?”,attach_evaln_applied,COI,Check COI
Forecasts estimate limited cultured meat production through 2050,https://forum.effectivealtruism.org/posts/2b9HCjTiFnWM8jkRM/forecasts-estimate-limited-cultured-meat-production-through,"This form pertains to our applied and policy stream (see discussion of this stream here). We ask evaluators in this stream to: 
Write an evaluation. This may resemble a high-quality referee report for a standard journal (see standard guidelines), but geared towards the goals of our applied stream, and The Unjournal’s emphases. Please try to address any specific considerations mentioned in the our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Provide ‘claim identification and assessment’ if appropriate (see below)
Give quantitative metrics and predictions. As in our academic stream, this work will be evaluated for its credibility, usefulness, communication/logic, etc. However, for the applied stream, we do not need this to be assessed by the standards of academia in a way that yields a comparison to traditional journal tiers.
Answer a short questionnaire about your background and our processes.
Our full evaluation guidelines are here (for the ’academic-targeted’ stream; we’re building a version for the applied stream).

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. This is generally between $200 and $500 as a baseline, in addition to $150 per evaluation set aside for further incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. 


Note on this form: We’re trying the present Coda-based form for a few things. We have mainly used a PubPub based evaluation form, and may do so in the future.

“Pivotal questions” evaluators: Please try to substantially focus your evaluation on issues that are relevant to the identified Pivotal Question. You can discuss your beliefs for the focal PQ in the “Claim identification” boxes below, as well as in your report, and in the Metaculus interface (if we have shared one with you).

“Independent evaluators”:  We are exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate a paper), please see the explicit guidance  on how to use this form.",Estere Seinkmane,-,"My evaluation focused on assessing the assumptions and providing up-to-date context and background for the forecasting paper. The papet (post) was published in 2022, and relied on 2020-2021 data. Since then, at least eight cultivated meat (CM) products have been approved, claimed costs are below $20/kg, CM researchers exceed 250, and total funding surpassed $3B, coming from VC, public research funding, and philanthropic organisations. The CM definition (cell types, species, % in hybrid products), the media cost components, and other relevant technologies could be clarified, esp. in the context of more novel findings, to improve further CM forecasts.",https://docs.google.com/document/d/1iPWjHumd24tD5aa4jppE7691bTQPQXf1Nj1uiADtrGQ/edit?tab=t.t4w1n540ufpt  ,"Claim identification, assessment, and implications [PQ evaluators can skip this section]

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",70,60,80,,,,,,,,,,,,,,,,,,,"In our academic stream, we ask evaluators to consider “what journal ranking tier should this work be published in?” and “What journal ranking tier should this work be published in?”. As noted, most work in our *applied stream* will not be targeting academic journals. Still, in some cases it might make sense to make this comparison; e.g., if particular aspects of the work might be rewritten and submitted to academic journals, or if the work uses certain techniques that might be directly compared to academic work.  If you believe a comparison makes sense, please consider giving an assessment below, making reference to our usual guidelines for this here, and how you are interpreting them in this case.",,,,,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

","3 years actively following cellular agriculture field, 8 years cel biology experience","formally: ~2, informally for colleagues: ~7-9","Feedback

Responses to the questions below here will not be public or seen by authors.

","~12h on total work package incl. literture review, notes on PQ, comment replies etc; ~3h on post evaluation; ~30-45min on completing the form ","I feel it was designed for other Unjournal work and less suitable for this PQ-related work (as far as I’ve understood it anyway). So here for me the process was somewhat confusing due to many layers of the work, especially when it came down to evaluating the evaluation (D&Z) of evaluation (TEAs) of data, and its evaluation (4th layer, PQ operalisation devised by Unjournal) but David was helpful clarifying expectations and providing briefs","possibly, depending on the time commitment and timelines required",,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper according to the specified criteria considering the qualities of this research relative to applied and policy research you have read aiming at a similar audience, and with similar goals. (If possible, please define the reference group you are using in the comments.) For example, choose 51% if you think this paper is better than 51% of such work, according to this criterion.  We define and discuss this in more detail here.

Criteria:  For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.","D&Z state, ""The aggregated probabilities from our panel include a 54% probability that less than 100,000 metric tons of cultured meat (where >51% of the 'meat' is produced directly from animal cells) will be produced and sold at any price in a 12-month period before the end of 2051""","possibly, depending on the time commitment and timelines required","struggle to answer as I was assessing the context, not doing the forecasting exercise myself. broadly this seems in the right ballpark, though I think the production would be approaching and would already reach 100k ton level in the next 10-20 years with a slightly higher probability (>50%, <70% - though again very hard to say as forecasting here was not my focus)","specifically on the production volume, I would consult this post from Elliot Schwartz (GFI) estimating current production at 500 metric tons - https://www.linkedin.com/posts/elliot-swartz-19933420_cultivatedmeat-activity-7354897404975796224-CxNq?utm_source=share&utm_medium=member_desktop&rcm=ACoAADm8HHIBvmkelbecyQo8n4uVN6pWS9eG978 - and underlying sources; I suggested further literature etc in my evaluation ","CIs: You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval.  For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.


","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to future impactful applied research, and practical relevance and usefulness.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?",Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? ,To what extent does the project contribute to improving the rigor and reliability of applied research and evidence-based policymaking in this area? Focus on ‘improvements that are actually helpful’ for global priorities and impactful interventions?,"Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners. Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),"in the context of my specific tasks and requested help with PQ, I am not sure what I am supposed to be assessing here - the forecasts themselves, the forecasting exercise and write-up at the time of publishing (2022), or its relevance today. therefore I am skipping these questions where I can. where I’ve provided answers, it is assessing how D&Z brought together the forecasts at the time of their work (2021-2022).","Evaluation report
This may resemble a high-quality referee report for an academic journal. However, we have some particular requests:
Please put a strong focus on considerations that could inform practical users of the research and other applied researchers, directly or indirectly.  
We do want you to consider theoretical and technical concerns, leveraging your own technical expertise 
Insofar as these inform practice
But not if they are mainly relevant to building the academic field, deep knowledge of humanity for it’s own sake, etc. 
Address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.

Again, you may want to consider standard guidelines as well as The Unjournal’s emphases.",,,,,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research",True
A Review of Givewell’s Discount Rate,,"This form pertains to our applied and policy stream (see discussion of this stream here). We ask evaluators in this stream to: 
Write an evaluation. This may resemble a high-quality referee report for a standard journal (see standard guidelines), but geared towards the goals of our applied stream, and The Unjournal’s emphases. Please try to address any specific considerations mentioned in the our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Provide ‘claim identification and assessment’ if appropriate (see below)
Give quantitative metrics and predictions. As in our academic stream, this work will be evaluated for its credibility, usefulness, communication/logic, etc. However, for the applied stream, we do not need this to be assessed by the standards of academia in a way that yields a comparison to traditional journal tiers.
Answer a short questionnaire about your background and our processes.
Our full evaluation guidelines are here (for the ’academic-targeted’ stream; we’re building a version for the applied stream).

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. This is generally between $200 and $500 as a baseline, in addition to $150 per evaluation set aside for further incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. 


Note on this form: We’re trying the present Coda-based form for a few things. We have mainly used a PubPub based evaluation form, and may do so in the future.

“Pivotal questions” evaluators: Please try to substantially focus your evaluation on issues that are relevant to the identified Pivotal Question. You can discuss your beliefs for the focal PQ in the “Claim identification” boxes below, as well as in your report, and in the Metaculus interface (if we have shared one with you).

“Independent evaluators”:  We are exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate a paper), please see the explicit guidance  on how to use this form.",,salted cod and hash,"I have two topline thoughts:
I don’t think anyone would be offended by or think the constant 4.0% or 4.3% rate is unreasonable for discounting dollar or pound terms.
The report is too long and goes into many details to justify a rather imprecise exercise. While missing some of the most important questions I would think an organization like GiveWell would want to consider. In part, this is hard because I don’t know the scope of work the consultants had, but being a good consultant is about asking the right questions not answering the ones you are given. "," 
This was hard to evaluate because I needed to infer that (1) Givewell is using a cost-effectiveness approach and not a benefit-cost approach.  While this is written clearly, much of the writing pulls from the BCA literature and the places where I think they are referencing BCA. I think it very hard to assess a discount rate with a fully understanding of what is being discounted.
 
I don’t really know who this is written for? I think most economists will find it kind of random and just say go read Groom et al. Project managers won’t follow it.  Most donors won’t care. The executive summary is far too in the weeds for an executive summary for any policy white paper.
 
The first thing I would want to know is whose opportunity cost are we talking about? It sounds like GiveWell has a philanthropic budget to spend every period (e.g., year), and that budget is allocated to N regions. Thus, there is no opportunity cost of capital to GiveWell given the way it has chosen to operate (use it or lose it budgeting). The goal here is not to maximize value for donors but to use donor funds to maximize value for recipients in N regions. That means asking what projects would the recipients choose for themselves. If projects are evaluated across regions, then there is no point in being concerned about 4.0% v 4.3% because the appropriate number for region 1 is likely different from that of region 2 by at least 0.3 percentage points. That is, the people in region 1 likely have a different discount rate from the people in region 2. People in regions with greater than 4.3% discount rates would prefer more spending now and less in the future, and the reverse would be true for those in regions with less than 4.0%.  My point is that I like the bit about space matters, but it seems GiveWell might want to use a different discount rate in different places rather than coming up with a single number. This seems like a high level point that could have been made, but was not. 
 
In regions GiveWell works, there are distorted capital markets, governments don’t issue long-term maturities, and there is a lot of risk that GiveWell’s projects are intended to help mediate, so it is hard to have a standard opportunity cost of capital. Fine. But, I don’t think the report needs to drone on about this stuff. I feel like they are being very precise about something that turns out to be a very imprecise decision for a foundation with a lot of leeway. I think they would be better served with a highly level discussion. Empirically, I would ask how many GiveWell funding decisions would have changed with a 3% or 5% discount rate over the past 5 years? That analysis would tell you how precise you really need to be. 
This is especially true when you constrain yourself to choosing a single exponential discount rate. Frankly, I think anything between 2 and 15 percent could be justified, and that is a huge range. Just to put all this perspective, we have a 40-page report that is trying to justify 1 pound being 28p in 30 years v 1 pound being 30 p in 30 years. I also think you can get to both these numbers using a standard Ramsey rule.
 
Who is doing the analysis? Is it PhD economists or is the head of a local NGO in a project proposal? I ask because the logic of just choosing a single number has a lot to do with who is doing the analysis.
 
When an organization like GiveWell chooses a discount rate, it is imposing how long it is willing to wait for a social return (assuming BCA). This is basically how much risk I am willing to take when I think there could be big benefits in the future. I say this because in my experience, the discount rates are always lower there than the communities they are funding would choose for themselves with a private discount rate. The important point is that presumably the social discount rate is a convex combination of private discount rates. 
 
What would be more helpful would be to discuss that GiveWell projects probably don’t have GE effects, so g remains exogenous. GiveWell projects likely provide higher returns during economic downturns and are intended to be, by their nature, de-risking. This would justify a below-market rate in an economy with a large risk premium in the general opportunity cost of capital.
Groom et al. provide a review that would be helpful. Addicott et al. provide country-specific estimates that might be helpful. You might find it helpful to read.

I did not understand the discounting health benefits at a different rate than monetary benefits. First, I thought this was cost-effectiveness, not BCA, so we are discounting costs per health outcome. Second, the right thing to do with BCA is first monetize the health benefits. Third, the reason health projects might have a different discount rate is that health projects are investments in human capital, and the goal of the project is to reduce the depreciation rate of health capital. In the extreme, this endogenizes the discount rate (Jone 2016). The only reason that it might look like the discount rate is lower is that in standard approaches to valuing specialized capital, the depreciation rate enters additively to the opportunity cost of capital. A positive depreciation rate raises the sum of the opportunity cost of capital and the depreciation rate, which some people then call the project-specific discount rate.

Finally, I think there is good information on discounting in the 2023 version of the US A-4 document and in the UK greenbook that might be helpful. 
 
Addicott, Ethan T.; Fenichel, Eli P.; and Kotchen, Matthew J. ""Even the Representative Agent Must Die: Using Demographics to Inform Long-Term Social Discount Rates."" Journal of the Association of Environmental and Resource Economists, 2020, 7, pp. 379-415.
 
Groom, Ben; Drupp, Moritz A.; Freeman, Mark C.; and Nesje, Frikk. ""The Future, Now: A Review of Social Discounting."" Annual Review of Resource Economics, 2022, 14, pp. 467-91.
 
Jones, Charles I. ""Life and Growth."" Journal of Political Economy, 2016, 124, pp. 539-78.
 
 ","Claim identification, assessment, and implications [PQ evaluators can skip this section]

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",5,0,8,0,0,0,,,,,,,,,,,,,,,,"In our academic stream, we ask evaluators to consider “what journal ranking tier should this work be published in?” and “What journal ranking tier should this work be published in?”. As noted, most work in our *applied stream* will not be targeting academic journals. Still, in some cases it might make sense to make this comparison; e.g., if particular aspects of the work might be rewritten and submitted to academic journals, or if the work uses certain techniques that might be directly compared to academic work.  If you believe a comparison makes sense, please consider giving an assessment below, making reference to our usual guidelines for this here, and how you are interpreting them in this case.",,,,,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

","I have nothing private to say. My snarky comment that I cut was, “Are they getting paid by the page?”  Seriously, the Groom et al. review is only 28 pages long! 
The executive summary is awful for a policy white paper. ","Responses to these will be public unless you mention in your response that you want us to keep them private.

",20+,far far too many to count,"Feedback

Responses to the questions below here will not be public or seen by authors.

",2 hrs,OK - note I skipped a bunch. ,"no - I don’t have time, and I don’t think I am going to learn anything. ",Still don’t understand what this is. ,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper according to the specified criteria considering the qualities of this research relative to applied and policy research you have read aiming at a similar audience, and with similar goals. (If possible, please define the reference group you are using in the comments.) For example, choose 51% if you think this paper is better than 51% of such work, according to this criterion.  We define and discuss this in more detail here.

Criteria:  For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,no,,"Had I written this, I would have focused on the criteria for choosing one approach over another and explained in the context of GiveWells CEA or BCA process. ","CIs: You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval.  For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.


","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to future impactful applied research, and practical relevance and usefulness.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?",Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? ,To what extent does the project contribute to improving the rigor and reliability of applied research and evidence-based policymaking in this area? Focus on ‘improvements that are actually helpful’ for global priorities and impactful interventions?,"Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners. Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),"There is nothing wrong with where they get, but I suspect the decision makers at GiveWell would be better served reading Groom et al. and have the consultants do a short (3-4) write up on the choose of numbers. Because of this I really don’t know how to judge this. Assuming they are not paid buy the page, I would have had the time say read Groom et al for back ground or written a two page summary of that paper and had a page or so of possible numbers with pros and cons and been done. ","Evaluation report
This may resemble a high-quality referee report for an academic journal. However, we have some particular requests:
Please put a strong focus on considerations that could inform practical users of the research and other applied researchers, directly or indirectly.  
We do want you to consider theoretical and technical concerns, leveraging your own technical expertise 
Insofar as these inform practice
But not if they are mainly relevant to building the academic field, deep knowledge of humanity for it’s own sake, etc. 
Address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.

Again, you may want to consider standard guidelines as well as The Unjournal’s emphases.",,,I don’t think this advances knowledge generally. ,,,,no appropriate for a journal. ,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research",True
Forecasts estimate limited cultured meat production through 2050,https://rethinkpriorities.org/research-area/forecasts-estimate-limited-cultured-meat-production-through-2050/,"This form pertains to our applied and policy stream (see discussion of this stream here). We ask evaluators in this stream to: 
Write an evaluation. This may resemble a high-quality referee report for a standard journal (see standard guidelines), but geared towards the goals of our applied stream, and The Unjournal’s emphases. Please try to address any specific considerations mentioned in the our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Provide ‘claim identification and assessment’ if appropriate (see below)
Give quantitative metrics and predictions. As in our academic stream, this work will be evaluated for its credibility, usefulness, communication/logic, etc. However, for the applied stream, we do not need this to be assessed by the standards of academia in a way that yields a comparison to traditional journal tiers.
Answer a short questionnaire about your background and our processes.
Our full evaluation guidelines are here (for the ’academic-targeted’ stream; we’re building a version for the applied stream).

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. This is generally between $200 and $500 as a baseline, in addition to $150 per evaluation set aside for further incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. 


Note on this form: We’re trying the present Coda-based form for a few things. We have mainly used a PubPub based evaluation form, and may do so in the future.

“Pivotal questions” evaluators: Please try to substantially focus your evaluation on issues that are relevant to the identified Pivotal Question. You can discuss your beliefs for the focal PQ in the “Claim identification” boxes below, as well as in your report, and in the Metaculus interface (if we have shared one with you).

“Independent evaluators”:  We are exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate a paper), please see the explicit guidance  on how to use this form.",David Manheim,PseudonymNotSaltedHash,"The review finds Rethink Priorities’ 2022 cultured meat forecast methodologically sound but limited and misleading. It critiques the pessimistic framing, exclusion of luxury-product adoption, and potential premature impact on funding. Despite uncertainty on mass scale by 2050, falling input costs and emerging luxury markets suggest greater promise than implied. Still, success depends on scaling, adoption, and investment.",https://docs.google.com/document/d/1vRLEVJt31BD8buRZmNcsWC5-AEtfso2B9J1KiBtUFhI/edit?usp=sharing,"Claim identification, assessment, and implications [PQ evaluators can skip this section]

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",85,60,91,50,40,85,84,94,80,92,80,94,50,43,67,92,84,98,95,87,99,"In our academic stream, we ask evaluators to consider “what journal ranking tier should this work be published in?” and “What journal ranking tier should this work be published in?”. As noted, most work in our *applied stream* will not be targeting academic journals. Still, in some cases it might make sense to make this comparison; e.g., if particular aspects of the work might be rewritten and submitted to academic journals, or if the work uses certain techniques that might be directly compared to academic work.  If you believe a comparison makes sense, please consider giving an assessment below, making reference to our usual guidelines for this here, and how you are interpreting them in this case.",,,,,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",I discussed this with David quite a bit already,"Responses to these will be public unless you mention in your response that you want us to keep them private.

","Construed broadly, 8 years.",5-10 relevant items,"Feedback

Responses to the questions below here will not be public or seen by authors.

",~10 hours,9/10,They won’t revise...,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper according to the specified criteria considering the qualities of this research relative to applied and policy research you have read aiming at a similar audience, and with similar goals. (If possible, please define the reference group you are using in the comments.) For example, choose 51% if you think this paper is better than 51% of such work, according to this criterion.  We define and discuss this in more detail here.

Criteria:  For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",There is limited promise for scaling to reduce costs of cultured meat enough to greatly reduce consumption of traditional animal farming by 2050.,"Happy to, if people are interested.",50%-70%,,"CIs: You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval.  For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.


","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to future impactful applied research, and practical relevance and usefulness.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?",Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? ,To what extent does the project contribute to improving the rigor and reliability of applied research and evidence-based policymaking in this area? Focus on ‘improvements that are actually helpful’ for global priorities and impactful interventions?,"Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners. Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
This may resemble a high-quality referee report for an academic journal. However, we have some particular requests:
Please put a strong focus on considerations that could inform practical users of the research and other applied researchers, directly or indirectly.  
We do want you to consider theoretical and technical concerns, leveraging your own technical expertise 
Insofar as these inform practice
But not if they are mainly relevant to building the academic field, deep knowledge of humanity for it’s own sake, etc. 
Address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.

Again, you may want to consider standard guidelines as well as The Unjournal’s emphases.",,,,,,,"Journals wouldn’t publish this because it matters but isn’t novel or academic enough, so irrelevant","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research",True
Towards best practices in AGI safety and governance: A survey of expert opinion,https://arxiv.org/abs/2305.07153,"This form pertains to our applied and policy stream (see discussion of this stream here). We ask evaluators in this stream to: 
Write an evaluation. This may resemble a high-quality referee report for a standard journal (see standard guidelines), but geared towards the goals of our applied stream, and The Unjournal’s emphases. Please try to address any specific considerations mentioned in the our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Provide ‘claim identification and assessment’ if appropriate (see below)
Give quantitative metrics and predictions. As in our academic stream, this work will be evaluated for its credibility, usefulness, communication/logic, etc. However, for the applied stream, we do not need this to be assessed by the standards of academia in a way that yields a comparison to traditional journal tiers.
Answer a short questionnaire about your background and our processes.
Our full evaluation guidelines are here (for the ’academic-targeted’ stream; we’re building a version for the applied stream).

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. This is generally between $200 and $500 as a baseline, in addition to $150 per evaluation set aside for further incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. 


Note on this form: We’re trying the present Coda-based form for a few things. We have mainly used a PubPub based evaluation form, and may do so in the future.

“Pivotal questions” evaluators: Please try to substantially focus your evaluation on issues that are relevant to the identified Pivotal Question. You can discuss your beliefs for the focal PQ in the “Claim identification” boxes below, as well as in your report, and in the Metaculus interface (if we have shared one with you).

“Independent evaluators”:  We are exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate a paper), please see the explicit guidance  on how to use this form.",,Reviewer n,"This paper makes a valuable contribution to AGI safety by identifying areas of consensus in best practices, with implications for standards-setting. While the methodology is rigorous, more justification is needed for the selection and classification of the 50 practices. Potential biases—particularly AGI labs endorsing their own practices—should be better addressed. Organizing and contextualizing practices more clearly would aid practitioners. The interpretation of results should in some occasions be caveated with what the agreement gathered itself affords, and avoid going beyond that. Overall, this paper makes a great first step toward best practices for AGI and sets the grounds for important future work on the topic.","Contribution: This paper constitutes an important contribution to the development of best practices in AGI safety and governance. Importantly, it does so by identifying which practices already have broad support and where more work is needed. As the authors suggest, the findings also have important implications for standards-setting, by highlighting emerging areas of agreement in best practices that can inform standards processes.


Here are some remarks for further improvement:
The paper sets out to select a range of practices extracted from (1) current practices at individual AGI labs, (2) planned practices at individual labs, (3) proposals in the literature, and (4) discussion with experts and colleagues. While the paper makes clear how these practices were selected, more effort could go into explaining why they were selected and why they are considered “best” practices. The paper selects 50 “statements”, which is quite a high number. It could be useful, especially from a practitioner’s perspective, to introduce an inclusion/exclusion criterion to provide a better justification as to why some statements are selected. Such criterion does not have to be necessarily narrow, it can also just take into consideration excessive overlap between statements. Alternatively, the selection of such a number of best practices could be justified by wanting to ensure a representative set, and thus wanting to include more rather than less given their fast-changing and emerging nature. Additionally, the wealth of statements/practices presented may relate to each other. For example, there might be trade-offs between the choice of different practices (e.g., choosing between increasing levels of external scrutiny and industry sharing of security information) or some practices might be more specific instantiations of some more general statements (e.g., notify affected parties could be a specific instatiation of a more general practice like report safety incidents). While the paper makes a first important step in identifying a broad set of practices, it might be useful for practitioners to have a paragraph where they are contextualised or elaborated upon a bit more.
The paper thoroughly presents its methods in section 2, showing great rigour. While the section is very clear, some methodological choices might introduce some important biases that are currently not fully reflected in limitations. Importantly, the paper states that the selected practices are extracted from (1) current practices at individual AGI labs and (2) planned practices at individual labs, among other sources. At the same time, results suggest that AGI labs show general agreement on practices (even that “respondents from AGI labs had significantly higher overall mean agreement ratings than respondents from academia or civil society”), and even that on some items they (somewhat surprisingly) showed more agreement on some individual practices than other actors (e.g. the statement that AGI labs should grant independent researchers access to deployed models was more supported by AGI labs than academia and civil society, even though the result was not statistically significant). These results might suggest a selection bias where statements selected from labs practices are agreed on by labs themselves, and thus decrease the value of what that agreement really means for practitioners who read this research. Another choice which needs further backing is the decision to cluster “government” into the category “other”, together with actors such a consulting firms. While this is not an insensible choice overall, it constrains the ability of the paper to elaborate on implications for regulators (section 4.3) and thus, it misses an important opportunity. 
The paper does a great job in presenting the results of its analysis in the discussion. This section is very clear and well-written. As authors recognize in their limitations, the number of statements as well as the “high-level” at which they were presented to participants may suggest that agreement on practices could indicate a host of different things. While this is recognized in the paper as a limitation, it is important that it is considered in the way that results are interpreted too. For example, in the policy implications (section 4.3), the paper states that its “findings suggest that AGI labs need to improve their risk management practices. In particular, there seems to be room for improvement when it comes to their risk governance.” While one can agree with such claim, it is difficult to see how this conclusion can be reached from the paper’s results. For example, agreement (or lack thereof) on a set of practices by labs might mean many more different things than a lack of work on those practices (e.g. lack of agreement on a set of practices might mean that labs prefer an alternative set of practices to achieve the same goal). In order for the paper’s results not to be misleading for practitioners, it is important that implications do not go beyond the scope of the paper or that this nuance is made clear.
In terms of future directions, the paper states that the paper was accompanied by a workshop (Section 4.5). In the workshop, participants were asked about identifying the main blockers for the creation of best practices as well as open questions. One thing a follow-up workshop could do is to invite some (if not all) of the participants to the survey to delve deeper into some of the findings (e.g. look into the potential “why” of some of the replies or try to prioritize between the different elements that were voted on). Still, this is just a suggestion and I believe that the workshop which took place is a great first step into sketching future directions for the paper. 


Overall, I think this paper is a great first step into developing best practices for AGI and I wish the best to the authors, looking forward to their future work.

","Claim identification, assessment, and implications [PQ evaluators can skip this section]

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",75,65,85,80,70,90,70,80,60,70,60,80,80,70,90,80,70,90,70,60,80,"In our academic stream, we ask evaluators to consider “what journal ranking tier should this work be published in?” and “What journal ranking tier should this work be published in?”. As noted, most work in our *applied stream* will not be targeting academic journals. Still, in some cases it might make sense to make this comparison; e.g., if particular aspects of the work might be rewritten and submitted to academic journals, or if the work uses certain techniques that might be directly compared to academic work.  If you believe a comparison makes sense, please consider giving an assessment below, making reference to our usual guidelines for this here, and how you are interpreting them in this case.",,,,,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

","All relevant info is in the evaluation above. I would like that my evaluation is kept anonymous. As discussed via email, I do not sign the evaluation, but I agree to add a specific statement along the lines of 'the evaluator is a co-author with the first author in a paper with over ten authors.  Due to their recent work engagements, they also have some professional ties with at least two of the other co-authors.’
","Responses to these will be public unless you mention in your response that you want us to keep them private.

",For 8 years.,"It is a bit hard to count, but perhaps an average of 5 per year.","Feedback

Responses to the questions below here will not be public or seen by authors.

",half a day,Very well structured and clear,Maybe,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper according to the specified criteria considering the qualities of this research relative to applied and policy research you have read aiming at a similar audience, and with similar goals. (If possible, please define the reference group you are using in the comments.) For example, choose 51% if you think this paper is better than 51% of such work, according to this criterion.  We define and discuss this in more detail here.

Criteria:  For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"CIs: You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval.  For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.


","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to future impactful applied research, and practical relevance and usefulness.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?",Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? ,To what extent does the project contribute to improving the rigor and reliability of applied research and evidence-based policymaking in this area? Focus on ‘improvements that are actually helpful’ for global priorities and impactful interventions?,"Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners. Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
This may resemble a high-quality referee report for an academic journal. However, we have some particular requests:
Please put a strong focus on considerations that could inform practical users of the research and other applied researchers, directly or indirectly.  
We do want you to consider theoretical and technical concerns, leveraging your own technical expertise 
Insofar as these inform practice
But not if they are mainly relevant to building the academic field, deep knowledge of humanity for it’s own sake, etc. 
Address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.

Again, you may want to consider standard guidelines as well as The Unjournal’s emphases.",,,,,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research",False
The wellbeing cost-effectiveness of StrongMinds and Friendship Bench,https://www.happierlivesinstitute.org/report/the-wellbeing-cost-effectiveness-of-strongminds-and-friendship-bench-combining-a-systematic-review-and-meta-analysis-with-charity-related-data-nov-2024-update/,"This form pertains to our applied and policy stream (see discussion of this stream here). We ask evaluators in this stream to: 
Write an evaluation. This may resemble a high-quality referee report for a standard journal (see standard guidelines), but geared towards the goals of our applied stream, and The Unjournal’s emphases. Please try to address any specific considerations mentioned in the our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Provide ‘claim identification and assessment’ if appropriate (see below)
Give quantitative metrics and predictions. As in our academic stream, this work will be evaluated for its credibility, usefulness, communication/logic, etc. However, for the applied stream, we do not need this to be assessed by the standards of academia in a way that yields a comparison to traditional journal tiers.
Answer a short questionnaire about your background and our processes.
Our full evaluation guidelines are here (for the ’academic-targeted’ stream; we’re building a version for the applied stream).

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. This is generally between $200 and $500 as a baseline, in addition to $150 per evaluation set aside for further incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. 


Note on this form: We’re trying the present Coda-based form for a few things. We have mainly used a PubPub based evaluation form, and may do so in the future.

“Pivotal questions” evaluators: Please try to substantially focus your evaluation on issues that are relevant to the identified Pivotal Question. You can discuss your beliefs for the focal PQ in the “Claim identification” boxes below, as well as in your report, and in the Metaculus interface (if we have shared one with you).

“Independent evaluators”:  We are exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate a paper), please see the explicit guidance  on how to use this form.",,storywidencard,"This is a thorough report on the cost-effectiveness of two charities offering psychotherapy/ psychoeducation, which are found to be very cost effective. The evidence, however, is highly uncertain, both due to the quality of the studies and many of the somewhat arbitrary analytical choices. I discuss some of these subjective decisions and chiefly recommend a multiverse approach that transparently maps and compares such decisions. ",,"Claim identification, assessment, and implications [PQ evaluators can skip this section]

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",87,84,90,93,90,96,85,90,80,85,75,90,75,70,85,76,66,80,91,89,93,"In our academic stream, we ask evaluators to consider “what journal ranking tier should this work be published in?” and “What journal ranking tier should this work be published in?”. As noted, most work in our *applied stream* will not be targeting academic journals. Still, in some cases it might make sense to make this comparison; e.g., if particular aspects of the work might be rewritten and submitted to academic journals, or if the work uses certain techniques that might be directly compared to academic work.  If you believe a comparison makes sense, please consider giving an assessment below, making reference to our usual guidelines for this here, and how you are interpreting them in this case.",4.2,,3.5,4.4,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",~10,~50,"Feedback

Responses to the questions below here will not be public or seen by authors.

",~20 hours across 3 days,Very helpful and clear.,"Possibly - will certainly be happy to have a look but depending on the revision types may or may not be able to comment. If they relate to any of my recommendations here or general expertise, then it will likely be a yes.",,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper according to the specified criteria considering the qualities of this research relative to applied and policy research you have read aiming at a similar audience, and with similar goals. (If possible, please define the reference group you are using in the comments.) For example, choose 51% if you think this paper is better than 51% of such work, according to this criterion.  We define and discuss this in more detail here.

Criteria:  For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",Both charities evaluated at 5-6 times more cost-effective than cash transfers at improving subjective wellbeing (as reported in abstract and in report).,"Yes (though as above, if I can contribute meaningfully)","I believe the general claim about cost-effectiveness of the charities in a moderate to strong manner, as therapy provided by trained lay people in LMICs can be affordable. I am uncertain about the specific estimates due to the quality of the literature and the many stacking subjective analytical decisions.","More direct data (RCTs) independent evaluating these charities. For robustness checks, see my discussion of multiverse.","CIs: You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval.  For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.


","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to future impactful applied research, and practical relevance and usefulness.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?",Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? ,To what extent does the project contribute to improving the rigor and reliability of applied research and evidence-based policymaking in this area? Focus on ‘improvements that are actually helpful’ for global priorities and impactful interventions?,"Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners. Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
This may resemble a high-quality referee report for an academic journal. However, we have some particular requests:
Please put a strong focus on considerations that could inform practical users of the research and other applied researchers, directly or indirectly.  
We do want you to consider theoretical and technical concerns, leveraging your own technical expertise 
Insofar as these inform practice
But not if they are mainly relevant to building the academic field, deep knowledge of humanity for it’s own sake, etc. 
Address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.

Again, you may want to consider standard guidelines as well as The Unjournal’s emphases.",,,,Transparency and logic are of high level but the communication and structure are at times disjointed.,I didn’t not see a way to examine the data or code. Many of the procedures are well described but some are at a very broad level.,,I think the systematic review and meta-analysis (assuming the details in the prereg I could not verify are OK) can be published in a very good journal and my ratings are based on this. I am more hesitant and doubtful about the CEA component so do not mark this.,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",HLI-Review-Unjournal.docx,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research",True
A review of GiveWell’s discount rate,https://rethinkpriorities.org/wp-content/uploads/2023/11/Discountrate.pdf,"This form pertains to our applied and policy stream (see discussion of this stream here). We ask evaluators in this stream to: 
Write an evaluation. This may resemble a high-quality referee report for a standard journal (see standard guidelines), but geared towards the goals of our applied stream, and The Unjournal’s emphases. Please try to address any specific considerations mentioned in the our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Provide ‘claim identification and assessment’ if appropriate (see below)
Give quantitative metrics and predictions. As in our academic stream, this work will be evaluated for its credibility, usefulness, communication/logic, etc. However, for the applied stream, we do not need this to be assessed by the standards of academia in a way that yields a comparison to traditional journal tiers.
Answer a short questionnaire about your background and our processes.
Our full evaluation guidelines are here (for the ’academic-targeted’ stream; we’re building a version for the applied stream).

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. This is generally between $200 and $500 as a baseline, in addition to $150 per evaluation set aside for further incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. 


Note on this form: We’re trying the present Coda-based form for a few things. We have mainly used a PubPub based evaluation form, and may do so in the future.

“Pivotal questions” evaluators: Please try to substantially focus your evaluation on issues that are relevant to the identified Pivotal Question. You can discuss your beliefs for the focal PQ in the “Claim identification” boxes below, as well as in your report, and in the Metaculus interface (if we have shared one with you).

“Independent evaluators”:  We are exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate a paper), please see the explicit guidance  on how to use this form.",,What?,This is a useful review of discounting practices.,"Summary
 
This paper examines and affirms the logic behind GiveWell’s discount rate. It looks at the following issues: whether the discount rate should be based on the Ramsey formula (or similar) or on the opportunity cost of investing in the private market; the extent to which health benefits should be discounted at a lower rate than consumption benefits; projected consumption growth; the argument for a 0 pure rate of time preference; and temporal uncertainty.
 
The paper offers an impressive review and summary of discounting practices in other institutions, and suggests minor changes in the discounting practice of GiveWell (to guarantee consistency with the magnitude of the curvature of the utility function that it assumes elsewhere).
 
Assessment
 
I think that this is a useful review, and I was impressed by the philosophical nuance. Obviously, this report will be particularly useful for GiveWell, but there may be other practitioners interested in this type of analysis. I think that the accompanying data regarding the discount rates used by various organizations will also be useful for academics interested in discounting.
 
Comments
 
1.     I printed out the paper to read it, which might be old fashioned on my behalf, but it did serve to highlight that too much important content is delegated to hyperlinks. It would be useful to summarize some of the more crucial points in boxes or tables, to make the paper more self-contained (in particular, a table summarizing the spreadsheet of discount rates in a few key organizations would have been nice).
2.     To broaden the paper’s appeal beyond EA circles, it might be useful to provide more background about GiveWell’s mission and practices, and explain the reasoning behind the GiveWell-specific statements. For example, the statement that positive discounting for kinship is unlikely to apply in GiveWell’s context needs clarification about what that context is.
","Claim identification, assessment, and implications [PQ evaluators can skip this section]

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",90,80,100,89,82,100,90,100,79,43,36,51,96,91,100,100,100,100,100,100,100,"In our academic stream, we ask evaluators to consider “what journal ranking tier should this work be published in?” and “What journal ranking tier should this work be published in?”. As noted, most work in our *applied stream* will not be targeting academic journals. Still, in some cases it might make sense to make this comparison; e.g., if particular aspects of the work might be rewritten and submitted to academic journals, or if the work uses certain techniques that might be directly compared to academic work.  If you believe a comparison makes sense, please consider giving an assessment below, making reference to our usual guidelines for this here, and how you are interpreting them in this case.",,,,,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",,,"Feedback

Responses to the questions below here will not be public or seen by authors.

",,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper according to the specified criteria considering the qualities of this research relative to applied and policy research you have read aiming at a similar audience, and with similar goals. (If possible, please define the reference group you are using in the comments.) For example, choose 51% if you think this paper is better than 51% of such work, according to this criterion.  We define and discuss this in more detail here.

Criteria:  For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",GiveWell should change its social discount rate from 4% to 4.3%.,,"There is enormous uncertainty about the parameters, and the argument here is mainly one of consistency (that is, that they should apply the same parametric assumptions about utility curvature in the context of discounting as they do elsewhere). I have high confidence in the need for consistency, but low confidence that the parameter is correct","This is a conceptual question that is very hard to get at. We need to start by understanding what utility is, only then we can say something about how its curvature should be measured... (But again, this is not the point of this paper)","CIs: You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval.  For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.


","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to future impactful applied research, and practical relevance and usefulness.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?",Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? ,To what extent does the project contribute to improving the rigor and reliability of applied research and evidence-based policymaking in this area? Focus on ‘improvements that are actually helpful’ for global priorities and impactful interventions?,"Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners. Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
This may resemble a high-quality referee report for an academic journal. However, we have some particular requests:
Please put a strong focus on considerations that could inform practical users of the research and other applied researchers, directly or indirectly.  
We do want you to consider theoretical and technical concerns, leveraging your own technical expertise 
Insofar as these inform practice
But not if they are mainly relevant to building the academic field, deep knowledge of humanity for it’s own sake, etc. 
Address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.

Again, you may want to consider standard guidelines as well as The Unjournal’s emphases.",,,,,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research",True
The wellbeing cost-effectiveness of StrongMinds and Friendship Bench: Combining a systematic review and meta-analysis with charity-related data,https://www.happierlivesinstitute.org/wp-content/uploads/2024/11/HLI-The-wellbeing-cost-effectiveness-of-StrongMinds-and-Friendship-Bench-Combining-a-systematic-review-and-meta-analysis-with-charity-related-data-Nov-2024-Update.pdf,"This form pertains to our applied and policy stream (see discussion of this stream here). We ask evaluators in this stream to: 
Write an evaluation. This may resemble a high-quality referee report for a standard journal (see standard guidelines), but geared towards the goals of our applied stream, and The Unjournal’s emphases. Please try to address any specific considerations mentioned in the our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Provide ‘claim identification and assessment’ if appropriate (see below)
Give quantitative metrics and predictions. As in our academic stream, this work will be evaluated for its credibility, usefulness, communication/logic, etc. However, for the applied stream, we do not need this to be assessed by the standards of academia in a way that yields a comparison to traditional journal tiers.
Answer a short questionnaire about your background and our processes.
Our full evaluation guidelines are here (for the ’academic-targeted’ stream; we’re building a version for the applied stream).

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. This is generally between $200 and $500 as a baseline, in addition to $150 per evaluation set aside for further incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. 


Note on this form: We’re trying the present Coda-based form for a few things. We have mainly used a PubPub based evaluation form, and may do so in the future.

“Pivotal questions” evaluators: Please try to substantially focus your evaluation on issues that are relevant to the identified Pivotal Question. You can discuss your beliefs for the focal PQ in the “Claim identification” boxes below, as well as in your report, and in the Metaculus interface (if we have shared one with you).

“Independent evaluators”:  We are exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate a paper), please see the explicit guidance  on how to use this form.",,pseudonym,"The paper evaluates the wellbeing cost-effectiveness of two psychotherapy interventions in Sub-Saharan Africa, concluding they are significantly more effective than cash transfers. The methodology is comprehensive and transparent. It combines multiple data sources and conservative modelling assumptions. While robust and transparent, the evaluation relies on project-specific choices that may affect programme rankings. While the conclusions are largely convincing, the paper would benefit from clearer discussion of assumption impacts and standardised adjustment rules. Overall, this is a rigorous and valuable contribution.","The paper evaluates the wellbeing cost-effectiveness of two psychotherapy interventions in Sub-Saharan Africa, concluding they are cost-effective and 5 to 6 times more cost-effective than cash transfers.
For each programme, Friendship Bench and StrongMinds, the authors combine evidence from three main sources: a general meta-analysis of psychotherapy in LMICs, charity-related causal evidence (RCTs), and charity-related pre-post monitoring and evaluation data. They estimate the total effect of recipients over time, apply a series of internal and external validity adjustments, estimate household spillovers, and then aggregate the findings using informed subjective weights. The cost-effectiveness is calculated by dividing the estimated overall wellbeing effect by the cost per person treated.
Friendship Bench is estimated to be almost 25% more cost-effective than StrongMinds. StrongMinds is estimated to have over twice the effect of Friendship Bench on wellbeing, but it is also more than 2.5 times more expensive per recipient.
The paper answers an important question about comparative effectiveness of the work of different charities in improving wellbeing. The adoption of WELLBYs as a common outcome measure allows for a direct comparison between different types of interventions, e.g. comparing therapy to cash transfers, but also other interventions. The authors combine the evidence from a meta-analysis of therapy effectiveness in LMICs with programme-specific data, that allows balancing better-quality evidence and more relevant evidence. The main report and the extensive appendix are transparent regarding the methodology, data sources, and analytical choices. The authors acknowledge the limitations of the work, and the subjectivity involved in certain parts of their methodology, particularly the weighting of evidence sources.
My main concern is that there are many project-specific choices that the authors make for each evaluation. Some of them affect the coefficients significantly. Because it is a lengthy and detailed report, it is not clear how much each of these choices affects the final result. These are all perfectly justifiable modelling choices. However, given the complexity of the task and the level of uncertainty involved, one might imagine a different set of perfectly justifiable choices (more on this below), that might lead to a different ranking. To be clear, the authors are consistently choosing more conservative estimates across available options. If anything, the effects of the interventions are more likely to be underestimated rather than overestimated. However, I would treat the comparative ranking with caution, because of the level of uncertainty involved.
 
My comments:
1.    As a general point, it would have been helpful if the authors had clarified what type of evidence they would consider ‘ideal’ for the purposes of their comparative evaluation -- e.g. a quasi-experimental evaluation conducted in the field, an RCT of this programme or a similar one, or a combination of both. First, this could help motivate charities to design evaluations accordingly. Second, it might guide the authors in systematically weighting other sources based on their proximity to the preferred evidence.
2.    It would also make the claims more convincing if the authors provided a set of rules for the adjustments they are making and the list of these adjustments, which would be applied consistently across all evaluated projects. This seems particularly relevant given the group’s ambition extends beyond evaluating just these two programmes.
For example, the authors go through a series of external validity adjustments using various sources. To make these adjustments more systematic, they could include characteristics -- such as number of sessions (possibly as indicators for different durations), therapy type, whether the target group has a clinically diagnosable condition or is from the general population, and whether instructors are lay or professional -- as moderators in the meta-analysis. This would allow generating estimates that are best suited to each programme as a combination of coefficients. I appreciate that some of these coefficients may not be statistically significant; however, I see merit in relying on a single source, rather than selecting different sources for each adjustment.
3. Given that readers may be more or less convinced by certain adjustments, or may find them more or less relevant for their purposes, it would be helpful to provide a concise visual summary of how these adjustments affect the final estimates. I would suggest including an analogue of Figure 2 -- with numerical results for both interventions -- early in the paper. This could draw on a combination of Tables 4, 8, 11, 13, and the cost estimates. If possible, incorporating the uncertainty reported in Tables 20 and 21 would also be valuable. This would help readers understand what drives the differences in results. If a reader is unconvinced by a particular step, they could clearly see how excluding it would alter the findings.
Two examples of alternative choices that would significantly affect the results include:
a)    The authors include Baird et al. as an RCT related to the programme, yet they explain in detail that the implementation, target group, and other key characteristics differ. Given all the differences, it is not clear why this paper should not be included in the meta-analysis instead.
b)   The authors inflate the cost estimate for StrongMinds from $31 (based on the 2024 report) to $44.60. Without this adjustment -- or if it were only partially applied -- StrongMinds would appear more cost-effective than Friendship Bench.
4. The meta-analysis carries the largest (though not equal) weight in both evaluations. I have the following comments and questions regarding the meta-analysis:
a)    You include several outcomes, when available, from each study. How are these weighted? If they are not, it would mean that studies reporting more outcomes contribute disproportionately to the analysis. Is your result sensitive to this?
b)   You use mental health and wellbeing measures interchangeably. While this is acknowledged on page 57, it would be helpful to make this distinction earlier. Figures such as B3 and those that follow would be more informative if they clearly distinguished between mental health and wellbeing outcomes in colour coding. Although they are colour-coded, it is not clear what the colours represent. Alternatively, you could present summary statistics or meta-regression results both with and without the inclusion of mental health measures.
c)    Do you apply any exclusion criteria regarding the type of therapy? While it may be too restrictive to include only the therapies used by the charities in question, could you consider relying on studies that differentiate between interventions with higher or lower levels of evidence-based support?
 
Other comments:   
1.       The authors consider spillover effects for household members. As an observation, under the current modelling, all else being equal, the programme will be evaluated as more effective in countries with larger households.
2.       Because there is no PPP adjustment for costs, all else being equal, the programme will appear more effective in countries with lower purchasing power.
3.       I would expect a linear decay to zero to be a conservative assumption.
4.       For Table 3, could you include a histogram of effect sizes for each of the three groups? This would illustrate how the estimates shift when certain studies are excluded.
5.       On p.27, you state “See Appendix P for how much it influences the analysis (not much).” It would be helpful to quantify this more precisely -- for example, by indicating the percentage change.
6.       How do you estimate confidence intervals for the figures that are based on combinations of regression estimates (e.g. the final three rows of Table 4)?
7.       On page 58, you write: “Moreover, spillovers can be greater or lesser for one intervention: our previous working has found that cash transfers have a relatively bigger spillover effect than cash.” Could you please clarify what this means? 
8.       Regarding variance adjustments: I understand that initial response variance is lower among respondents above the clinical threshold compared to the general population. However, we would also expect the treatment effect to be larger in that group. How does this affect your analysis?

Thank you for your work.","Claim identification, assessment, and implications [PQ evaluators can skip this section]

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",85,75,95,85,75,95,90,95,80,90,85,95,85,80,90,90,85,95,95,90,97,"In our academic stream, we ask evaluators to consider “what journal ranking tier should this work be published in?” and “What journal ranking tier should this work be published in?”. As noted, most work in our *applied stream* will not be targeting academic journals. Still, in some cases it might make sense to make this comparison; e.g., if particular aspects of the work might be rewritten and submitted to academic journals, or if the work uses certain techniques that might be directly compared to academic work.  If you believe a comparison makes sense, please consider giving an assessment below, making reference to our usual guidelines for this here, and how you are interpreting them in this case.",4.2,3.5,,,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",Over 5 years,Over 20,"Feedback

Responses to the questions below here will not be public or seen by authors.

",Two days,,Yes,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper according to the specified criteria considering the qualities of this research relative to applied and policy research you have read aiming at a similar audience, and with similar goals. (If possible, please define the reference group you are using in the comments.) For example, choose 51% if you think this paper is better than 51% of such work, according to this criterion.  We define and discuss this in more detail here.

Criteria:  For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.","Claim 1: The paper evaluates cost-effectiveness, in terms of wellbeing, of two psychotherapy interventions in Sub-Saharan Africa. These are Friendship Bench and StrongMinds. The paper concludes that both interventions are cost-effective, in particular, 5-6 times more cost-effective than cash transfers, as evaluated by a meta-analysis by a group with the same first author:
* Friendship Bench has a cost-effectiveness of 49 WBp1k, or $21 per WELLBY.
* StrongMinds has a cost-effectiveness of 40 WBp1k, or $25 per WELLBY.
* GiveDirectly has a cost-effectiveness of 7.55 WBp1k (i.e., $132 per WELLBY) using a meta-analysis (McGuire et al., 2022a).
Claim 2: The authors also provide estimates of WELLBYs per $1,000 donated to each of the organisations running the interventions and find Friendship Bench to be almost 25% more effective than StrongMinds.
These claims are important because they address the question of how best to allocate resources to improve wellbeing in LMICs. By providing quantitative evidence suggesting that these psychotherapy interventions are significantly more cost-effective than interventions like cash transfers, the research makes a case for increased funding and prioritisation of mental health programmes in these settings. ",,"As discussed in my report, I find the first claim very convincing. I am less convinced by the second, as it could be reversed by using a different set of justifiable assumptions.","My detailed suggestions are provided in the main body of the review. 
My main concern is the number of arguably ad hoc adjustments based on different sources. I would be substantially more confident in the comparative cost-effectiveness estimates if the authors provided a clear set of rules for the adjustments they are making, along with a list of these adjustments to be applied consistently across all evaluated projects. 
Given the number of choices the authors need to make, it is unlikely that every reader will be convinced by all of them — though they are well justified. I believe the most effective course of action would be to ensure maximum transparency around how these choices impact the final estimates.","CIs: You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval.  For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.


","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to future impactful applied research, and practical relevance and usefulness.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?",Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? ,To what extent does the project contribute to improving the rigor and reliability of applied research and evidence-based policymaking in this area? Focus on ‘improvements that are actually helpful’ for global priorities and impactful interventions?,"Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners. Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
This may resemble a high-quality referee report for an academic journal. However, we have some particular requests:
Please put a strong focus on considerations that could inform practical users of the research and other applied researchers, directly or indirectly.  
We do want you to consider theoretical and technical concerns, leveraging your own technical expertise 
Insofar as these inform practice
But not if they are mainly relevant to building the academic field, deep knowledge of humanity for it’s own sake, etc. 
Address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.

Again, you may want to consider standard guidelines as well as The Unjournal’s emphases.",,,,,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research",True
The Returns to Science In the Presence of Technological Risks,https://www.dropbox.com/scl/fi/ft06c1kg1weqaeu6p0fbh/The-Returns-to-Science-In-the-Presence-of-Technological-Risks-v2.pdf?rlkey=3zl918t2p1fdtg92qo0b7aqap&e=1&dl=0,"This form pertains to our applied and policy stream (see discussion of this stream here). We ask evaluators in this stream to: 
Write an evaluation. This may resemble a high-quality referee report for a standard journal (see standard guidelines), but geared towards the goals of our applied stream, and The Unjournal’s emphases. Please try to address any specific considerations mentioned in the our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Provide ‘claim identification and assessment’ if appropriate (see below)
Give quantitative metrics and predictions. As in our academic stream, this work will be evaluated for its credibility, usefulness, communication/logic, etc. However, for the applied stream, we do not need this to be assessed by the standards of academia in a way that yields a comparison to traditional journal tiers.
Answer a short questionnaire about your background and our processes.
Our full evaluation guidelines are here (for the ’academic-targeted’ stream; we’re building a version for the applied stream).

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. This is generally between $200 and $500 as a baseline, in addition to $150 per evaluation set aside for further incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. 


Note on this form: We’re trying the present Coda-based form for a few things. We have mainly used a PubPub based evaluation form, and may do so in the future.

“Pivotal questions” evaluators: Please try to substantially focus your evaluation on issues that are relevant to the identified Pivotal Question. You can discuss your beliefs for the focal PQ in the “Claim identification” boxes below, as well as in your report, and in the Metaculus interface (if we have shared one with you).

“Independent evaluators”:  We are exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate a paper), please see the explicit guidance  on how to use this form.",Gregory Lewis,Liminal,"I was principally tasked with kicking the tires on the ‘biocatastophe’ parameters as this is my subject of expertise. They check out, and I can find little to criticise. 
Either I’m missing something, or the paper seems to be making a large mistake by comparing ‘all of the benefits’ of science on the one hand versus ‘only the technical risks of biocatastrophe’ on the other. Is AI risk the elephant in the room?
",https://docs.google.com/document/d/1wk3OtM5lC_E4Psyb6BdHaKezEszJr9i31f2FVprpRPQ,"Claim identification, assessment, and implications [PQ evaluators can skip this section]

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",71,27,87,79,16,90,75,88,33,81,56,96,88,64,95,89,76,99,65,18,89,"In our academic stream, we ask evaluators to consider “what journal ranking tier should this work be published in?” and “What journal ranking tier should this work be published in?”. As noted, most work in our *applied stream* will not be targeting academic journals. Still, in some cases it might make sense to make this comparison; e.g., if particular aspects of the work might be rewritten and submitted to academic journals, or if the work uses certain techniques that might be directly compared to academic work.  If you believe a comparison makes sense, please consider giving an assessment below, making reference to our usual guidelines for this here, and how you are interpreting them in this case.",,,,,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",9 years,~50,"Feedback

Responses to the questions below here will not be public or seen by authors.

",15 hours,"Pretty handy, although I I’d guess sometimes a reviewers core criticism will largely determine a lot of the fields.",Yes,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper according to the specified criteria considering the qualities of this research relative to applied and policy research you have read aiming at a similar audience, and with similar goals. (If possible, please define the reference group you are using in the comments.) For example, choose 51% if you think this paper is better than 51% of such work, according to this criterion.  We define and discuss this in more detail here.

Criteria:  For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,"Sure, although (again) I expect only worthwhile if I’m at least sort of “onto something” in my general critique (the bionumbers look basically fine).",,"(Besides patiently explain whatever dumb conceptual mistake I am making which is leading me to mistakenly think the paper is missing something obvious.)
A ‘like-for-like comparison of the returns of life sciences vs. the technological risk of biocatastrophe seems a more ‘like for like’ comparison. (See Coda).","CIs: You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval.  For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.


","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to future impactful applied research, and practical relevance and usefulness.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?",Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? ,To what extent does the project contribute to improving the rigor and reliability of applied research and evidence-based policymaking in this area? Focus on ‘improvements that are actually helpful’ for global priorities and impactful interventions?,"Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners. Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),A lot of the variance here (and for others) is governed by whether my “you can’t compare all the upsides to one of the downsides” critique actually has merit. ,"Evaluation report
This may resemble a high-quality referee report for an academic journal. However, we have some particular requests:
Please put a strong focus on considerations that could inform practical users of the research and other applied researchers, directly or indirectly.  
We do want you to consider theoretical and technical concerns, leveraging your own technical expertise 
Insofar as these inform practice
But not if they are mainly relevant to building the academic field, deep knowledge of humanity for it’s own sake, etc. 
Address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.

Again, you may want to consider standard guidelines as well as The Unjournal’s emphases.",,Methods clear and (conditional on underyling assumptions) reasonable and appropriate. Again my worry is the findings are predicated on inappropriate assumptions.,,"Generally clearly written and transparently reasoned. Use of benchmarking, breakevens etc. particularly helpful.",,"Again, a lot of the lower tail is owed to >10% credence from me on “this could be substantially misleading”.",,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.","I was principally tasked with kicking the tires on the ‘biocatastophe’ parameters as this is my subject of expertise. They check out, and I can find little to criticise. ","Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research",False
"Forecasting Existential Risks
Evidence from a Long-Run
Forecasting Tournament",,"This form pertains to our applied and policy stream (see discussion of this stream here). We ask evaluators in this stream to: 
Write an evaluation. This may resemble a high-quality referee report for a standard journal (see standard guidelines), but geared towards the goals of our applied stream, and The Unjournal’s emphases. Please try to address any specific considerations mentioned in the our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Provide ‘claim identification and assessment’ if appropriate (see below)
Give quantitative metrics and predictions. As in our academic stream, this work will be evaluated for its credibility, usefulness, communication/logic, etc. However, for the applied stream, we do not need this to be assessed by the standards of academia in a way that yields a comparison to traditional journal tiers.
Answer a short questionnaire about your background and our processes.
Our full evaluation guidelines are here (for the ’academic-targeted’ stream; we’re building a version for the applied stream).

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. This is generally between $200 and $500 as a baseline, in addition to $150 per evaluation set aside for further incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. 


Note on this form: We’re trying the present Coda-based form for a few things. We have mainly used a PubPub based evaluation form, and may do so in the future.

“Pivotal questions” evaluators: Please try to substantially focus your evaluation on issues that are relevant to the identified Pivotal Question. You can discuss your beliefs for the focal PQ in the “Claim identification” boxes below, as well as in your report, and in the Metaculus interface (if we have shared one with you).

“Independent evaluators”:  We are exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate a paper), please see the explicit guidance  on how to use this form.",,Rev Bayes,"This report considers a forecasting tournament in which experts in relevant domains and super-forecasters from previous tournaments are asked to predict a large number of quantities of interest related to existential threats to human existence on Earth. A third group, taken from the public more widely, is also consulted. The tournament structure involves several rounds with interaction within and between groups in online forums, sharing anonymised predictions and rationales, rewards for active individuals and assessment of the quality of predictions in terms of how well individuals could predict the values given by others and accuracy on quantities to be revealed over short time horizons.
As an exercise it is very interesting and could potentially be valuable, both to society in general and perhaps decision makers at various levels. It is written appropriately for a relatively non-technical audience, with most of the detail contained in the appendices. The methodology used is consistent with best practice in the literature for this type of tournament, although as the report points out there are question areas where they had to make decisions based on inconclusive research such as predictions for low probability events. Overall, I enjoyed reading the document and found that the summaries do for the most part match the data collected. I do think the report would benefit from revision, based on the comments in my full report
",,"Claim identification, assessment, and implications [PQ evaluators can skip this section]

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",80,,,,,,,,,,,,,,,,,,,,,"In our academic stream, we ask evaluators to consider “what journal ranking tier should this work be published in?” and “What journal ranking tier should this work be published in?”. As noted, most work in our *applied stream* will not be targeting academic journals. Still, in some cases it might make sense to make this comparison; e.g., if particular aspects of the work might be rewritten and submitted to academic journals, or if the work uses certain techniques that might be directly compared to academic work.  If you believe a comparison makes sense, please consider giving an assessment below, making reference to our usual guidelines for this here, and how you are interpreting them in this case.",,,,,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",MANAGER’S NOTE: the evaluator followed up with further rating information which we will incorporate,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",,,"Feedback

Responses to the questions below here will not be public or seen by authors.

",,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper according to the specified criteria considering the qualities of this research relative to applied and policy research you have read aiming at a similar audience, and with similar goals. (If possible, please define the reference group you are using in the comments.) For example, choose 51% if you think this paper is better than 51% of such work, according to this criterion.  We define and discuss this in more detail here.

Criteria:  For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,I would like to see more reporting of interquartile ranges in the report.,"CIs: You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval.  For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.


","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to future impactful applied research, and practical relevance and usefulness.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?",Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? ,To what extent does the project contribute to improving the rigor and reliability of applied research and evidence-based policymaking in this area? Focus on ‘improvements that are actually helpful’ for global priorities and impactful interventions?,"Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners. Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
This may resemble a high-quality referee report for an academic journal. However, we have some particular requests:
Please put a strong focus on considerations that could inform practical users of the research and other applied researchers, directly or indirectly.  
We do want you to consider theoretical and technical concerns, leveraging your own technical expertise 
Insofar as these inform practice
But not if they are mainly relevant to building the academic field, deep knowledge of humanity for it’s own sake, etc. 
Address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.

Again, you may want to consider standard guidelines as well as The Unjournal’s emphases.",,,,,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Unjournal 2024.docx,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research",False
