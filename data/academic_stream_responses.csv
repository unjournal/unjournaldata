Name of the paper or project,Link,About our evaluation process,Identification as an author,Code,Summary of Eval,Evaluation,"Claim identification, assessment, and implications",Overall assessment ranking,Lower Cl Overall assessment ranking,Upper Cl - Overall assessment ranking,Claims - midpoint ranking,Lower Cl - Claims,Upper Cl - Claims,Methods:  - midpoint ranking,Upper Cl - Methods,Lower Cl - Methods,Advancing Knowledge ranking,Lower Cl - Advancing Knowledge,Upper Cl - Advancing Knowledge,Logic & communication - ranking,Lower Cl - Logic & communication,Upper Cl - Logic & communication,"Open, collaborative, replicable - ranking","Lower Cl - Open, collaborative, replicable","Upper Cl - Open, collaborative, replicable","Relevance to global priorities, usefulness for practitioners - ranking",Lower Cl - Relevance,Upper Cl - Relevance,Journal ranking tiers,midpoint_normative_journal,midpoint_predicted_journal,lower_ci_normative_journal,upper_ci_normative_journal,lower_ci_predicted_journal,upper_ci_predicted_journal,Confidential comments,Confidential comments section,Survey questions,How long have you been in this field?,"How many proposals, papers, and projects have you evaluated/reviewed (for journals, grants, or other peer-review)?",COI tickbox,Approximately how long did you spend completing this evaluation?,How would you rate this template and process?,Would you be willing to consider evaluating a revised version of this work?,"Do you have any other suggestions or questions about this process or The Unjournal? (We will try to respond, and incorporate your suggestions.)",Metrics,Main claim,Independent evaluation,Confidence in claim,"What additional information, evidence, replication, or robustness check would make you substantially more (or less) confident in this claim?",ratings and CIs,Overall assessment: rating,claims_strength_characterization,"Methods: Justification, reasonableness, validity, robustness",Advancing knowledge and practice,Logic & communication,"Open, collaborative, replicable","Relevance to global priorities, usefulness for practitioners",normative_journal,Final section,Comments on Overall Assessment rating,_______________________,"Comments on ""Claims..."" rating (optional)","Comments on ""Methods..."" rating (optional) 2","Comments on ""Advancing knowledge & practice"" rating (optional) 3","Comments on ""Logic & Comms"" rating (optional)","Comments on ""Open... replicable"" rating (optional)","Comments on ""Relevance"" rating (optional)","Comments on ""Journal ranking tiers"" ratings and predictions (optional)",journal_predictiuon,attach_evaln,Implication of claim,eval_pool,Collaborative evaluation,COI explanation,coi_conditions,Feedback,Field/expertise,research_link_coda,pubpub_final_links,Journal publication title,status,Major critiques (identified by UJ) [~overlaps 'Key critiques' in research table],Major critiques classified,Date entered (includes transfer from PubPub),hours_spent_manual_impute
Adjusting for Scale-Use Heterogeneity in Self-Reported Well-Being,https://www.nber.org/papers/w31728,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Alberto Prati,Prati,"This is an extraordinary paper. It approaches a fundamental issue in wellbeing measurement, and does so constructively, by suggesting and testing a potential solution.",,"Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",95,90,100,95,90,100,95,100,90,95,90,100,95,89,100,95,90,100,86,74,95,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",5,4.8,5,5,4.5,5,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",About 10 years,Between 40 and 60,True,3 days,It would be good to be able to download the template and fill it up offline,"Maybe. I enjoyed doing it, but it took a lot of time.",,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,"From the current WP, it is not entirely clear how good the correction performs when only two CQ are used and when short SWB scales are used. I don't think these tests","For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),"Ironically, the paper shows that this type of assessment will change from a reviewer to another...!","Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,"I don't think the replication files are available yet,  but this publication is a work in progress.",,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",Report.pdf,,False,Not sure.,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",Behavioral economics and subjective wellbeing measurement,,,,,,,2025-11-14T12:20:49.405-05:00,
Adjusting for Scale-Use Heterogeneity in Self-Reported Well-Being,https://www.nber.org/papers/w31728,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Caspar Kaiser,Friedrich,This is a major methodological innovation in how we can adjust for differences in scale-use. The empirical component would especially benefit from more diverse and reliable samples.,,"Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",95,80,100,80,70,90,90,100,80,90,80,100,75,60,90,85,70,90,0,0,,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4.1,4,4.7,5,4.4,5,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

","1) Please note my comment on conflicts of interest. 
2) Before the review is made public, will the authors be able to view and respond? I would appreciate that.","Responses to these will be public unless you mention in your response that you want us to keep them private.

",Since about 2018,10+,True,8 hours approx.,"I am not sure if the confidence ratings were suitable to all questions for a work such as this, especially those on impact.",Yes,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.","There is new method to adjust for scale-use differences which can be implemented both with existing data containing vignettes, and with new data that require only few additional questions. ",,There certainly is a new method. I would be interested in seeing comparisons with existing methods.,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),I know few papers in this specific field from the last few years that are more important than this. ,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,"Again, these ratings are relative to field of subjective wellbeing research, not to work on global priorities in general. ","The reasoning itself, to the extent that I was able to assess it, was sound and very thorough. As noted in my review, the paper itself could have been more accessible.","The authors have shared the data. I was able to replicate the simplest version of their method using Stata (they were using R). I have not attempted to fully replicate the paper, so cannot comment on this.","I left this open because it seems hard to assess. In my mind this is a paper about a fundamental scientific question, which will eventually be of relevance to (policy) practitioners. However, the paper, at present, is not aimed at that.",,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
","review_scaleuse_heterogeneity.pdf,review_scaleuse_heterogeneity.zip",,True,Yes.,"None of the above applies. However it is worth noting that the field is small, that I had previous discussions with the authors about this and related papers, and I that would want to work with the authors in future projects. I tried not to let this affect my judgement, but it seems worth noting. ","Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",Methods for subjective wellbeing research,,,,,,,2025-11-13T03:31:12.687-05:00,
Does online fundraising increase charitable giving? A nationwide field experiment on Facebook,https://pubsonline.informs.org/doi/10.1287/mnsc.2020.00596,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",David Reiley,Feynman,"The experiment is a well-designed example of geographic randomization to measure the treatment effects of an advertising campaign.
The authors are demonstrate a marginally significant ATE.  They also demonstrate large, positive, significant spillover effects across geographically close postal codes, indicating that their ATE is a lower bound on the truth.  I would like to see more exploration of this result, because it is a surprisingly large effect.
I love that the authors measure the final outcome of actual donations made, instead of some intermediate proxy, because I am never confident that a proxy actually tells us what researchers want it to tell us about final outcomes.   But I think the authors might be more careful to present confidence intervals and point out how much statistical uncertainty they have in their estimates.  ","Link to Google Doc containing my evaluation:
https://docs.google.com/document/d/1XBHC-Bo9dMtuk3vvIAg82bJUSI-7fCD2ulTFfNoEjN8/
","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",90,85,95,85,80,90,90,94,85,90,80,95,93,90,98,92,88,95,90,80,94,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4.5,,3.9,4.7,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

","I was a referee on this paper for three rounds at Management Science.  I revealed this in one of my other answers above.  I don’t mind the authors knowing this about me, but I point this out in case you would prefer not to tell them this.","Responses to these will be public unless you mention in your response that you want us to keep them private.

","Eighteen years working on digital advertising, thirty years evangelizing for the use of field experiments in economics.",Probably around 200 over 30 years,True,Three to four full days,"It’s a little cumbersome.   Picking sliders on so many dimensions feels like it requires a lot of extra thinking on my part, and I’m not sure how useful all these dimensions of sliders are.","The paper is already published, so I can’t imagine the authors revising it again.  I’d consider reviewing a follow-up paper on the spillover effects.",,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.","To me, the most stunning result in the paper is the measurement of huge geographic spillovers in a geo-randomized advertising experiment on Facebook in Germany.  The primary research question was to ask the average treatment effect for an ad campaign on donations for the charity Save the Children.  The ATE was barely statistically significant, as I would expect from my years of working in this area, because these questions are hard to answer.  But the secondary question about spillovers shows an indirect effect (the effect of treating neighboring postal codes) that is ten times higher than the direct effect (the effect of treating one’s own postal code).  Though the randomization was set up to answer the primary question, it also creates a lot of randomness in the share of neighboring postal codes that were treated, and the authors exploit that variation to ask the spillover question.   Assuming it is true, this spillover effect is creating a large downward bias on the estimated main treatment effect.",,"I think the claim is probably true, but I worry that it may be exaggerated by the choice of functional form.   I cannot find any concrete reason I expect the claim to be false, but the spillover effects seem implausibly huge to me.  I believe the geographic randomization was likely done well, so I have very little doubt about the measurement of the main average treatment effect (a simple difference in means gives no room for specification searching), but the spillover result relies on a number of arbitrary assumptions, such as the distance over which spillovers can versus cannot occur, and the functional form through which we measure these spillover effects.   ","I would like to know whether the huge result is robust to different variations on the arbitrary assumptions chosen by the authors.  Differences in the distance threshold.  Allowing the spillover effect to vary continuously with distance, rather than being a step function that is “on” if the neighboring postal code is within 30km and “off” otherwise.  Modeling the spillover effect not just as proportional to the share of treated neighboring postal codes, but as proportional to the share of population that was treated in the neighboring postal codes.","For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),"I really appreciate seeing a study of advertising that looks at final consumer outcomes.  I think the authors have done a nice job.  My main complaint is that they oversell some of their statistically insignificant, and marginally significant, results.","Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.","My main complaint is in relying too much on point estimates, and not providing enough confidence-interval estimates for the more subtle results (beyond measurement of the overall average treatment effect).",The experimental design and analysis is well explained.,"The measurement of large spillover effects is really interesting.  In future work, I would like to see the authors go into more detail on this analysis, because the spillovers seem surprisingly high (average indirect effect nearly 10x larger than the direct effect).  I wonder how robust these results are to different choices of functional-form specification.  It might be fruitful to pursue the observed dominance of urban-to-rural spillovers. And if the indirect effects are really so much higher than the direct effects, I would love to know why that’s the case.  Can we find direct, corroborating evidence that a huge number of people in the sample live in rural postal codes but work in nearby urban postal codes, for example?  I could imagine writing an entire paper to explore this phenomenon and better understand it.","This paper is written much more clearly than the average academic paper.   I like to think that I helped the authors achieve this positive outcome, as I was a referee on the paper at Management Science.","I have not tried loading the data and running the authors’ code, but I verified that it existed on the Management Science website.   I also find that the experimental design was described clearly enough that I could try to replicate it, with sufficient budget, on Facebook.","These results help us understand more about how digital advertising really works, particularly in the realm of charitable fundraising.   Scholars learn important ideas from this paper about how to run more successful experiments to measure the effects of advertising, and charitable fundraisers learn ideas about whether Facebook advertising can be profitable as well as how they might improve their ad campaigns.",This has already been published at Management Science.,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,"If true, the spillover claim means that the Facebook ad campaign might be much more effective than what was measured in the main average treatment effect.   If this is a generally true phenomenon in digital advertising, it means that folks are generally underestimating the effects of advertising with geo-experiments, so ad campaigns might be a lot more effective (3x, 5x, 10x) than has been typically measured in advertising experiments.",True,I’m open to this.  But I think I’ve done a good job of writing a coherent report of my own.,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",Field experiments on the effects of advertising campaigns,Does online fundraising increase charitable giving? A nationwide field experiment on Facebook,https://unjournal.pubpub.org/pub/evalsumfundraisingcharitablegiving/,Management Science,"50_published evaluations (on PubPub, by Unjournal)",,,2025-09-28T18:50:07.715-04:00,32
Deadweight Losses or Gains from In-kind Transfers: Experimental Evidence,https://www.dropbox.com/scl/fi/q4ur1sx9p1wtzew7vxira/Manuscript-Cash_or_Kind-20May-2025.pdf?rlkey=o7fy9utxq7gfhpxk4wr58srit&dl=0,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",,Referee_01,"Innovative and policy relevant study, with strengths including careful piloting, repeated rounds, and local rice sourcing that avoids quality differences between donated and purchased food. Key limitations include the absence of a cash option equal to the market value of rice, raising the possibility that findings are partly explained by design choice. The lack of pre-registration or a pre-analysis plan, and reliance on weak proxies such as household headship, also weaken the bargaining power analysis.",,"Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",65,55,75,55,50,65,50,55,45,45,35,50,65,50,75,55,45,65,55,45,65,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",3,3.2,2,4,2.8,3.4,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",More than 10 years,Close to 100,True,One day,"It’s good. However, despite multiple attempts I wasn’t able to log in, and so I worry that my responses might be lost if there’s a glitch after I submit the form.",Yes,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.","The authors’ most important claim is that the average willingness to pay (WTP) for 5 kilograms of rice among poor urban women in India exceeds its market value of Rs 160, implying a negative deadweight loss (i.e., a deadweight gain) of about Rs 19 on average (Table 3). This claim is based on incentivized choice experiments, repeated across three rounds, in which respondents chose between rice and cash offers ranging from Rs 50 to Rs 500.
This is the central quantitative finding because it overturns the standard theoretical prediction that in-kind transfers should generate welfare losses relative to equivalent cash. If valid, it provides a novel beneficiary-side rationale for maintaining or expanding in-kind food programs, such as India’s Public Distribution System (PDS).",,"I assign relatively low confidence to the precise quantitative claim that average WTP exceeds the market value by about Rs 19. The design forces respondents with a true WTP of Rs 160 to be coded between Rs 150 and Rs 200, with midpoint coding at Rs 175. Since Table 1 shows that most switching occurs in this interval, the reported deadweight gain could largely reflect the discretization rather than true preferences. Moreover, the result does not seem to be robust across experimental rounds, further reducing confidence in its reliability.",,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",report.docx,,True,Possibly.,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",I conduct impact evaluations related to the effectiveness of social safety net programs and have studied cash vs food preferences in similar contexts to those studied in this paper.,Deadweight Losses or Gains from In-kind Transfers? Experimental Evidence from India,,WP,10_seeking_(more)_evaluators,,,2025-09-23T17:37:03.998-04:00,8
Irrigation Strengthens Climate Resilience: Long-term Evidence from Mali using Satellites and Surveys,https://academic.oup.com/pnasnexus/article/3/2/pgae022/7604271?login=false,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",N/A,Ag space adventurer,"One of the article’s strengths is offering a longer time horizon of analysis of smallholder irrigation, which gives insights beyond localized case studies or pilot plots. Another value is that the authors connect spatial data across water, agriculture and nutrition, to provide insight beyond water efficiency or yield gains. The analysis on longer-term livelihood outcomes may be beneficial for potential decision-making on irrigation policy. However, the article has short-comings in terms of causal factors and control areas for comparison. For example, is the differences in nutritional outcomes related to improved income, increased household consumption of own produce, changes in dietary behavior, market development, or other factors? Context-specific information on the causal pathways are critical for investment and program design. ","The paper holds value in advancing knowledge on the linkages between irrigation investments and nutritional (caloric) outcomes. The literature and the relevant datasets, particularly for the Global South, are scant. The authors used the tools available to push the discussion forward on a couple of very key issues in the West Africa (Sahelian) region. Correlations were observed with statistical significance between investments in irrigation and nutrition and child development and conflict incidents. The paper applies available  tools appropriately and makes efforts in multi-methods, interdisciplinary research. In this ways, the paper is a good contribution to the small but important literature on irrigation and nutrition.
That said, some claims and assumptions of the paper should be considered further. These include:
The images provided appear to show that the irrigation areas are fairly contained, i.e., there does not appear to be irrigated extensification around the project areas. Given that farmer-led irrigation is often undercounted, it is important to use the tools to try to identify the extent to which the irrigation area expanded by farmers’ own investments. This may or may not be significant for establishing causation of the nutrition and conflict outcomes.
The difference between pumped and flooded areas suggests that key causal factors are being missed. The differences are not sufficiently addressed and left without further analysis.
Explanations for the observed, statistical outcomes are largely socio-economic, but the analysis relies on some FGDs and broad surveys that are likely insufficient. The survey data used is not detailed for nutrition; the surveys tend to use recall on household caloric intake rather than important nutrition indicators, such as dietary diversity for different members of the household. The same surveys are also household level, which makes the conclusions related to intra-household relations curious. As the paper acknowledges, recall may be an issue, notably on the low reporting on illness.
Authors attribute household nutritional improvements to increased production, particularly of rice. The actual causal relationship is most likely more nuanced. More rice may support more income which may in turn support household purchase of foods for increased dietary diversity. This granularity of nutritional outcome will not be captured by the surveys used and therefore, requires the authors to make some guesses.
Factors that could be contributors in addition to irrigation infrastructure are not considered, such as added investments or programs in infrastructure, capacity, and education by the major irrigation development donor or government. Irrigation investment are often accompanied by programs that could also improve livelihoods.
Authors acknowledge spillover is possible but it is not clear how they address that without comparison or control groups. Generally, FGDs are not adequate to compensate for the lack of a control group, particularly when the findings being emphasized are statistical.
Explanations on reduced conflict seem unsubstantiated. The reasoning that follows correlation may be described as best guess. Other investments may have been implemented in the area that affected outcomes for conflict incidents. For example, irrigation scheme development often occurs in parallel with development of water points for sanitation and domestic use, engagement with and sometimes payments to local authorities, increased security of land use rights, and increased general security to ensure protection of infrastructure and production area. To narrow the explanation to the irrigation investment is to oversimplify what is most likely a nuanced causal relationship. Comparative or control cases would strengthen the validity of the conclusions, as would more in-depth social science beyond FGDs.
Despite some questions on the conclusions drawn by the authors, the paper nonetheless offers insights based on the correlations. The conclusions as they stand could be detrimental to future programs and practice without more nuance. ","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",46,0,,35,0,51,34,,0,32,,,40,,,70,,,30,,,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",2.5,3.2,,,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",15,25+,True,3 hours,Metrics with the intervals is confusing. ,The work is already published so it won’t be revised. ,The review process is confusing and not intuitive. The guidelines are helpful but take time to understand and apply. ,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",There is a correlation between an irrigation investment using pumped water in an area of Mali and improved nutritional status of children in the same area. ,,"The basic proposition is credible, but the assumptions about causal relationship are less convincing. ","The work would have benefited from a comparative or control, and from much more in-depth social science through a targeted nutritional survey (that considers other indicators of nutrition beyond caloric intake), in depth interviews, and project tracing to understand other investments and contextual factors that may contribute to causation. ","For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,"Noting that knowledge and practice should have separate ratings. This may contribute to the academic literature, but there are risks to practice from using the content without further nuance and critique. ",,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,"Irrigation usually - but not always - has a positive impact on nutrition, but can have negative impacts on water quality, health, gender relations, among others. In short, the design of irrigation schemes matters to outcomes. Even within the paper, one type of irrigation had little to no impact and one type had positive impact (correlation observed). Without understanding the multiple causal factors in this context, an irrigation investment may not have the same (assumed) positive impact. No conclusions can be drawn from this study that would enable quantification of nutritional improvements from another irrigation pumping scheme in rice. In some cases, such investments may lead to maladaptation, outmigration, inequality, and declining health. ",False,"No
",,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",small scale irrigation (social science),Irrigation Strengthens Climate Resilience: Long-term Evidence from Mali using Satellites and Surveys,https://unjournal.pubpub.org/pub/evalsumirrigationresilience/,,"50_published evaluations (on PubPub, by Unjournal)",,,2025-09-11T16:48:59.398-04:00,3
Maternal cash transfers for gender equity and child development: Experimental evidence from India,https://www.nber.org/system/files/working_papers/w32093/w32093.pdf,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Eleanor Tsai,N/A,"Weaver et al. evaluate the first randomized trial of UCTs in India, examining impacts on a battery of measures of household spending, consumption, nutrition, and child growth and development. Strengths include scale, comprehensiveness, and rigorous analysis of treatment effects and effect heterogeneity. Critiques center around interpretation of results in light of (1) findings from previous studies of cash transfers and integrated sanitation and nutrition interventions, and (2) the context of concurrent CCTs and health services targeting early childhood.",,"Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",85,80,90,90,80,100,85,90,80,95,90,100,80,75,85,90,80,100,95,90,100,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4.2,4.1,3.6,5,3.6,4.6,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",,,True,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,eleanortsai@berkeley.edu,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,"Data and code were not provided for this particular review, but likely will be after publication",,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",The Unjournal Evaluation Form.docx,,True,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Cash Transfers for Child Development: Experimental Evidence from India,https://unjournal.pubpub.org/pub/evalsummaternalcashtransfers/,,"50_published evaluations (on PubPub, by Unjournal)",,,2025-09-08T14:50:25.940-04:00,
Maternal cash transfers for gender equity and child development: Experimental evidence from India,,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",,Reviewer1,"This is a timely study that adds insight into whether and how unconditional cash transfers to mothers can improve their health and the health and development of their children in a low resource setting in India.  Major comments include the need to think through equitable authorship, further describe certain aspects of the intervention (i.e., messaging provided to participants), and add detail to the methods around the use of the Ages and Stages Questionnaire.",,"Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",70,65,75,85,80,90,60,62,58,90,89,91,85,85,85,30,20,40,88,84,92,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",3.1,2.9,2.9,3.3,2.1,3.1,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",10 years,>100,True,4 hrs,The sliding scales were difficult to use.,Yes,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.","The authors claim that unconditional cash transfers to mothers in India increase calorie intake, dietary diversity, and nutrient consumption for mothers and children, that they narrow gender disparities in food consumption, and improve early childhood development. These claims are stated in the paper’s abstract. These findings are important because there is current debate over the effectiveness of unconditional cash transfers on functional outcomes in women and children. The findings demonstrate a clear and significant benefit to outcomes that are meaningful to policy makers. ",,"Yes, I believe these claims are true as the data show.","As I wrote in my evaluation, I would like to see the estimates translated into more tangible and clinically meaningful conclusions. Statistical significance is different from meaningful effects and I am hoping the authors are able to demonstrate how the effects are indeed meaningful.","For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,"There is critical information about the intervention that would make it impossible to replicate the study. My score here reflects this opinion. As for replicating the analysis, I do think it would be possible.",,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",Unjournal review.docx,"As stated in my evaluation, I would like authors to be more clear in the meaning of their estimates, so that I could state something to the effect of: unconditional cash transfers to mothers in India, if scaled up, could results in an increase in X IQ points among young children living in resource limited settings.",True,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Cash Transfers for Child Development: Experimental Evidence from India,https://unjournal.pubpub.org/pub/evalsummaternalcashtransfers/,,"50_published evaluations (on PubPub, by Unjournal)",,,2025-08-14T16:59:07.526-04:00,4
"Ends versus Means: Kantians, Utilitarians, and Moral Decisions",https://www.nber.org/system/files/working_papers/w32073/w32073.pdf,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Valerio Capraro,ValerioCapraro,"I greatly appreciated the Ends vs Means paper, especially its effort to bring moral dilemmas into economics. I see two main limitations in the current version of the paper: (1) the Save a Life dilemma may mischaracterize Kant’s categorical imperative; (2) the finding of unstable deontological preferences may depend on the dilemmas used. I hope these issues will be resolved, as I think this paper has high potential. ","I have genuinely enjoyed the Ends vs Means paper by Bénabou, Falk, and Henkel. I am really glad that moral dilemmas between deontology and consequentialism are beginning to attract the interest of economists. I think this was an important aspect that was missing in the economic modeling of human behavior. From this perspective, my expectation (and perhaps also my hope) is that this study will help bring moral reasoning more fully into mainstream economic discourse.
At the same time, I have identified a few shortcomings that I hope can be addressed either in a revision of the paper or in future research. Below, I outline the three main points that emerged as I read.
1) My main comment regards the Save a Life dilemma. As the authors correctly note, this decision is essentially equivalent to a standard switch-track trolley problem. However, I believe the underlying deontological principle differs from Kant’s categorical imperative of not using humans solely as a means to an end. Rather, it aligns more closely with Aquinas's ""doctrine of double effect"", according to which causing harm as a side effect can be morally acceptable if that is not the intended effect. Everett, Pizarro, and Crockett (2016) have a thorough discussion about the different principles at play in various trolley problems. To isolate Kant's imperative, one can use the trapdoor dilemma, where the moral dilemma involves opening an electric trapdoor via a remote control, through which a large man gets thrown on the tracks. I recognize that it is hard to build an incentivized version of this dilemma, but this is ultimately the reason why moral psychologists are sticking to hypothetical scenarios: some of the most interesting cases are really hard, and perhaps impossible, to incentivize.
2) My second comment regards the result that there does not seem to be stable deontological preferences in the authors’ dataset. I was wondering how generalizable this is and whether this depends on the particular dilemmas considered. For instance, if we were to consider personal moral dilemmas in which one needs to directly harm someone to save a greater number of people (think of the footbridge dilemma, where one has to decide whether to push a large man off the bridge to stop the trolley), I would intuitively say that deontological preferences would be much more stable in this case, as they would be highly correlated with relatively stable psychological traits like reliance on intuition or reliance on deliberation. In this regard, I think one important point to highlight is that deontological preferences represent a class of preferences, and not a single type, depending on the underlying deontological moral rule(s). There are many deontological rules and the fact that preferences over one or some of these rules are not stable does not imply that preferences over each of these rules are unstable. It is possible that preference for not inflicting direct harm, or for not using humans solely as means to an end, are more stable.
3) My final comment regards the related literature. The authors’ results are fascinating, however, some aspects are not entirely novel. In particular, I felt personally reassured that the authors did not find correlations between deontological decisions and prosociality in economic games. This extends a result of mine, where I found no significant correlations between deontological decisions in the trapdoor dilemma and prosocial decisions in the dictator and trust games (Capraro et al., 2018). I have also appreciated the relationship between various forms of prosociality and the reference to the brilliant work of Chapman et al. However, there is an earlier paper by Peysakhovich, Nowak, and Rand (2014) making a very similar point, which I think should be mentioned. There are also two experimental papers showing a positive correlation between not telling Pareto white lies and giving in the dictator game and cooperating in the prisoner's dilemma. These results are very hard to explain using consequentialist preferences and can be seen as a first attempt at showing the existence of deontological preferences using incentivized decisions (Biziou-van-Pol et al., 2015; Cappelen et al., 2013). Finally, I would like to mention a JEL review of moral preferences models, touching on many of the points that the authors make in their article (Capraro, Halpern & Perc, 2024).

References
Biziou-van-Pol, L., Haenen, J., Novaro, A., Liberman, A. O., & Capraro, V. (2015). Does telling white lies signal pro-social preferences?. Judgment and Decision Making, 10(6), 538-548.
Cappelen, A. W., Sørensen, E. Ø., & Tungodden, B. (2013). When do we lie?. Journal of Economic Behavior & Organization, 93, 258-265.
Capraro, V., Halpern, J. Y., & Perc, M. (2024). From outcome-based to language-based preferences. Journal of Economic Literature, 62(1), 115-154.
Capraro, V., Sippel, J., Zhao, B., Hornischer, L., Savary, M., Terzopoulou, Z., ... & Griffioen, S. F. (2018). People making deontological judgments in the Trapdoor dilemma are perceived to be more prosocial in economic games than they actually are. PLoS One, 13(10), e0205066.
Everett, J. A., Pizarro, D. A., & Crockett, M. J. (2016). Inference of trustworthiness from intuitive moral judgments. Journal of Experimental Psychology: General, 145(6), 772.
Peysakhovich, A., Nowak, M. A., & Rand, D. G. (2014). Humans display a ‘cooperative phenotype’ that is domain general and temporally stable. Nature communications, 5(1), 4939.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",85,75,95,80,70,90,80,90,70,95,93,97,90,83,97,95,92,98,85,75,95,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4.2,4.5,3.7,4.7,4.2,4.8,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",10 years,100+,True,4 hours,90/100,yes,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,True,no,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
","moral psychology, behavioural economics","Ends versus Means: Kantians, Utilitarians, and Moral Decisions ",https://unjournal.pubpub.org/pub/evalsumpopintuitions/release/5,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-07-29T05:46:21.323-04:00,4
Asymmetry in Civic Information: An Experiment on Tax Participation among Informal Firms in Togo,,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",,Evelyn Hervey,"This paper reports an RCT of a taxpayer education campaign in Togo, showing increased tax knowledge, a shift in the distribution of tax payments, and higher reported revenues. The reviewer finds the study valuable in its focus on small firms and the large tax knowledge effects. It offers suggestions concerning measurement bias due to enumerator involvement in the intervention, accountability measures, and interpretation of payment effects.","Unjournal Review
This paper studies a randomized taxpayer education campaign within a sample of 383 small firms in Lomé, Togo. The treatment includes modules on tax code information as well as information about the types of public goods that tax revenues should be spent on. They find that the campaign increases tax knowledge substantially. They also find evidence that the campaign decreases tax payments on the extensive margin but increases the average amount paid. They also find increases in reported orders and revenues. They rationalize these findings in a simple model in which greater bargaining power among firms leads to fewer (slightly larger) firms paying taxes but those payers actually paying more.
This is an interesting paper. It makes a nice contribution to the literature by (a) studying taxpayer education, and providing to my knowledge the first RCT on this topic, and (b) studying small firms, which are difficult to study in the field but of great interest to tax authorities in developing countries. The large increase in taxpayer knowledge is striking: there are clearly massive information frictions facing small firms in this setting. The effects on tax payments are interesting, though I have some questions about their interpretation (see below). I also found it very interesting that providing this information (likely the section about public goods) leads firms to update negatively about the performance of the government.
I have a few comments on the current version.
First, as I understand it, the intervention was delivered by the same enumerators who conducted the baseline and endline surveys. This is a bit concerning because it introduces the possibility of endline reporting varying because respondents have greater knowledge of and trust in the enumeration team. While this does not concern validated measures like tax payment—which relies on showing a receipt—it does make me a bit more wary of interpreting reports of economic activity. I also think it leads to an exclusion restriction violation for the 2SLS strategy, which currently assumes the treatment can only operate through the channel of improving knowledge. Can the authors test this in some way in the data, i.e., by looking at proxies for familiarity with or trust in the enumerator? This leads me to think the authors might consider dropping the 2SLS and replacing it with correlational analysis between knowledge and tax payments.
Second, I wonder if it is wise to emphasize the fiscal contract and accountability as much here given that the measure of accountability is, in my mind, quite indirect and a bit difficult to interpret as accountability. Here is the relevant section: “The second indicator is whether the firm specifies an opinion about whether taxes are being put to good use. Firms can answer “Yes”, “No” or “I don’t know.” The variable then indicates the firms that do not answer “I don’t know” since firms with an opinion are more likely to view taxation as a fiscal contract (Accountability).” Firm respondents could answer “I don’t know” for many reasons. Similarly, is a “yes” here really consistent with firms demanding accountability? I was not sure. I am assuming that the level of public goods provided remains low. This intuition is supported by Table 10, which shows a negative effect of the treatment on a question about government performance. Is this the same question? It seems perhaps closer to accountability, but I did not see Table 10 referred to in the text. Yet, I found this table quite interesting. It appears that informing taxpayers what public goods should come from their taxes leads them to lower their views of government. That seems plausible and quite interesting and relevant for the design of future taxpayer education campaigns. In particular, it suggests that designers of information campaigns must think carefully about taxpayers’ potentially high expectations about public goods. On this, see van den Boogaard et al “Enabling Tax Bargaining: Supporting More Meaningful Tax Transparency and Taxpayer Engagement in Ghana and Sierra Leone” 2025. I would bring Table 10 more closely into the analysis.
Third, on the tax payment results, I would have appreciated further details in the analysis. In particular, I had several questions while reading this section:
a)     These firms are only responsible for paying TPU if I understood well. It seems the treatment causes firms to realize the tax they pay is the TPU. But I was not clear if we can interpret this reduction in payment as (smaller) firms realizing that they are exempt from other non-TPU taxes and stopping paying those? That seems the most natural interpretation, and it would help the manuscript to make this a bit clearer.
b)    Alternatively, the authors find that “treated firms are 19 percentage points more likely to report that the hypothetical firm would have declared that they had not earned any revenue” This is another hypothesis: smaller firms learn that they can self-declare a zero. This could be a reason for negative effects. Why is there not a table looking at the effect of treatment when self-declaring a zero?
c)     The receipt-based measure of tax compliance is nice, however it would help to discuss (i) whether this only includes TPU receipts, (ii) how receipts were validated – do they have photographs or are they relying on enumerator perceptions?
Fourth, for the economic activity results, is it possible that by better understanding their requirements for paying TPU, firms simply pay more attention to their orders and revenue, i.e. by improving their book-keeping? That seems like a plausible explanation of the results, and I was wondering if the authors could test it with the survey data.
Fifth, treatment information: I found the information provided about the content of the taxpayer information intervention to be limited. All we get in Figure 1 concerns a tax on advertising that does not come up much in the manuscript (which seems to focus more on TPU). It would be helpful to provide a more complete summary (ideally translated) of the information contained in the intervention.
Sixth, power: an obvious concern is the small sample size, particularly when the authors cut the same to look at heterogeneity. However, I commend the authors for being transparent about their power in the manuscript. It might be nice to do the same thing for the sample splits considered in the heterogeneity analysis (Figures 7-8).
Seventh, I like the theoretical framework. The intuition it provides seems well suited for the context. One small question in terms of how Proposition 1 maps to the data, however, is whether tax collectors were sufficiently aware of the intervention to change their audit targeting strategies? It seems the variation in firm size within the sample is small and would be hard for tax inspectors to observe and adjust their behavior accordingly?
Eighth, “While the treatment itself could affect the likelihood of a visit, the patterns that we observe are still instructive.” Can’t they test whether the treatment affected the likelihood of a visit directly?","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",75,65,85,,,,,,,,,,,,,,,,,,,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",3.5,,,,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",,,True,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Asymmetry in Civic Information: An Experiment on Tax Participation among Informal Firms in Togo,https://unjournal.pubpub.org/pub/evalsumcivicasymmetry/draft?access=p76ghuo8,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-07-24T17:35:56.610-04:00,
"A systematic review and meta-analysis of social safety nets, women’s economic achievements and agency",,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Rachel Sabates-Wheeler ,wheeler,"The paper helpfully reviews the impact of social safety nets (SSNs) on gender inequality. It highlights that SSNs can enhance women's economic achievements and agency, particularly through increased labour force participation and decision-making power. However, the effects on care work intensity are not significant. The cost-benefit analysis is limited due to a lack of comprehensive data. The paper is an excellent resource, but needs some improvements on policy suggestions.",,"Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",77,71,80,81,74,85,82,90,76,72,68,77,83,80,86,91,85,94,68,57,72,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",3.6,3.7,3.3,4.5,3.4,3.8,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",27 years,100 +,True,3.5 hours,i found it very heavy in terms of the number of questions being asked.  It also took me a long time to understand the ranking process.  It would eb good if you gave a couple of worked examples.,yes,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.","The authors that there is sufficient robust quantitative evidence to support the claim  that social safety nets (SSNs) have the potential to address gender inequality by enhancing women's economic achievements (specifically  labour force participation and productive work intensity) and agency. The same is not true for other indicators such as impacts on voice, autonomy, and care work intensity.
",,"I believe this claim fully as within my own work on social protection in lower income countries over the past 20 years, it is clear that SSNs have the ability to support and promote productive and growth impacts.  the frequent myth that SSNs lead to dependency and laziness is misguided i, especially in lower income countries, an this evidence support my long held belief. That said, this is an aggregate/average result...that needs to be contextualised ",,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",The Unjournal.docx,,False,"no,  that would take alot of time",,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",Development Studies/ Agricultural Economist,A systematic review and meta-analysis of the impact of cash transfers on subjective well-being and mental health in low- and middle-income countries,,,901_PQ_candidate_eval,,,2025-07-22T06:12:21.331-04:00,3.5
Meaningfully reducing consumption of meat and animal products is an unsolved problem: A meta-analysis,https://osf.io/preprints/osf/q6xyr_v1,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Matthew B. Jané,MetaGuy,"Strengths: The study is highly transparent, with fully reproducible code and open data. The analytic pipeline is well-documented and is an excellent example of open science.

Limitations: However, major methodological issues undermine the study's validity. These include improper missing data handling, unnecessary exclusion of small studies, extensive guessing in effect size coding, lacking a serious risk-of-bias assessment, and excluding all-but-one outcome per study. Overall, the transparency is strong, but the underlying analytic quality is limited.",,"Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",39,14,62,59,30,85,25,43,10,50,5,95,40,19,65,91,86,100,88,45,93,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",1.1,3.2,0.4,1.9,2.3,4.1,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",Sorry for the delay in this evaluation!,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",4 years,>10,False,"few days, probably 15 hours total",two enthusiastic thumbs up,Depends on my dissertation progress but I would consider it,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.","The main claim of the study is: “We conclude that while existing approaches do not provide a proven remedy to MAP consumption, designs and measurement strategies have generally been improving over time, and many promising interventions await rigorous evaluation.”
The claim is based on the overall mean effect of SMD = 0.07 [0.02, 0.12]. The claim is central to practitioners, because they are trying to evaluate interventions to reduce MAP consumption and based on the evidence the authors are concluding that there does not seem to be any clear way of doing so at the moment.",,"I agree with the claim because this is all I have read on this topic and it does not appear that the evidence of effectiveness (or ineffectiveness) is very compelling. Outside of this paper I have no knowledge of this field, so based on this paper alone the evidence is not convincing of an effective intervention.","Obtain all relevant outcomes and studies (not filtered by sample size <25). Work with a librarian to make a supplementary systematic search to ensure good coverage. Use proper missing data methods for non-significant and unreported effects (R package called metansue does this exact thing). Add a rigorous risk-of-bias assessment that includes glaring issues such as effect size approximations, outcome relevance, attrition bias, selective outcome reporting, etc.","For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",Evaluation.docx,,False,Depends on my dissertation progress but I would consider it,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",Meta-analysis,Meaningfully reducing consumption of meat and animal products is an unsolved problem: A meta-analysis,https://unjournal.pubpub.org/pub/evalsumreducingconsumption/draft?access=l28mitd9,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-07-20T01:18:44.795-04:00,15
Population ethical intuitions,https://www.sciencedirect.com/science/article/pii/S0010027721003644?via%3Dihub,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",,Rufton,"This research provides valuable empirical insights into population ethics intuitions through well-designed experiments. The methodological progression from simple to complex scenarios demonstrate important patterns in moral reasoning. While the utilitarian framework dramatically overlooks crucial dimensions of human flourishing, reducing the validity of these studies, the data show how ordinary people resist purely arithmetic approaches to population welfare.",,"Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",65,60,80,60,50,70,70,80,50,30,20,50,60,50,75,90,80,100,40,30,50,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4.1,4,3.4,4.4,4,4,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",5 years,20,True,20 hours,Good!,N/A,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.","“Next, a one-sample t-test against the midpoint 4 revealed that participants on average judged it as an improvement to add one neutral person into the world, (M = 4.23, SD = 0.67), t(156) = 4.40, p < .001, d = 0.35. This suggests the existence of a weak general preference to create a new person, even if their happiness level is neutral.” (p. 9).

My report states why I believe this claim is important. It could induce effective altruists to take increasing the birth rate more seriously.",,"Credible Interval: [0.8, 1]",Study 2a should be globally replicated; whether a new neutral person is seen as beneficial to this world should be correlated with life conditions in each country that is studied.,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),"Experiments are well-designed and minimal, though sample selection is an issue","Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.","If one accepts the utilitarian framework, the questions, methods and analyses generally make sense, though heterogeneity is missed",,The is-ought problem limits the application of this paper to cases where normative judgments can be informed by population-level empirical parameters,,,,The paper has already been published in a top field journal.,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",evaluation.md,,True,Sure,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",Experimental economics,Population ethical intuitions.,https://unjournal.pubpub.org/pub/evalsumpopintuitions/,Cognition,"50_published evaluations (on PubPub, by Unjournal)",,,2025-07-16T13:28:02.501-04:00,20
Misperceptions and Demand for Democracy under Authoritarianism,https://economics.mit.edu/sites/default/files/2024-09/Misperceptions%20and%20Demand%20for%20Democracy%20under%20Authoritarianism_0.pdf,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",,Anonymous Reviewer,"This paper makes a valuable contribution to research on authoritarianism and democratic resilience by combining an online survey and a large-scale field experiment in Turkey to test whether correcting factual misperceptions about institutional decline can shift political attitudes and electoral behavior. The study is theoretically ambitious, well-powered, and methodologically transparent, with the field experiment demonstrating real-world behavioral effects on opposition vote share. The paper could be improved by addressing the mismatch between the authors’ normative framing of democracy and media freedom and the more instrumental, performance-based content of their treatments. The authors could also do more to address concerns about treatment spillover and compliance heterogeneity in the field experiment. Overall, the study is rigorous and policy-relevant, and it offers a strong foundation for future work on information-based interventions in hybrid regimes.",,"Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",88,80,95,83,75,90,88,95,80,88,80,95,88,80,95,83,75,90,88,80,95,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4.3,4.3,3.5,5,3.5,5,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",10 years ,~200,True,1.5 work days,9.5/10,yes,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.","The most important and impactful factual claim made in this paper is that exposure to nonpartisan, factual messages about institutional decline led to a measurable increase in opposition vote share in the 2023 Turkish presidential election. Specifically, the authors report that treated neighborhoods experienced a 2.4 percentage point increase in opposition vote share, which represents a 4.4% increase relative to baseline support levels. This estimate is based on administrative data from 1,887 ballot boxes across 554 neighborhoods in Izmir, comparing treatment and control areas using a 2SLS design that instruments actual treatment exposure (measured by completed household visits) with randomized treatment assignment.
This claim is impactful because it provides novel real-world evidence that factual, nonpartisan informational interventions can meaningfully influence electoral behavior in a hybrid regime. It moves beyond attitudinal outcomes, demonstrating that even modest corrections of misperceptions can scale to population-level political effects. This is an important finding for policymakers and scholars concerned with democratic backsliding and information resilience under authoritarian conditions.
",,"I am inclined to believe this claim as the field experiment is well-powered, pre-registered, and analyzed using appropriate instrumental variable (2SLS) methods, which lends credibility to the causal inference. The use of administrative ballot-box data reduces concerns about self-report bias, and the authors show that the result is robust across multiple specifications. I am somewhat cautious due to 1) potential spillover across adjacent neighborhoods; 2) variation in treatment compliance and delivery success; 3) uncertainty about the underlying mechanism as messages focused on performance outcomes rather than normative democratic appeals.
","Spillover Analysis: Given the risk of treatment, conducting spatial spillover checks would help validate the localized nature of the treatment effect. For instance, the authors could re-estimate treatment effects excluding neighborhoods adjacent to treated areas or include spatial lags of treatment assignment as covariates. Tools such as spatial autoregressive models or geographically weighted regressions (e.g., using R packages like spdep or spatialreg) could help model and quantify potential diffusion effects.
CACE Estimation: The 2SLS approach accounts for partial compliance, but presenting CACE estimates would clarify treatment effects among those who received the intervention. This would be especially helpful given variation in completion rates by canvasser affiliation. CACE can be estimated using the same instrumental variable approach already implemented, but with a focus on interpreting the LATE for compliers. Reporting these estimates alongside covariate profiles of compliers (households that opened doors or received pamphlets) would improve transparency.
 ","For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.","While the evidence is credible, the authors’ claims regarding “demand for democracy” are quite broad and normative despite the narrower more instrumental nature of their treatments. ",,,,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",unjournal_review_final.docx,"The central implication of the paper’s main claim is that nonpartisan, factual messaging about institutional decline can meaningfully shift electoral outcomes even under authoritarian or hybrid regimes, suggesting that targeted information campaigns may be a cost-effective strategy for bolstering democratic opposition and civic engagement. If this effect generalizes beyond Turkey, it implies that donors, pro-democracy NGOs, and election-monitoring organizations should invest in informational interventions that highlight the performance consequences of democratic erosion, especially in settings where direct normative appeals to democracy may be less persuasive or politically risky.
I believe this implication, especially in contexts where political competition still exists and where the ruling regime has made clear governance failures. However, generalizability may be limited. The effect likely depends on contextual factors such as, salience of recent scandals or disasters, and audience receptivity to nonpartisan messengers.
 ",True,sure! ,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",Political Science; Authoritarian Politics; MENA Politics; Experimental Methods ,Misperceptions and Demand for Democracy under Authoritarianism,,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-07-13T18:10:00.693-04:00,12
Artificial Intelligence and Economic Growth,http://dx.doi.org/10.3386/w23928,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Philip Trammell,,,"This piece is the chapter on AI and economic growth in Agrawal et al.’s 2019 Economics of Artificial Intelligence: An Agenda. In introducing their chapter, Aghion et al. (2018) write that their “primary goal” with it “is to help shape an agenda for future research.” In total, the piece seems to have three goals. First: section 2 contributes to the theory of bread-and-butter automation and industrial growth, supported in part by empirical observations presented in section 6. Second: sections 3 and 4 contribute to the theory of AI and economic growth, in the setting of an R&D-based growth model. (An appendix does so in the setting of a Schumpeterian growth model.) Finally: section 5 informally discusses the implications of AI for growth within models that give firm incentives a central role, and topics for future research in this area.
It is a shame that the authors felt compelled to pack so much in. Each of these three components could easily have generated an excellent piece of its own. Indeed, in my judgment, both of the paper’s original contributions far outshine its commentary on future research directions. Some compression of this kind was probably warranted in context, given the rest of the Agenda’s relative neglect of growth, and how much on the subject there is to say. Nevertheless, the result is a document that both abounds with a truly remarkable array of important new insights about AI and growth, and has somewhat more than the usual share of mistakes and awkward inclusions or omissions.
The outright mistakes are perhaps the more minor flaws, since they are easily corrected on a close reading. Indeed, the PDF on one author’s website when this review was being written already corrected five, in red, from the version published in the Agenda. Writing this review uncovered five more (now incorporated in a further edited PDF, mostly in blue). Of course, some mistakes are understandable, and none so far identified overturn the paper’s central conclusions. Still, they make it harder for a reader to trust any results he has not checked.
The greater flaws, in my view, are the scattered inclusions and important-seeming omissions. As discussed further below, furthermore, these decisions on both counts tend to steer the paper away from scenarios in which AI produces a departure from the “Kaldor Facts” of constant growth rates and factor shares.
The body of the paper opens by exploring how AI might come to replace human labor in every task yet fail to produce any break in economic trends. It does so by introducing in section 2 a simple model in which, over time, asymptotically 100% of tasks are automated, yet the stylized facts of historical growth all asymptotically obtain. In particular, the model asymptotically yields a constant and positive labor share, growth rate, level of capital-augmenting technology, and growth rate in labor-augmenting technology.
Though the model is presented as a baseline from which to explore AI and growth further, it is a brilliant insight on its own. Uzawa’s (1961) Theorem teaches us that to match the broad strokes of industrialized growth, all technology growth can—and sometimes must—be modeled as labor-augmenting. This result offers a valuable guide to closed-form modeling, but no intuition about how technology develops “under the hood”. The image it most directly invokes—of workers buzzing about their work ever faster, and capital accumulating unchanged beside them—is absurd. But more realistic models, in which technological progress consists primarily in the creation of more capable machinery (and leaves workers’ flesh and bones largely untouched), had proved difficult to reconcile with the stylized facts above. Zeira’s (1998) model of automation, for instance, predicts an ever-increasing growth rate and capital share. For offering such an elegant, tractable, and intuitive reconciliation of automation with the stylized facts, I would say that the model of section 2 deserves a place in all but the most elementary introductions to growth theory.
Its quality as a contribution to the theory of historical growth, in turn, strengthens it as a contribution to the theory of growth under AI. The insight that, in the long run, an arbitrarily high fraction of human jobs may be automated without changing the labor share or growth rate is valuable, and at odds with much of the public conversation around automation and work. But after reflecting briefly on the implications of low substitutability across tasks, it is not very surprising that one can write down some model in which this occurs. The surprise, at least to me, is that arguably the most reasonable stylized account of historical automation to date turns out to be just such a model. This observation constitutes a powerful argument for the classic view that, for the foreseeable future, AI advances will amount only to “more of the same”.
The case for this model, or at least this view, is bolstered by the observation in section 6 that in industries with more automation, labor productivity rises but not the capital share.
Having delivered this excellent contribution, section 2 closes with a rather ad hoc simulation in which automation proceeds not continuously but on and off in 30-year spurts. The simulation reveals that exogenous fluctuations in automation can produce fluctuations in growth rates and factor shares, and can generate a capital share that rises, falls, or stays constant over the longer run. The motivation for this flourish is evidently that the simple model generates constant growth and a capital share that rises over time (albeit asymptotically, to a value below 1), whereas the received wisdom is that growth rates and factor shares fluctuate but exhibit no trend at all.
On its own, the fact that fluctuations in make fluctuations out is no surprise. Moreover, the fact that the simpler model produces an asymptotically rising capital share is to my mind not a weakness but yet another strength. The capital share has risen over time, both recently and over the longer run, as documented by e.g. Piketty (2014). This trend has coincided with a rise in the capital-to-output ratio: a coincidence that, given a conventional CES production function, would imply that labor and capital are already gross substitutes.
Piketty famously accepts this conclusion, despite extensive evidence against it from other domains, and makes it the cornerstone of his policy agenda. The Aghion et al. (2018) model of automation, meanwhile, departing only slightly from conventional CES, manages to reconcile the evidence of a historically (but not boundlessly) rising capital share with the evidence that labor and capital are still gross complements. Any reflections on the significance of this reconciliation, however, are seemingly crowded out of the paper by an awkward model-tweak that eliminates the reconciliation so as to hew to the “stylized facts” even more closely.
With this foundation, sections 3 and 4 explore conditions under which even more thorough automation does produce more extreme consequences. In particular, it explores a Jones (1995)-style R&D-based growth model in which both a “final goods” sector and a “research” sector may be automated. Unfortunately, the results are presented in a way that somewhat deemphasizes the most radical growth possibilities. Still, they are taken more seriously than in any other economics publication to date.
The paper’s first contribution in this direction is a labeling of explosive growth scenarios. Those in which the time-path of output has a vertical asymptote—a time before which output exceeds any finite level—are termed “Type II” growth explosions. (These vertical asymptotes are the mathematical singularities for which techno-accelerationist views are sometimes called “singularitarian”.) Growth scenarios in which the exponential growth rate of output rises boundlessly without producing a vertical asymptote are termed “Type I” growth explosions. Objections that either scenario is physically impossible miss the point. Eternal exponential growth, and even eternally constant output, are presumably impossible as well. What a taxonomy of this kind gives us is a guide to the circumstances under which AI developments should be expected to accelerate growth, and, at least in qualitative terms, how dramatically.
Section 3 explains that asymptotic automation of research tasks, along the lines of section 2’s asymptotic automation of good production tasks, can allow for exponential growth in research inputs, technology, and thus output, even without population growth or any automation of final good production. Absent research automation, one of the latter two processes would be necessary for exponential output growth.
The discussion here feels incomplete. Since the automation of research tasks is presumably itself the result of technological development, one wonders under what conditions this process can sustain itself. Here, however, the automation of research is simply presented as exogenous.
Section 4.1 gives four examples of scenarios in which automation, within the frameworks introduced so far, can yield a growth explosion. Again, the discussion feels incomplete, now for two reasons. First, the examples are not systematic. Indeed, the scenario that would follow most straightforwardly from section 3—it turns out that, for some parameter values, growth is not only sustained but explosive when research automation is modeled as the output of technological development—is not discussed at all. Second, the discussion of the scenarios themselves is sometimes patchy, as outlined below.
Example 1 notes that full automation of final good production generates an “AK” economy. Output thus grows exponentially absent growth in technology (“A”), and double-exponentially given exponential growth in A. Not discussed is that output exhibits a Type I growth explosion even if we don’t simply stipulate exponential growth in A, but instead maintain the standard Jones idea production function and a constant population. In this case, A rises subexponentially but still unboundedly, and the exponential growth rate of output accordingly does the same.
Examples 2 and 4 find that full automation of idea production alone suffices to produce a Type II growth explosion as long as ideas do not “get harder to find” too quickly. (That said, as noted in section 4.2, recent estimates suggest they do.)
Example 3 finds that sufficient automation of good and idea production together produce a Type II growth explosion. A fortiori, it thus finds that the full automation of good and idea production—i.e., simply general AI—always produces a Type II growth explosion, whatever the rate at which ideas get harder to find and whatever values any other parameters take on. This could have been the paper’s headline result, but it is not even quite stated, let alone emphasized.
Section 4’s discussion of explosive growth scenarios concludes by giving various roadblocks to them the “last word”. Some tasks may be near-impossible to automate, for instance, or near-impossible to make more productive even once automated. In the face of bottlenecks like these, singularitarian dynamics might break down.
Finally, section 5 informally explores the growth implications of AI on a variety of views in which growth depends centrally on firm incentives. An appendix then delves formally into one such view: a Schumpterian model in which AI can slow growth by making it easier for actors to steal or replace each other’s innovations, thus disincentivizing their development.
The three classes of considerations discussed in sections 5.1, 5.2, and 5.3 are AI’s growth implications via impacts on market structure, resource allocation across sectors, and firm organization respectively. In short, AI could increase or decrease an industry’s competitiveness, by making it easier to overcome barriers to entry (say, verifying quality in the absence of reputation) or to erect them (say, with closed networks). Models like that of Aghion and Howitt (1992), in turn, teach us that increases in competitiveness can increase or decrease innovation incentives. AI could also affect growth in other ways (say, by facilitating adjudication in the face of incomplete contracts). Collectively, these considerations render AI’s growth implications complex and ambiguous. They allow for the paper’s most explicit calls for follow-up research, and its most wide-ranging.
It is not clear why this broad and open-ended discussion is reserved for firm-centric growth considerations in particular. As noted earlier, one can imagine a version of this paper that remains focused on formal results within the Jones-style R&D-based framework. But a paper surveying AI’s growth possibilities more broadly would ideally explore these implications from something closer to the full range of mainstream growth perspectives, including e.g. those with a central role for institutions or for human (and given AI, presumably machine) capital accumulation. Also, even within the firm-centric discussion, a singularity-sympathetic reader will again find something of a de-emphasis of AI’s radical potential. The singularities of section 4.1 are followed in 4.2 by the point that automation could face bottlenecks, for instance; the observation that attempts at growth-slowing idea theft could face bottlenecks too is left to the reader.
The conclusion reinforces this slant. The only paragraph on explosive growth opens by noting it as a “(theoretical) possibility”, and goes on primarily to summarize why the possibility may fail.
In fairness, this reticence may be due to a perception that many economists, jaded by the Luddite track record, would react poorly to models in which capital ever thoroughly substitutes for labor. For what it’s worth, Patrick Francois’s comment on the paper, published just after it in the Agenda, offers at least some evidence to the contrary: he quickly accepts the plausibility of near-term general AI, but muses on its implications for political economy rather than growth.
All that said, in sum, Aghion et al.(2018) provide excellent and wide-ranging analyses of automation and of AI-driven growth. They take several valuable steps beyond prior work in either area (such as Nordhaus’s 2020-published exploration of a model with high substitutability in final goods but mere exogenous growth in technology). In effect, they synthesize and rigorize a number of observations about AI’s growth potential from the likes of Solomonoff (1985)—formerly perhaps best summarized by Sandberg (2010)—and bring them to economists’ attention. They then augment these observations with powerful new results and framings of their own. The result is the best economics paper published to date on what has as good a claim as anything to being the most important subject in the history of the world.
Link to corrected proof.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",92,80,100,,,,70,90,40,97,80,100,45,30,70,80,,,92,80,100,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",5,,,,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

","~4 years, if you mean doing original research in economic theory with a large proportion of my time; ~2.5 years, if you mean having some particular focus on growth theory or the economics of AI.","I’m not sure how to interpret this.
I’ve peer-reviewed one paper on the economics of AI.
I’ve “evaluated” around 30 papers on the economics of AI in the course of writing a literature review on the subject.
More generally, I’ve given informal feedback on many research ideas and papers in progress by fellow researchers at GPI, fellow economics graduate students, and people (usually undergraduates) interested in doing EA-relevant economics work who reach out or are put in touch with me in some way.",False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,"Somewhat scattered, and in that sense less robust",,"An awkward combination of intensive focus on some things and selective breadth in others. Also, unusually many typos and minor errors. On the other hand, logical and very clearly written.","80? It’s theory, and clearly written","[This is for relevance to GP]
Sharpens our thinking about an extremely important topic, but does not include direct discussions about decision-relevance.","N/A (published), But I think I would have guessed ~3.5, i.e. a decent field journal, in light of other AI-and-growth-theory papers’ long periods of languishing before publication (such as Nordhaus’s).
As currently written, it is a chapter in a wide-ranging book, not a journal article. But I think that the content could have been written as (perhaps two) journal articles, each of which would have had very important new things to say about a very important subject. “Medium-High” [confidence].","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Artificial Intelligence and Economic Growth,https://unjournal.pubpub.org/pub/aimetrics,University of Chicago Press,"50_published evaluations (on PubPub, by Unjournal)",,,2025-07-07T09:29:38.750-04:00,
Artificial Intelligence and Economic Growth,http://dx.doi.org/10.3386/w23928,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Seth Benzell,,,"Manager’s note (David Reinstein): I converted math and Greek letters into latex format, mainly to demonstrate this capacity. I made no other changes.*
Thanks to the Unjournal for their invitation to review “Artificial Intelligence and Economic Growth”. In this essay the authors have three announced goals: Help set an agenda for research on the impact of AI on growth, refine research questions on the subject, and summarise and recontextualize key previous findings with an emphasis on Baumol’s cost disease. This is an ambitious task, but the authors largely succeed!
In the first four sections of the paper, the authors do a wonderful job of outlining a general neoclassical model of automation. They explain how the key parameters of the model determine the impact of automation. They distinguish between two types of economic singularity, and show how the more extreme variety emerges naturally from some parameterizations of their model – something which I believe is an important innovation of this paper (including above Nordhaus (2015) a direct antecedent paper). These models stimulate the reading researcher to ask how these parameters could be estimated, opening a door to applied economists to contribute to the macroeconomic question of growth and AI. After this, section 5 is a bit of a disappointment. It lists several additional economic phenomena that might be caused by AI and automation, and occasionally ties these ideas back to economic growth, but in a less organised way without the assistance of a model. The essay closes with empirical evidence on capital shares and automation, which was adequate for the time empirically, but is somewhat lacking in its interpretation of the data.
Let me start by going into detail about what I liked about the first several sections, including some complementary thoughts it inspired in me. Then I’ll explain what I consider the main factor omitted in these sections: the impact of automation and AI on saving and investment. I’ll close with some thoughts on the limitations of sections 5 and 6, and how they might be improved.
Section 2 of the paper lays out a general, neoclassical, model of automation, drawing on Zeira (1998) and Acemoglu and Restrepo (2016). The key equations are clearly presented. The authors highlight Baumol’s ""cost disease"" -- the phenomenon that an increase in output of one sector of the economy will make goods in a complementary sector more expensive -- as a key phenomenon to be understood for projecting AI and automation-led growth. '\rho' is the parameter in the model that governs how substitutable different goods (for example, automatable and non-automatable ones) are in the economy. When \rho is smaller, the economy is relatively more limited by its scarce labor than it is boosted by automation. It is more likely for interest rates and the capital share to even decline because of greater automation. This effect is exacerbated by capital accumulation over time, in contrast to labor which is inelastically supplied. The way I once heard this phenomenon described is ""You can always have more capital per-capita, but you can't have more capita per-capita"", and the authors do a good job of explaining this theme from the previous literature.
The authors do a great job of highlighting the importance of “\rho” to economic growth. Implicitly the authors are suggesting to applied researchers to go out and measure this elasticity! Between automated and non-automated tasks, or between relatively capital intensive and labor intensive sectors, for example.
The authors explain several special cases of their model, to explain how other parameters balance against each other as well. They focus on the role of “\beta”, the share of sectors which are automated. I think the authors are correct in taking a narrative approach to possible paths ‘\beta’ can take, rather than following Acemoglu and Restrepo (2016) and trying to endogenize it to the decisions of scientists. It's the right level of detail to stop at, given their more general concerns.
Sections 3 and 4 go farther beyond the current state of the literature, introducing AI as an input to technology production functions and considering versions of an economic singularity. Section 3's formalization is clear, but I might have appreciated a note from the author that other approaches to modelling ""AI in the idea production function"" might be better -- whereas I think the model in section 2 is more solidly paradigmatic. The key parameter here turns out to be ""\phi "", the rate at which knowledge growth is increasing/decreasing in the stock of knowledge.
In section 4, the authors lay out what I think are the best taxonomy of economic singularities I've seen (I think the best alternative that would have been in the literature at the time would have been Nordhaus 2015's). While these are somewhat extreme scenarios, they immediately ground themselves by showing how a type I case is the natural result of the oldest economic model of automation -- the AK growth model. I would make the connection between the AK growth model and the ""\rho=\infty"" (i.e. all goods are perfect substitutes) case of the general model in section 3 more explicit. The authors then show that the key parameter determining whether type-2 singularity is \phi. In the simpler model (example 2), \phi being greater than 0 is enough to create an infinite-economic-output singularity. In the third example, the condition is a slightly more complicated function of \phi. The section closes with an ok discussion of some more general related concerns regarding an economic singularity, returning again to \rho and the role of 'scarce bottlenecks' in output.
I really appreciated these sections, and feel they do a generally good job at agenda setting for both theorists and applied researchers. For applied researchers, I think the way the paper identifies ""\rho"", ""\phi"", and ""\beta"" as especially important serves as a useful directive towards what they should attempt to measure. What might have made the paper even better is a small table with empirical evidence on these parameters so far, to give the applied researcher inspired by this paper a starting point.
For the theorist, the mind swims with possible extensions to and variations on the approaches presented. Obviously a paper like this can't cover or even suggest every possibility. One might imagine variations of a growth model that allows for ""\rho” - which can be interpreted as a taste parameter - to be endogenous in some way. In section 5, the authors hint that markups changing over time could be important. They do the same, in referencing Acemoglu and Restrepo (2016) about making ""\beta"" endogenous. Another natural extension makes labor supply endogenous, or might explore an automation —> politics —> growth public choice mechanism. I don't think it's a problem that the authors failed to mention all these possibilities, but some of these I do think are more interesting and directly connect AI and growth than some of the other epiphenomena discussed in section 5 (some of which are less clearly reasoned — for example, isn't it just as plausible to think that AI will increase centralization and superstar firms as it is to decrease it?).
Still I do think that the authors fall down in not focusing more heavily on the role of saving in the model. Throughout the paper, the saving rate in the model is assumed to be constant — a hypothesis that isn't well grounded in either a representative agent model (which achieves a constant interest rate in the long run) or an OLG model (in which saving will be a function of many other considerations). I think this is an important oversight for a document that wants to set the agenda.
I’ll admit I’m a bit of a partisan for this issue, having considered it in (Benzell et al. 2015) and (Benzell et. al. 2022). In the first paper, we show how in OLG models automation technologies can actually lower output and welfare for future generations. The reason is that savings are made by the young out of their labor incomes, for consumption in their retirements. When automation accumulates, the share of income going to young and laboring savers decreases, and the share going to old spenders increases. This reduces the amount which is saved and reinvested. In certain cases, the reduced saving effect is large enough to more than offset the productivity growth effect of automation. The possibility that a new technology could lower long-run output is not admitted for in the authors' model – ruling out certain conceptually coherent scenarios such as the one imagined in Asimov’s “The Caves of Steel” – where highly productive AGIs and automation exists, but a low saving and reinvestment rate by a socialist government keeps society impoverished.
More generally, the exogenous saving framework pursued by the authors doesn't allow for any inter-generational analysis of the impact of automation. On a more practical level, interpreting the decrease in the global interest rate as telling us something about automation (for example, see the recent ""cite"") needs to account for global demographic and distributional factors that have created a ""global saving glut"" (cite). In (Benzell et al. 2021), we find that even a rate of automation at 5x the historical rate would fail to overcome this headwind and increase interest rates.
This brings me to the final section of the paper, on the evidence to date on automation and capital shares. Karabarbounis and Neiman (2013) is correctly taken as the starting point, and I think the discussion is ok for the time overall. My main quibble is with the characterization of Autor et. al. (2017) and Barkai (2020). These are presented as 'alternative theories of capital share's increase' but they're more like alternate theories of what K+N are measuring. These papers and Barkai and Benzell (2018) claim it is the profit share of income which is increasing, not the capital share, a theory that is consistent with the microevidence on markups (for example, De Loecker et al. 2020). That has tremendous implications for its interpretation in a model of automation. For example Benzell et al (2022) theorise that the profit share has increased because certain inelastically supplied inputs in the economy are complements to automation and measured as profits. Why do I mention this? Well, because it has dramatic implications for whether the \rho<1 or \rho>1 case is true: If \rho \lt 1 then ""capital share"" shouldn't be increasing, especially if interest rates and growth are low. On the other hand, \rho>1 implies an AK world asymptotically, which also seems unlikely. We think it more likely that \rho is \lt 1 , but physical capital's share is actually decreasing, which is how Benzell et al (2022) reconciles this riddle.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",80,70,90,,,,80,85,75,75,65,85,70,60,80,95,90,100,90,85,100,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4,,3.5,5,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

","I started my PhD in Economics in 2012. I became interested in the impact of automation on economic growth shortly after, so about 10 years.",I have reviewed about 30 papers. I’d say about ⅓ to ½ of these are broadly on the subject of automation.,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,[This is for relevance to GP],"The paper seems (counterfactually) very likely to get accepted by a mid-tier journal, even if only due to the authors, and is moderately likely to be accepted into a better journal.
I think it would be a good fit for the Journal of Economic Perspectives or Journal of Economic Literature","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Artificial Intelligence and Economic Growth,https://unjournal.pubpub.org/pub/aimetrics,University of Chicago Press,"50_published evaluations (on PubPub, by Unjournal)",,,2025-07-07T09:29:23.562-04:00,
Advance Market Commitments: Insights from Theory and Experience,http://dx.doi.org/10.3386/w26775,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Joel Tan,,,"Link to spreadsheet calculations
Summary of Kremer, Levin & Snyder's Primary Findings
Kremer, Levin & Snyder (KLS) find that the pneumococcal vaccine (PCV) advanced market commitment (AMC) probably resulted in around 700,000 lives saved that would otherwise have been lost.
Counterfactual increase in coverage due to pneumococcal vaccine advanced market commitment
To estimate the counterfactual impact of the PCV AMC, KLS compare the actual impact of the PCV AMC rollout to the rotavirus (RV) non-AMC rollout. This choice of comparator does make sense; as KLS writes: ""We selected rotavirus from the six global vaccine initiatives proceeding around that time for the following reasons. Three of them (IPV, second dose of measles, birth dose of hepatitis) involved early-vintage rather than new vaccines. The yellow-fever vaccine was not rolled out in many high-income countries, leaving no good base rate for coverage speed comparison. We conjecture the results would be stronger using HPV, the remaining candidate apart from rotavirus, for comparison, but any slow rollout of HPV vaccine in GAVI countries could be attributed to its administration to older children, slowing coverage expansion.""
That said, there is a reasonable worry over whether the results are robust to a different comparator class of vaccines when estimating the counterfactual impact of the pneumococcal vaccine (PCV) advanced market commitment (AMC). To check this point, I ran a rough quantitative analysis of my own. I use vaccination data from the following sources:
For RCV and RV vaccination, I use the International Vaccine Access Center & John Hopkins Bloomberg School of Public Health's View-Hub database (last access, 2023-02-02), which draws on WUENIC estimates. WUENIC estimates are official WHO/UNICEF estimates of national immunization coverage, created by drawing on country-reported data as well as published and grey literature, even while correcting for potential biases
For human papillomavirus vaccine (HPV) vaccination, I pull from UNICEF (last access, 2023-02-02.
For yellow fever (YF) vaccination, I rely on Shearer et al’s (2017) estimate of global yellow fever vaccination coverage (last access, 2023-02-03).
The disparate datasets are not ideal, but no single source I found while undertaking this quick review provides comprehensive data on all vaccines of relevance. Calculations were done in the linked Google sheet, last edited 2023-02-05.
Using this data, I first look at the proportion of PCV coverage with the AMC (across GAVI countries, for the first 12 years from introduction) relative to full coverage. This was calculated by taking a population-weighted average of coverage rates per country-year across all 54 current GAVI countries across 12 years (from introduction in 2010 up to 2021). Based on the aforementioned data and this specific methodology, overall coverage for GAVI countries across this time period was 49%.
Second, I look at three different comparator classes: (a) rotavirus vaccine (RV) coverage, (b) human papillomavirus vaccine (HPV) coverage, as well as yellow fever (YF) vaccine coverage (similarly looking across GAVI countries across for the first 12 years from introduction or equivalent). undefined undefined These three classes were used as the vaccines in question are either newer vaccines with a more recent introduction date (RV & HPV), or else an older vaccine where there appears to be a discrete recent push for vaccination by GAVI (YF) – allowing for us to perhaps observe the counterfactual world in which PVC was rolled out without an AMC but with standard GAVI support. In each case, I look at the proportion of the comparator vaccine's coverage across GAVI countries for the first 12 years. And again, this is calculated by taking a population-weighted average of coverage rates per country-year across all 54 current GAVI countries across 12 years (n.b. RV: from introduction in 2006 up to 2017, for comparability; for HPV, from introduction in 2010 up to 2021; and for YF, from GAVI making a concerted effort on YF in 2001 up to 2012, for comparability).
I find that KLS's results should be fairly robust, insofar as the coverage rates for alternative comparator vaccines are in fact lower than the mainline RV comparator KLS chose: coverage was 12% for RV, 2% for HPV, and 9% for YF.
Going further, I estimate the probable proportion of PCV coverage across GAVI countries across 12 years from introduction without the AMC, by creating a weighted average of the 3 comparator classes. In doing this, I firstly penalize dissimilarity in target demographics (n.b. PCV and RV are both targeted for <1 year olds, while HPV is for older individuals and the YF vaccine is for 9 months+ individuals) – this matters insofar as the former two vaccines will more likely be deployed as part of post-birth immunization schedules. Secondly, I penalize data limitations (e.g. the uncertain YF extrapolations). Weights are assigned subjectively and fairly aggressively, with each penalty leading to a comparator class being weighed a magnitude less than it otherwise would have. In all, the weighted average is 11% – which is my best-guess estimate of PCV coverage in the absence of the AMC.
Putting this together, the counterfactual impact of the PCV AMC as a proportion of total disease burden avertable by vaccinations in the relevant time period is around 38% relative to total coverage. Importantly, this estimate here would not differ by much (~2%) even if we only used RV as the comparator to estimate the non-AMC counterfactual – which suggests that KLS's results are robust to comparator class. That said, I would caution against using these results for direct comparisons to KLS's findings – this analysis is very rough, and given the different datasets/methodologies, I would be wary of utilizing these results for anything except a sense-check for the matter of comparator class robustness.
There is a further caveat to note – the datasets used see a considerable amount of missing data; in such cases, I made the methodological choice to treat missing data for country-years as 0% coverage. The idea is that (a) any country lacking the state capacity to report is unlikely to be doling out vaccines; and (b) such GAVI countries by definition have GAVI support, and GAVI does publish its data (which then feeds into the WUENIC estimates) – so unless GAVI were failing to report vaccinations (unlikely), it seems reasonable to think that unreported country-years do in fact suffer 0% coverage.
Robustness of headline DALY estimates
In any case, there are other issues that may affect the accuracy of the final estimates of DALYs averted:
The estimate relies on Tasslimi et al's (2011) calculations of DALYs averted per PCV shot – and this in turn relies on O'Brien et al's (2009) estimate of the global burden of disease caused by streptococcus pneumoniae. However, DALYs lost per capita to pneumococcus were declining in poor countries year-on-year for two decades even before the AMC, possibly due – at least in part – to economic growth bringing improvements in sanitation/nutrition/access to healthcare. Hence, projections on future DALYs averted based on past disease burden data may overstate the benefit. Theoretically, a way to account for this would be to re-run KLS's analysis but discounting each year's DALY per dose estimate using the rate at which pneumococcus DALY burden per capita was experiencing a secular decline in the two decades before the introduction of the vaccine.
On the other hand, KLS do not model the speeding up of the development of existing vaccines – hypothetically, the credible commitment provided by the AMC will have a dynamic effect not just on new entrants (to enter) but also on existing pharmaceuticals with nearly-licensed vaccines (to speed up their activities). The idea here is that with guaranteed profits on the horizon, existing pharmaceuticals will be willing to expand more resources and make greater efforts at bringing the nearly-licensed vaccine to market faster than they would otherwise have, thus bringing forward the date of introduction relative to a counterfactual world where the AMC was not made. Notably, the PCV-13 vaccine was licensed in 2010, while the AMC was made in 2009 – it is theoretically conceivable that licensure would have been later absent the AMC. That said, this is speculative, and hard to test besides – no obvious way presents itself to me at this juncture, and more research on this point would be both valuable and interesting.
My sense is that effect 1 would outweigh effect 2, such that the true effect of the PCV AMC is lower than currently estimated, but it is hard to say by how much, if at all.
Conclusion
Overall, relative to the null hypothesis (i.e. the AMC did nothing), I would (a) have extremely high confidence that the AMC made a difference and saved a significant number of lives; and (b) only moderate confidence that at least 700,000 lives were saved, per KLS's original estimate.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",79,59,94,,,,70,90,50,90,70,100,70,50,90,50,30,70,90,70,100,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",5,5,,,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

","1 year for cause prioritization, 5 years for broader economic research and analysis",Zero in an academic context,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Advance Market Commitments: Insights from Theory and Experience,https://unjournal.pubpub.org/pub/amcmetrics/release/8,AEA Papers and Proceedings,"50_published evaluations (on PubPub, by Unjournal)",,,2025-07-07T09:17:19.247-04:00,
Advance Market Commitments: Insights from Theory and Experience,http://dx.doi.org/10.3386/w26775,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Dan Tortorice,,,"In “Advance Market Commitments: Insights from Theory and Experience.” Kremer, Levin and Snyder describe the Advanced Market Commitment (AMC) mechanism that was used to increase coverage of Pneumococcal Conjugate Vaccines (PCV) in low-income countries. This mechanism represents one of the most important public health achievements of the recent past leading to PCV coverage rates of around 50% in eligible countries compared to an almost 0% coverage rate before the onset of the AMC. The authors estimate the AMC saved 700,000 lives. This paper provides a convincing argument for AMCs and a reasonable method for evaluating its effects. I will highlight two areas where further research could be beneficial.
In evaluating the impact of the AMC, the authors assume that, absent the AMC, PCV adoption would have followed the same path as Rotavirus vaccine adoption in the same countries. While this is a reasonable baseline, there are many reasons why that may not be the case. These vaccines are produced by different manufacturers who may have different willingness to supply vaccines to low-income countries. They also impact different diseases and take-up may reflect the relative importance of treating these diseases in low-income countries. Future research would be useful in this area. Perhaps constructing a counterfactual using each country's coverage of additional GAVI supported vaccines and using data on PCV coverage in countries that are not eligible for the AMC could be useful.
The theoretical portion of the paper provides a clear model of how the AMC provides value. Additionally, it makes important points about the cost of setting the subsidy too low and the potential costs of including co-payments for eligible countries. In future work it would be valuable to consider if the AMC is a permanent mechanism or one used to promote the adoption of a vaccine. Since adoption of a vaccine usually necessitates large annual expenditures indefinitely (as vaccines are rarely removed from a national immunization program), it would be helpful to know how to fund an AMC in perpetuity or how to transition off the AMC. This point also relates to considering country co-financing commitments as the hope with these is that countries will eventually be able to finance these vaccines on their own. Therefore, the cost of removing county co-financing requirements may be larger than the model assumes. Relatedly, the model is a one-shot model, but, in reality, there can be multiple bargaining periods and manufacturers may initially reject offers but then accept higher offers in later periods. In this case, risk of setting the price too low initially is less important. Finally, in many of these cases, the manufactures are not strictly profit maximizing but are constrained by social pressures to provide vaccines at an affordable price. It would be useful to understand how these pressures impact the ability of the manufacturer to bargain over the vaccine price.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",80,,,,,,80,,,90,,,80,,,90,,,95,,,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4,4,,,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",About 15 years.,I have evaluated roughly 50 papers and proposals in this time.,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,[this is relevance to GP,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Advance Market Commitments: Insights from Theory and Experience,https://unjournal.pubpub.org/pub/amcmetrics/release/8,AEA Papers and Proceedings,"50_published evaluations (on PubPub, by Unjournal)",,,2025-07-07T09:16:56.333-04:00,
Advance Market Commitments: Insights from Theory and Experience,http://dx.doi.org/10.3386/w26775,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",David Manheim,,,"Advance Market Commitments (AMCs) are a promising and already burgeoning area for global health aid for new vaccines and therapies, and have been widely used and discussed in the short time since their initial proposal. They also have been suggested as promising to speed up response for urgent needs during an emerging disease pandemic, making them potentially relevant for reducing global catastrophic biorisk.
This paper reviews recent uses and theory about AMCs, and in addition to a more general review, it seems useful to consider some of the claims more critically, and then to consider the relevance for key global priorities. I therefore begin with a review of the contents and claims, including a critique, then discusses relevance to global health and to pandemic preparedness and response.
As an overview of Advanced Market Commitments, the paper is excellent. It addresses many key points about the theoretical economic justification, provides data on the brief but very successful history of AMC usage in the past 2 decades, and the issues with the design which have been identified and are still relevant. The last section, especially the discussion of distant technological targets, has a number of implications, however, which are not fully explored - and which are critical to applications of the paper.
In both the initial theoretical section and the final section, the paper discusses use of AMCs for theoretically distant targets. The issues with doing this are discussed, however, the scope of the paper means that alternative approaches are not explored. They note that AMCs are intended to “supplement, not replace, direct R&D support.”
To discuss why the limitation in the scope of the paper to AMCs is problematic, it is useful to more clearly explain the conceptual framing the paper uses, “Push” and “Pull” funding. Push funding includes grants, scientific research funding, and other mechanisms which lead to but do not actually require production of the final product. This is designed to pay for innovation and research, but leaves the actual production to the market, with attendant market failures, which the paper discusses. Pull funding includes purchases and Advanced Market Commitments, and is designed to get the final product produced. But other mechanisms, such as prizes and option-based guarantees do not fit neatly into this framing, and are therefore unfortunately ignored. They also address very different parts of the potential market failure, and it seems likely that many of the issues which exist for AMCs to address technologically distant targets are better addressed using a combination of approaches, and AMCs may play a limited role.
As I have argued elsewhere, there are a variety of tools other than AMCs, including prizes, more direct public-private partnerships, and the proposed options-based guarantees. While the last category is most appropriate for urgent but technologically distant targets, the other two provide more of a spectrum. They also provide different tradeoffs, for example, because prizes do not need to pre-select the number of firms. An analysis of the appropriate tool for different future needs would therefore be even more useful.
The relevance of AMCs for global priorities is therefore mixed. They are appropriate and very sorely needed for continued and expanded use in global health and disease elimination campaigns. As the paper notes, they have likely already saved 700,000 lives, though how counterfactual this estimate is remains unclear - but they have the potential to be used further, and save an order of magnitude more lives in the near term. They also have potential to accelerate vaccines for known pandemic-potential pathogen threats, including use for universal influenza vaccines to address novel influenza, and similarly for pan-betacoronavirus vaccines. The optimal strategy presumably involves a mix of different tools, but policy considerations for which approach to use will also be critical, and the track record of AMCs is a significant advantage.
On the other hand, for technologically more distant targets, it seems the paper oversells the usefulness of AMCs compared to the alternatives. Most critically, the use of AMCs for novel diseases seems far more limited than other avenues. In the event of a novel disease pandemic, as COVID-19 showed, funding availability and advanced purchases were not the key constraints for getting vaccines developed. Instead, the slow nature of the testing and approval processes were critical, and AMCs typically do not provide direct marginal incentives for speed. Additionally, the relative lack of investment in manufacturing capacity to speed up availability was critical.
On the other hand, if combined with general research funding for programs like the proposed “prototype pathogen” research, which would develop candidate vaccines for critical viral families, then AMCs could be made more useful, though it remains unclear why they would be preferred to more direct funding and purchases.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",80,70,90,,,,95,98,85,25,20,40,75,60,90,,,,60,40,75,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",,3,,,2.5,4.5,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",,,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),"Excellent as an overview, and important for global health, but unfortunately somewhat disappointing about ways to address global catastrophic risks 

[Editor’s note: the evaluator has explained that they have not based their assessment on this point; we do not intend  'Relevance to Global Priorities"" to factor into the overall assessment)","Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,"Within the limited scope, the paper covered everything that would be expected regarding AMCs thoroughly.","This was a review, not intended to directly advance knowledge. To the extent that it claims additional usefulness of AMCs, it seems not to have addressed other options.",The implicit assumptions that resulted from the limited scope undermine some of the claimed usefulness of AMCs,"The paper used data appropriately, but seems to have no ability to fulfill this criteria.
[Editor’s comment: Some of the data and code they used in their simple empirical exercise could be made available.]","[this is for Relevance to GP, real world relevance N/A]
While AMCs are a critical tool for global health, and should be used more widely, other approaches which are not discussed seem more appropriate for the most critical future biological risks.","The paper seems (counterfactually) very likely to get accepted by a mid-tier journal, even if only due to the authors, and is moderately likely to be accepted into a better journal","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Advance Market Commitments: Insights from Theory and Experience,https://unjournal.pubpub.org/pub/amcmetrics/release/8,AEA Papers and Proceedings,"50_published evaluations (on PubPub, by Unjournal)",,,2025-07-07T09:16:29.358-04:00,
Mental Health Therapy as a Core Strategy for Increasing Human Capital: Evidence from Ghana,https://www.aeaweb.org/articles?id=10.1257/aeri.20210612,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Evaluator 2,,,"Thank you for inviting me to review this manuscript. I was asked to review the version shared on NBER and I note for the purposes of this review I have read the NBER paper, as well as the now-published version in AER: Insights. In the published version, the authors have already addressed some of my original minor comments (e.g. clarity regarding report sample sizes per group and randomization process - now much clearer, e.g. through a flowchart). I find this manuscript to be very clearly written and sound in terms of experimental design.
Summary:
This study is an RCT delivered in Ghana, providing group cognitive behavioral therapy (CBT). Participants were reported to be from the general population, although notably the 40 compounds selected for inclusion were those reporting the lowest average household proxy means test score. Individuals were not targeted based on pre-existing mental health problems or levels of distress. Results are assessed after a 2-3 month follow-up. Somewhat surprisingly, there is no evidence of heterogeneity by baseline mental distress. Authors report improvements in a variety of outcomes of interest, broadly described as ‘mental and physical health, cognitive and socioemotional skills, and downstream economic outcomes’.
Rationale & Framingundefined
My main critical comment pertains to the rationale of providing CBT to ‘a general population of poor people’. I acknowledge that the included population does include a large proportion of individuals reporting distress (70% moderate or severe levels) and equally that the authors address the lack of specific targeting to a given extent. Nevertheless, I think this could have been further developed and the language made more precise. Communicating around the policy implications could be particularly important if such otherwise promising and effective interventions are to be scaled and adopted more widely.
There has been a real boom in investigations of therapy provision in low- and middle-income countries (LMICs) and I worry that the current work sits in the context of a growing literature that poses a risk of stigmatizing people who live in poverty and a danger of pushing towards the provision of psychosocial support as a primary vehicle to poverty alleviation, above and beyond much needed developmental work. This argument has already been posed by others such as Lant Pritchett (“Development work versus charity work”).undefined
Improvements in infrastructure and the availability of accessible mental health care globally are needed, as well as effective interventions that improve the lives of vulnerable people living in poverty. At the same time, mental health interventions focus on improving targetable individual-level factors rather than factors at a macro-level. In broader behavioral science contexts, others have potently argued that individual-level solutions have led ‘public policy astray’ and are a misuse of resources (Chater & Loewenstein 2022).
Follow-ups
A key step in further research on this topic should be longer follow-ups. This is a limitation not only to this particular work but also more broadly to psychotherapeutical interventions in LMICs (e.g. meta-analysis from Lund et al., 2022; NB working paper). For instance, as the authors acknowledge, in a related comparable paper with a longer follow-up, Haushofer et al. (2020) contrastingly find null effects unlike the positive effects reported here. The potential of time decay is a crucial limitation for me. More positively, in their working paper Lund and colleagues find some initial evidence to suggest that psychotherapy can somewhat outperform unconditional cash transfers both in terms of cost and effectiveness regarding health and economic outcomes. More research is needed to make stronger claims with greater confidence, but taking the present paper and the existing literature, providing psychosocial support to people in LMICs seems to be a promising way to improve lives and can effectively sit within a package of offered support to help empower and lift people out of poverty. Notably, as well, Lund et al. provide more exploratory evidence that pairing economic and therapeutic support outperforms therapy on its own.
Outcomes
● Authors describe that the intervention led to ‘meaningful’ average increases in the studied outcomes. I’m unsure if increases in reports regarding good mental health ‘0.53 days per month’ is particularly strong in this regard.
● As a clinical psychologist, the convention in my discipline is to consider p > .05 as non-significant so my disposition regarding some outcomes that are barely significant at the .1 level is one of skepticism (e.g. 0.53 days with better mental health, p = .097; on average 0.48 days when poor health prevented engaging in regular activities for people with distress, p = .097).
● While the improvements in digit span could likely be interpreted as based on increases in ‘bandwidth’ associated with CBT, other cognitive scientists have been more skeptical about the use of working memory tests in the first place and have suggested these tend to be stable and working memory largely not malleable; hence their use should occur in the context of a broader cognitive battery.
● Generally I find the created indices to be reported clearly, although a touch further detail on the exact procedure (weights?) would have been welcomed. I note in passing that there are some existing recommendations in my field that caution the use of composite measures as this can pose a challenge to clarifying the mechanisms of action. In this particular case I think the risk is fairly low, given that the mental health index includes exclusively self-report measures clustering around distress, and the socioemotional skills index is also self-report, clustering around self-control.
● The ‘downstream economic outcomes index’ also comprises entirely self-reported measures on ‘the number of work days missed due to poor mental or physical health, self-reported economic status… and a self-evaluation of expected economic status in five years.’ Including objective measures would provide a stronger degree of evidence. That an intervention targeting (in part) improvements in cognition and self-assessment changed self-reported measures pertaining to ‘self-evaluation’ is a sign that the intervention worked well in its primary domain, but further measurement specificity and validity will be beneficial in future work.
Mechanisms
● Hard to isolate effects of CBT vs socialization effects (therapy provided in groups of 10 people) but this is a general concern for group therapy interventions and a question for appropriate control condition selection.
● Although the authors propose one underlying mechanisms (CBT -> better cognitive and socioemotional skills -> improvements in human capital), other possible mechanisms are still likely (cf. Lund et al 2022) and future efforts should be directed to clarifying the causal mechanisms so interventions can be optimized.
Other
● Data for this trial were collected prior to the announcement of other interventions (including provisioning of a cash transfer). I appreciate the transparency regarding reporting this. I still wonder how likely it is that people who were surveyed extensively about their financial position did not anticipate any further trial. Hard to assess the likelihood of any further demand characteristics, though some have argued this is generally common for interventional contexts in LMICs.
● I note in passing that a broader cognitive battery with greater ecological validity in its tests would have provided further support that the scarcity/bandwidth framing is appropriate. Scarcity experiments have largely been lab-based and so more abstract with potential not to correspond to lived experience. I’m uncertain how helpful framing around ideas such as ‘poverty leads people to give into temptation’ really is. We know people managing restricted incomes may spend their finances in seemingly ‘suboptimal’ ways - for instance, choosing to buy sugar instead of more nutrition-dense foods but these choices may not reflect irrationality but a more diverse set of priorities (e.g. wanting to enjoy flavor after eating only rice).
● As another minor comment expressed with some uncertainty, I’m not sure about the validity/appropriateness of claims such as people living in poverty being constantly presented with ‘stimuli regarding their own status… an individual born into a poor farming family may misinterpreted his low income as evidence of his own low levels of talent…” and beliefs such as ‘my efforts never pay off” etc. These are certainly defensible positions (as is the whole theoretical framing of the paper) but I find it equally plausible that there might not be self-centering in attributing ‘blame’ for all. People can still experience distress while recognizing external factors (e.g. “my yield is bad because of the bad weather that affected my crops”). I then wonder if the external environment is not a target as well. Otherwise if we improve people’s mental health in the short-term via psychosocial intervention but do not protect against negative environment shocks by directly addressing macro-level/environmental factors, people can still be trapped in the vicious cycle between poverty and poor mental health (e.g. Lund et al 2011; Ridley et al. 2020).","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",75,70,84,,,,90,94,82,60,55,65,70,62,82,90,80,95,50,48,52,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4,,,,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",[about 3-5 years]… (in global mental health specifically),[about] 20,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,"I wish these categories were separated - would rank it high for logic but lower for communication. Even though this is an extremely well written (clear and easy to follow) paper, I struggled with some of the framing and messaging (i.e. higher-level communication).","Data and code are provided alongside the published paper (https://www.openicpsr.org/openicpsr/project/164481/version/V1/view) I have not tried to reproduce any of the analyses as I do not have access to Stata, hence wider CIs. Readme file is detailed and seems clear enough.","Relevance to GP: 40, 50, 60
While the overall topic is relevant to global priorities research, I am hesitant to rely on the results of this manuscript to make any direct policy recommendations given the short follow-up period, the selection of some of the measures, and the corresponding p-values",N/A already published,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Cognitive Behavioral Therapy among Ghana’s Rural Poor Is Effective Regardless of Baseline Mental Distress,https://unjournal.pubpub.org/pub/barkeretalsummary,American Economic Review: Insights,"50_published evaluations (on PubPub, by Unjournal)",,,2025-07-07T09:07:42.027-04:00,
Mental Health Therapy as a Core Strategy for Increasing Human Capital: Evidence from Ghana,https://www.aeaweb.org/articles?id=10.1257/aeri.20210612,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Evaluator 1,,,"Summary: This paper uses a field experiment to explore the impact of a 12-week CBT program among poor households in rural Ghana. The authors find that the CBT program increases mental and physical well-being, as well as cognitive and socioemotional skills and downstream economic outcomes. There are no heterogeneous treatment effects by baseline mental health. However, a measure of vulnerability to mental distress does predict the impact of CBT on mental and physical health, but not on cognitive and socioemotional skills or downstream economic outcomes. The authors argue that CBT operates via two channels: It alleviates vulnerability to mental distress for those most at-risk, and it also generates greater cognitive bandwidth across the population.
Major comments: This paper addresses an important topic in development economics with clear policy implications; if effective, CBT programs could improve not just mental well-being but also, as this paper suggests, key downstream economic outcomes at a relatively low cost. There is relatively little large-scale, well-identified work on the impacts of CBT in low-income, developing country contexts, and as such additional evidence is quite valuable. The paper is also clearly written and a pleasure to read.
My more substantial comments primarily regard my desire to better understand both the context and results. I understand (and appreciate!) the concise writing, but at times the paper felt a bit barebones. In particular:

A. Nods to attrition, balance, and other “standard” discussions in experimental work were missing from the main text. As far as I could tell attrition was not once mentioned, and yet it seems likely that all participants did not in fact complete all 12 weeks of the CBT program; similarly I would expect that some participants did not respond to the follow-up survey. This feels quite important to report to the readers, at least in a footnote. In the case of balance, I found the rich discussion of the randomization procedure in Appendix A to be quite compelling and worth at least some reference in the main text, in part just to clearly state the variables for which balance was (or was not) ultimately achieved. [Evaluation manager’s note: the authors do consider and report on attrition in the AER-Insights published version.]

B. I would have appreciated access to survey materials, either via the appendix or the authors’ websites. It is possible to access the CBT program guide online, but from what I can tell this does not include the questions that make up the survey and lab outcomes included in the analysis. This is important for several reasons. Three that come to mind in particular:
I was a bit puzzled as to the selection of mental health outcomes. Three question types were included in an index (presumably these were the only three relevant questions asked in the survey?), but only one of the question types (K10) is included in many of the key analyses. The K10 score also happens to be the outcome with the most statistically precise and positive treatment effect but with quite variable scores over time, which makes an understanding of the decision to focus on this outcome especially important. Access to the survey materials would help readers to understand this selection decision, as well as the wording of the particular questions that made up the index.
Access to the survey would also allow readers to better understand how experimenter demand may affect self-reports, which was not discussed in the paper. Were participants aware that the survey questions were related to their participation in the CBT program, for instance, or was this link obfuscated? This is important for the interpretation of study findings, and perhaps for understanding the difference between these results and the Haushofer et al findings (see more on this below).
More generally, access would be appreciated for ease of replication and/or scale-up efforts.

C. On page 4 when discussing the hypothesis that the poor are particularly vulnerable to mental health difficulties, it would be helpful to point to empirical data supporting this claim. Similarly, when discussing the observed baseline rates of mental health in this sample, it would be helpful to see how the data compare with other samples. There is a note on page 8 comparing the BRFSS averages in the US to the K10 scores collected in Ghana. Given that the authors also collect at least one BRFSS question in the Ghana sample, I would encourage them to report this here for the sake of comparison to the US sample.
My preference would have been to put a bit less weight in the introduction and discussion on the particular mechanisms underlying how CBT operates. The data suggest that CBT has some impact on downstream outcomes (cognitive, socioemotional, and economic) independent of the effect it has on mental and physical wellbeing, largely because a measure of vulnerability to mental distress only predicts treatment effects for mental and physical wellbeing. As a result, the authors claim that “CBT directly improves bandwidth, increasing cognitive and socioemotional skills and hence economic outcomes.” However – especially given that there are not heterogeneous effects by predicted bandwidth – I would encourage the authors to discuss the scarcity mechanism as one possible mechanism, rather than suggesting that this channel has been mechanistically isolated. My sense is that the note in the conclusion that “the poor can generally benefit from CBT whether they have mental health problems or not” nicely captures the policy implications of these findings, and I would encourage this set of results to be more broadly framed in this light.

The authors appropriately cite a Haushofer et al (2020) paper which finds no impact of a 5-week CBT training program at a 12-month follow-up. While it is of course no critique of this paper that different results were observed here, the difference does raise questions about how the respective studies ought to inform our posterior. To aid in this endeavor, I would like to see a somewhat more complete discussion of the two sets of results. For instance, it seems worth mentioning why a 12-month follow-up wasn’t collected here to best compare results; was it because the other treatments in this trial had begun to be implemented, and so the randomization was no longer clean? The authors also suggest that perhaps given the Haushofer et al results there may be fade out in the longer run (Haushofer et al may have also observed effects if they looked at shorter-run outcomes). But if this is the case that significantly impacts how we interpret the value of the CBT program, and so a longer-run follow-up in this context would be very valuable. Finally, another related paper that seems worth including in this broader discussion is Bhat et al (2023).

While this is not an actionable suggestion, it seems worth noting that a pre-analysis plan would in my view have been quite useful for this project. For instance, it would have allowed the authors to clarify why the analysis focuses on the particular K10 outcome (Was it a question of power [but then why not rely on the full index]? Was it necessary to restrict the outcomes for a secondary analysis?). It would have also provided a space to register decisions such as pooling the control group households in CBT treatment communities with the control group households in control communities.

Minor comments:
I found the degree of churn in the K10 mental wellbeing scores over time to be a bit surprising. It would be helpful to see the correlations between K10 and the other mental wellbeing measures. Was there a similar degree of churn over time across these measures?
It seems as though there is a lot of rich data here, and indeed even more that could perhaps be gleaned. I was interested in a few questions around spillovers that might be further discussed, for instance:
a. Both adults in control households are included in the analysis, but only the treated member of the treated household is included. This presents a nice opportunity to look at spillovers for spouses of treated participants: I’m interested in what the data tell us?
b. The lack of spillover effects in general is rather interesting, and I would have liked to see more on this.
The randomization procedures described in Appendix A are very neat – a great way to ensure proper balance. I’d suggest including some of this discussion in the main text, at least in a footnote.
More table notes would be appreciated, e.g. for Table 1.
There is a typo in footnote 17; “results” is not spelled correctly.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",75,,,,,,60,,,65,,,75,,,50,,,75,,,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4,,,,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",[About 10 years … Editors: removed some mildly-identifying content here],20-30 formal referee reports,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,Relevance to GP: 75,NA - already published,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Cognitive Behavioral Therapy among Ghana’s Rural Poor Is Effective Regardless of Baseline Mental Distress,https://unjournal.pubpub.org/pub/barkeretalsummary,American Economic Review: Insights,"50_published evaluations (on PubPub, by Unjournal)",,,2025-07-07T09:07:39.884-04:00,
The Comparative Impact of Cash Transfers and a Psychotherapy Program on Psychological and Economic Well-being,http://dx.doi.org/10.3386/w28106,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Evaluator 2,,,"This paper studies the economic and psychological effects of providing two different interventions to low-income households in rural Kenya: a program in Cognitive Behavioral Therapy (CBT, a well-established form of psychotherapy) and an unconditional cash transfer. The authors use a randomized controlled trial with a 2-by-2 design to estimate the effect of each intervention alone and of both interventions combined. Both types of intervention have been studied separately in low and middle-income countries, but less research has compared them in the same context or looked at the effects of combining the two.
Strikingly, the authors find no effect of the therapy program on any of their primary economic or psychological outcomes: consumption, wealth, revenue, and an index of psychological wellbeing including depression and anxiety symptoms and functioning. This holds even for those with poor mental health at baseline. The cash transfer, meanwhile, significantly improves all these outcomes. Unsurprisingly given the null effect of therapy, the combination of cash and therapy has similar effects to cash alone.
Overall assessment and study contribution
The randomised controlled trial itself was well-executed and analyzed (as I’d expect from these authors, who in this and other work set a high standard for conducting and reporting randomised trials). Below I discuss this more; I have only minor comments on the implementation and find the results believable and unlikely to be biased.
The contribution is limited in one sense by the null effect of the CBT intervention. The most novel feature of the design was the comparison and interaction of CBT and cash, but as it is, without a ‘first stage’ effect on mental health the study can’t answer the research question of how an effective CBT program (which exist) might compare to, or have complementarities with, cash transfers.
But it’s not wholly fair to penalise the study for this – and the results are nonetheless interesting – because the CBT intervention should have worked. The program was a faithful replication of one that a high-quality trial found effective elsewhere in Kenya (Bryant et al. 2017). With hindsight, maybe more could have been done to improve the chances it worked. But overall it seems reasonable to expect it would have (I would have guessed so). So the study was ex ante well set up to investigate the research questions above. I’ve tried to judge it on that basis; penalising well-designed experiments for null results creates publication bias.
Moreover, the study still provides important new evidence on the (economic) effectiveness of CBT programs for general – rather than clinical – populations. A huge literature finds CBT is effective for people with mental illnesses, including in low- and middle-income countries (Lund et al. 2022), but there is much less evidence on whether CBT benefits general low-income populations. In principle, the techniques behind CBT – such as how to rework negative ‘automatic’ thought patterns – could both prevent mental illness and aid general decision-making, not least for those in poverty given its known effects on the mind. But this study finds no such effect. I know only one other study on this question: Barker et al. (2022), conducted contemporaneously in Ghana, who find the opposite result. So I think this is a significant contribution (though the literature on CBT is large and there could be other studies I’m unaware of).
This contribution is limited a bit because it’s not yet entirely clear why the CBT intervention didn’t work. The authors discuss this in depth and rule out several possibilities. But there are differences with the previous evaluation, Bryant et al. (2017), in terms of both sample and when the effects were measured (1 year later, rather than 3 months). We also don’t know if the CBT managed to change any of the thoughts or behaviours it targeted, which would help to sort out different possibilities. The authors have an interesting hypothesis about the intervention lacking a specific goal, but I didn’t quite understand the underlying argument here.
(Meanwhile, the cash transfer effects are - as intended, I think - in line with a large prior literature including previous work by these authors (Haushofer and Shapiro 2017), so I don’t focus on them more here though they are certainly a useful replication).
Below I organise my specific comments by sections: the design of the trial and intervention, the analysis, and the mechanisms (why there was no effect). I have tried to give positive comments, not just criticisms, and be constructive where I can. My main constructive suggestions are mostly to do with the discussion of why the CBT intervention didn’t work here, particularly when the previous evaluation in Kenya did.
Intervention and trial design
Overall, the randomised controlled trial was conducted and analysed using ‘best practice’ techniques and I could not find significant threats to internal validity. Randomization was stratified on key variables including psychological wellbeing; other variables are balanced at baseline between treatment and control; there is very low attrition which is non-differential by treatment status, and the authors account for potential spillovers in the randomization design. The outcomes are appropriate and measured well.
I think the authors also chose the intervention well. With hindsight, some might argue they should have picked a more intense CBT program – at 5 weekly sessions, this one is shorter than most – and one with a larger evidence base, to improve the chances of a first stage effect on mental health. But I’m not sure such an evidence base exists for the Kenyan context specifically, and there’s meta-analytic evidence that brief CBT can still be effective (Cuijpers et al. 2023).
Main comments on the design:
The compliance reported by the NGO -- 95% of those in the treatment group attended all 5 CBT sessions – is almost surprisingly high, given that in Bryant et al. (2017) only 60% of treated people attended all five sessions, and most people in the authors’ survey 18 months later recalled attending less than five. It’s worth explaining briefly how such high compliance was achieved (were attendance payments more generous?). Otherwise, I worry that maybe some health workers here claimed a session had happened when it hadn’t. Not the most likely possibility, and I don’t mean to be unfair – maybe the authors and the NGO simply did an excellent job ensuring people showed up!
It would have been nice, in hindsight, to have a manipulation check for the CBT intervention – asking participants post-treatment if their thought or behavior patterns had changed, in the way CBT targets. I think some scales to do this for elements of CBT exist, such as the Behavioral Activation for Depression Scale.
Again in hindsight, to facilitate comparison with Bryant et al. (2017) [undefined] outcomes could have been measured at 3 months rather than just one year as the authors do. But I understand the authors’ desire to look for longer-run effects (other evidence suggests CBT can improve mental health at such time frames - Cuijpers et al. (2023)), and budgetary considerations may have prevented multiple endline surveys.
Additional suggestions:
It wasn’t clear whether endline surveyors were blinded to treatment status (I see baseline surveyors were). Of course, I’d expect any bias from surveyor unblinding to make the estimated effect larger, which makes this a minor point given the results.
 
Analysis
The analysis, including the exact specification, was pre-registered, and is done according to good practice: standard errors are clustered appropriately at the treatment level and there are corrections for multiple hypothesis testing across primary outcomes. It is very nice that the authors aid interpretation of their null result by calculating the negative predictive value, something which is easy to do but not done often in other papers. Also nice is including a discussion of cost-effectiveness.
Mechanisms: why was there no effect?
The key question is why this study found null effects of CBT when the previous study (Bryant et al. 2017) found such large ones. The authors rule out several obvious explanations. First, this study was statistically well-powered enough to rule out  effect sizes close to those in Bryant et al., meaning the difference is probably not due to ‘chance’. (It also appears that Bryant et al. was largely well-powered and executed, so their result was not just a fluke – could be worth discussing this in the paper too). Second, Bryant et al. studied a different sample: women victims of gender-based violence in peri-urban Nairobi, rather than low-income household heads in rural Nakuru county. But the authors in this study still find no effect among women, victims of intimate partner violence, or people with high psychological distress.
Beyond this, I have some comments on the potential mechanisms:
Sample differences. There are more differences in the samples that seemed underexplored currently:
The authors say the rural-urban difference is unlikely to explain their findings but it wasn’t clear to me why: rural and urban areas could differ in, say, income and education (Nairobi county has about 30% higher GDP per capita). If there is a good reason why rural and urban should be similar it would be great for the authors to elaborate.
It would be helpful to see a full table of summary statistics in this paper, to be able to compare the sample in more detail with Bryant et al. (Perhaps the table could even make this comparison). I couldn’t find this in the current draft.
It would be good to test other heterogeneity cuts of the data such as age and education. I was able to verify that the sample in Bryant et al. is about ten years younger on average, but it was hard to compare any other variables between the two papers. Of course, these extra analyses would not be pre-registered, but the results might at least be suggestive (and multiple hypothesis testing could be corrected for).
Intervention purpose. The authors argue that being delivered without a particular goal in mind might have made the intervention less effective: in Bryant et al. (2017), the goal was addressing gender-based violence. I am not sure I understand the argument here, given that the intervention content was apparently unchanged (p.5). Is the idea that some elements are ‘improvised’ by the community health workers delivering the intervention, and they do this differently when there is a specific goal in mind (e.g. giving examples relevant to domestic violence)? Or is it something else? This is a really interesting potential mechanism and worth more discussion.
Intervention delivery. This study used different community health workers (CHWs) to deliver the intervention than Bryant et al., and the authors find that some CHWs were more effective than others. It would be good to comment on whether this could explain the difference with Bryant et al. (2017) (I know the NGO had the same selection and training procedures, but was it harder to find CHWs in Nakuru county?). Ideally, getting the data from Bryant et al. (2017) and looking at CHW fixed effects there too would be interesting but I understand it might not be possible.
Additional suggestions:
The authors could also report the estimated main treatment effects, MDEs and NPVs in the subsamples of female, psychologically distressed and high-IPV participants. This would help confirm whether there is enough power to rule out the effects in Bryant et al (2017) when restricting to a similar sample (I expect there is).
 The authors could also look at heterogeneity by exactly the sample selection criteria in Bryant et al (2017), who had a low threshold for GHQ-12 but a high threshold for WHODAS. I am not sure that the current heterogeneity analysis by severe distress exactly covers this.   ","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",75,65,85,,,,90,95,85,70,60,90,75,70,90,50,40,80,75,60,90,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4,3,3,4,2,4,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",,,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,A well-executed RCT with effort to avoid bias.,It provides useful evidence on the (in)effectiveness of CBT for general populations and was well-designed to investigate other important questions.,"Generally good, some claims could be better supported.",Data and code aren’t available but I think this is standard for unpublished papers so I downweight this category. There’s some small discrepancies between the numbers reported in section III.B and the actual numbers in Table 1. Pre-analysis plan is available.,"There is cost-effectiveness analysis and good nontechnical communication.
Relevance to GP: 80, 90, 100",,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,The Comparative Impact of Cash Transfers and a Psychotherapy Program on Psychological and Economic Well-being,https://unjournal.pubpub.org/pub/cashtransfersmetrics,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-07-07T09:00:45.427-04:00,
The Comparative Impact of Cash Transfers and a Psychotherapy Program on Psychological and Economic Well-being,http://dx.doi.org/10.3386/w28106,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Hannah Metzler,,,"Brief explanation:
The paper advances knowledge and practice on cash and psychotherapy interventions, testing long-term and spillover effects, replicating important earlier studies and specifying conditions for the cash transfer intervention further (lump-sum vs. dispersed/weekly transfers). It does not provide further insight on what would make the psychotherapy intervention work.
Methods are generally robust and well-justified. 
Logic and communication are clear, the reasoning is transparent, arguments make sense. Data and analysis are relevant to the arguments, and conclusions justified. Only when the authors conclude that the timing of the post-intervention measure is not a plausible explanation for why they don’t find PM+, I disagree to some extent.  Figures and tables are easy to understand. 
Open, collaborative, replicable science and methods: Methods and analyses are described in detail, but code and data is not shared, making computational reproducibility hard. Some, but not all materials are shared and can support future research. Pre-registration was done before analysis, not before data collection. Numbers are consistent throughout the paper.
Real-world impact quantification: the paper tests a real-world intervention, and its results and conclusions are very plausible and realistic. 
Global poverty and well-being is highly relevant to global priorities. 
Summary: 
The paper presents a pre-registered RCT in Kenya with the main goal of comparing cash transfers to poor households with a psychotherapy intervention called problem management plus (PM+), as well as testing their combined effect. Primary outcome measures of the RCT included both economic (consumption, assets, household revenue) and psychological well-being (scales for psychiatric screening, stress, happiness, life satisfaction scales, intimate partner violence). The cash transfer corresponds to about 20 months of per capita consumption (in the control group). PM+ is a five-week CBT-based program with one session per week, in which a trained volunteer (community health worker) works on stress management (e.g. relaxation and breathing exercises), problem-solving, behavioral activation, and strengthening social support with individual clients. 
This paper makes important contributions by testing earlier intervention effects for their robustness, including additional outcomes, and evaluating conditions under which the interventions work: (1) It adds further evidence on the robustness of cash-transfer interventions having effects on both economic and psychological outcomes. (A side finding is that weekly cash transfers over 5 weeks are somewhat more effective in increasing monthly non-durable consumption and revenue than one lump-sum.) (2) It does not replicate spill-over effects of cash transfers reported in Haushofer & Shapiro (2016), suggesting these are not robust across different regions in Kenya. (3) It does not find long-term effects of the PM+ intervention on economic well-being and (4) and does not replicate effects of PM+ on psychological well-being (previously reported for 3 months after this intervention) with a more long-term (13 months) measure. (5) In line with this null result, cash transfers combined with PM+ have a very similar effect on most outcome variables than cash transfers alone. 
One earlier RCT (Bryant et al., 2017 [undefined]) found effects on psychological well-being only (not on economic outcomes) 3 months after the PM+ intervention in a sample of women who were victims of intimate partner violence (IPV). The results on PM+ presented in the paper could indicate one of the following: that PM+ effects on psychological well-being…
do not replicate. (This seems possible given the larger sample of this RCT.)
do not replicate in a general sample, and are only successful when targeted at a specific problem (e.g. intimate partner violence in the earlier RCT).
last for shorter than 200 days (from 200 days onward, analyses of effects over time show no impact on psychological or economic well-being). 
They also indicate that the current implementation of the PM+ was a lot less cost-effective to increase both economic and psychological well-being 1 year after the intervention than cash transfers. 
Other positive aspects and strengths
First test of long-term effects of PM+, and of PM+ effects on economic well-being
Measures to reduce bias and questionable research practices: the analysis was pre-registered, this pre-analysis plan was largely followed, the paper corrects for multiple comparisons
How the authors transparently report on a null result for a replication of spill-over effects of cash transfers, which they published themselves in an earlier paper (Haushofer & Shapiro, 2016) illustrates the limited influence of bias.
Robust methods, for instance, a larger sample than the earlier studies it replicates, with over 500 households per group, careful randomization procedure, different data quality checks and smart checks for demand effects.
Results are reasonable, and (almost) all conclusions justified (see limitations for one exception)
Clearly written
Limitations and potential ways the work could be improved:
Below, I mention some small ways of improving the paper. I consider none of them major limitations, but more refinements of the discussion of some results, or small details that are missing in the paper. None of what follows should change the main practical conclusion of the paper that cash transfers are more cost-effective to improve economic and psychological well-being in the long-term than the described implementation of PM+, because PM+ had no such long-term effects. This conclusion is solid in my opinion.
The discussion of explanations for why PM+ effects could not be replicated seems a bit too confident about the fact that the timing of the follow-up survey did not matter. 
I think that the delay between intervention and follow-up could be a potential explanation for why PM+ has no effects on psychological well-being in the current study. The post-intervention survey was very late to plausibly observe the effect of a short and low-intensity psychotherapy intervention of maximum 5 sessions over 5 weeks. The median time of the follow-up survey is 13.5 months (range: 2-23 months) after the end of the intervention. First, from a psychological perspective, the assumption that such a low-intensity, individual-session therapy intervention would have a lasting effect over a year later seems implausible. Second, the comparison of a cash transfer intervention that corresponds to about 20 months of per capita consumption (quite a long-term/intensive intervention), with a low-intensity 5-week therapy intervention seems a bit unbalanced to me, especially when evaluating outcomes after a year. I had the impression that the study design was set up to measure cash transfer effects, and then added PM+ into the design without adapting it to measure the effects of this second intervention.

I would suggest more clearly mentioning that the study was not set up to detect effects at time scales shorter than 6.5 months (200 days) when discussing the results. The claim that timing is unlikely to explain the null-result could be phrased with more nuance, given that the current analysis of effects over time can only speak to effects after 200 days. Examples for changes I would make throughout the paper are: (1) after the last sentence on p. 5 (“ the delay between intervention and endline is also unlikely to explain our null results …”) - include the footnote number 6 into the main text. (2) change the statement “PM+ being ineffective” on p. 32 to be more specific and say “PM+ not having any long-term effects”.
Based on such discussions, future comparisons of other interventions with psychotherapy could then choose interventions that are equally plausible to have an effect (at the time scale measured), such as higher-intensity or more long-term interventions, interventions in group settings that could foster social support that lasts longer than the intervention itself, or focus on more specific populations or problems. Some of this is already discussed in the paper (e.g., in the final conclusion).

A note: Hindsight bias (thinking such an effect was implausible from the start) could have influenced this judgement of mine; I assume the NGO did believe the intervention could have a long-lasting effect at least on psychological well-being after 1 year. I can also imagine that one of the study’s main goals was to show that such low-intensity psychotherapy interventions are not effective compared to cash transfers in the long-term, if such claims were made before. This is not obvious from how the manuscript introduces its aims, however.
Some short/small clarifications on differences between the pre-analysis plan (PAP) and the paper plan would be important.
One pre-registered sub-component of subjective well-being (the custom worries scale) was omitted in the paper, and not included in the index, without mentioning why.  This should definitely be mentioned as a deviation from the PAP in the paper. 
Some terms in the PAP differ from terms used in the paper, and it is unclear if they refer to the same variables. 
“Depression” (probably refers to the GHQ12, distress in the paper). Depression is mentioned as the main outcome variable in research question 1 in the PAP. Yet, later in the PAP and in the paper only the subjective well-being index and its subcomponents (not including depression) are described. If depression was simply not the correct term in the PAP, I would briefly mention this somewhere in the paper. 
“Mental health intervention for IPV”: this sounds like an intervention targeted at a specific population and problem, rather than the general population PM+ intervention focused on many different kinds of problems described in the paper. I would clarify these terms briefly in the paper, for example as a footnote in the methods or in the Appendix. 
Before reading the PAP in detail, I was wondering why no results on the analysis of norms around IPV were reported. The norm measure is briefly mentioned on p. 17, and all indices/questions are reported in the appendix, but no results are reported. The PAP clarifies that the norms analysis was only planned as part of the analyses of the mechanisms behind spill-over effects, which were not found and thus not analysed (see section III.J, p39).  I would recommend making that clear in the paper (on page 39/40).
Heterogeneity of results: Victims of IPV
Since recruiting was not targeted at victims of intimate partner violence (IPV), I wonder if a median split on IPV to examine heterogeneity of results across women with and without IPV experience actually includes enough study participants with such experiences to rule out that PM+ has positive effects in this subsample. 
In parallel to the analyses conducted for participants with high baseline distress, and extreme distress, an analysis among women with IPV experience would be more convincing. If there are not enough such women in the current study, I would more clearly entertain the possibility that the intervention may only be helpful for this specific population when discussing the heterogeneity of results with regard to IPV on page 35 in section III.F. (The final conclusion does mention this already).  
Optional
I would find a Figure like Figure 3 but for secondary outcomes very useful (e.g. in the Appendix). 
A note for Figure 1 that explains the abbreviations could be helpful (HH = households, what is BL and EL?).
Including a link to the other study mentioned on p. 11 on Digital Financial Service incentives would be useful, because both pre-registration and paper mention several questions/details related to it. 
Back-check reports (p 15): Calling participants to verify attendance of sessions 18 months after the intervention is very likely influenced by memory biases, that is, failures to recall all attended sessions. The statement “Our phone resurvey could not confirm the high rates of participants receiving the entire schedule, with only 35 percent (20 percent) of PM+ (CT&PM+) subjects remembering having received all five sessions” therefore seems to portray the data on attendance provided by the NGO in a more negative light than what seems justified. Self-reports after 18 months are simply not a strong indicator for potential errors in the data. I would mention this to clarify what the back check results mean. 
I have some doubts about how warranted the request for future work taking the possibility that M+ might increase IPV seriously (p.28, end of 2nd paragraph) is. The results are much more suggestive of a demand effect, since the smiley and envelope tasks show no significant increase in IPV. Furthermore, the effects on IPV are not significant in the combined Cash and PM+ condition either. But I agree it’s better to err on the side of caution, I would probably just rephrase the sentence so that it suggests that demand effects are the most likely explanation. 
It was not obvious to me why data collection was stopped during elections - because that might have influenced well-being? This could be mentioned very briefly. 
Future research: 
Could evaluating group-based approaches for PM+ or other psychotherapeutic interventions be important to increase cost-effectiveness and potential long-term effects? (If such research has not already been done since 2020).
Are there spill-over effects in the same household, i.e. for the family of the recipient of an intervention?
My references for these 2 questions: work by the Happier Lives Institute evaluating the work of Strong Minds (interpersonal group-based therapy).
But see the debate on how robust the evidence for cost-effectiveness of StrongMinds: https://forum.effectivealtruism.org/posts/ffmbLCzJctLac3rDu/strongminds-should-not-be-a-top-rated-charity-yet ","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",90,,,,,,90,,,90,,,80,,,70,,,100,,,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",5,4,,,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",,,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,Relevance to GP: 100,"“Not very familiar with economic journals and their standards, how strong[ly] they weight novelty”","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,The Comparative Impact of Cash Transfers and a Psychotherapy Program on Psychological and Economic Well-being,https://unjournal.pubpub.org/pub/cashtransfersmetrics,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-07-07T09:00:32.730-04:00,
Long Term Cost-Effectiveness of Resilient Foods for Global Catastrophes Compared to Artificial General Intelligence Safety,https://linkinghub.elsevier.com/retrieve/pii/S2212420922000176,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Scott Janzwood,,,"I am a political scientist specializing in science policy (i.e., how expertise and knowledge production  influences the policymaking process and vice-versa), with a focus on “decision making under conditions of uncertainty,” R&D prioritization, and the governance of systemic and catastrophic risk. With respect to the various categories of expertise highlighted by the authors, I can reasonably be considered a “policy analyst.”
Potential conflict of interest/source of bias: one of the authors (Dr. Anders Sandberg) is a friend and former colleague. He was a member of my PhD dissertation committee. 
A quick further note on the potential conflict of interest/bias of the authors (three of the four are associated with ALLFED, which, as the authors note, could stand to benefit financially from the main implication of their analysis - that significant funding be allocated to resilient food research in the short-term). In my opinion, this type of “self-advocacy” is commonplace and, to some extent, unavoidable. Interest and curiosity (and by extension, expertise) on a particular topic motivates deep analysis of that topic. It’s unlikely that this kind of deep analysis (which may or may not yield these sorts of “self-confirming” conclusions/recommendations) would ever be carried out by individuals who are not experts on - and often financially implicated in - the topic. I think their flagging of the potential conflict of interest at the end of the paper is sufficient - and exercises like this Unjournal review further increase transparency and invite critical examinations of their findings and “positionality.”
I am unqualified to provide a meaningful evaluation of several of the issues “flagged” by the authors and editorial team, including: the integration of the sub-models, sensitivity analysis, and alternative approaches to the structure of their Monte Carlo analysis. Therefore, I will focus on several other dimensions of the paper.
Context and contribution
This paper has two core goals: (1) to explore the value and limitations of relative long-term cost effectiveness analysis as a prioritization tool for disaster risk mitigation measures in order to improve decision making and (2) to use this prioritization tool to determine if resilient foods are more cost effective than AGI safety (which would make resilient food the highest priority area of GCR/X-risk mitigation research). As I am not qualified to directly weigh in on the extent to which the authors’ achieved either goal, I will reflect on the “worthiness” of this goal within the broader context of work going on in the fields of X-risk/GCR, long-termism, science policy, and public policy - and the extent to which the authors’ findings are effectively communicated to these audiences. 
Within this broader context, I believe that these are indeed worthy (and urgent) objectives. The effective prioritization of scarce resources to the myriad potential R&D projects that could (1) reduce key uncertainties, (2) improve political decision-making, and (3) provide solutions that decrease the impact and/or likelihood of civilization-ending risk events is a massive and urgent research challenge. Governments and granting agencies are desperate for rigorous, evidence-based guidance on how to allocate finite funding across candidate projects. Such prioritization is impeded by uncertainty about the potential benefits of various R&D activities (partially resulting from uncertainty about the likelihood and magnitude of the risk event itself - but also from uncertainty about the potential uncertainty-reducing and harm/likelihood-reducing “power” of the R&D). Therefore, the authors’ cost-effectiveness model, which attempts to decrease uncertainty about the potential uncertainty-reducing and harm/likelihood-reducing “power” of resilient food R&D and compare it to R&D on AGI safety, is an important contribution. It combines and applies a number of existing analytical tools in a novel way and proposes a tool for quantifying the relative value of (deeply uncertain) R&D projects competing for scarce resources.
Overall, the authors are cautious and vigilant in qualifying their claims - which is essential when conducting analysis that relies on the quasi-quantiative aggregation of the (inter)subjective beliefs of experts and combines several models (each with their own assumptions). 
Theoretical/epistemic uncertainty
I largely agree with the authors’ dismissal of theoretical/epistemic uncertainty (not that they dismiss its importance or relevance - simply that they believe there is essentially nothing that can be done about it in their analysis). Their suggestion that “results should be interpreted in an epistemically reserved manner” (essentially a plea for intellectual humility) should be a footnote in every scholarly publication - particularly those addressing the far future, X-risk, and value estimations of R&D. 
However, the authors could have bolstered this section of the paper by identifying some potential sources of epistemic uncertainty and suggesting some pathways for further research that might reduce it. I recognize that they are both referring to acknowledged epistemic uncertainties - which may or may not be reducible - as well as unknown epistemic uncertainties (i.e., ignorance - or what they refer to as “cluelessness”). It would have been useful to see a brief discussion of some of these acknowledged epistemic uncertainties (e.g., the impact of resilient foods on public health, immunology, and disease resistance) to emphasize that some epistemic uncertainty could be reduced by exactly the kind of resilient food R&D they are advocating for.             
Presentation of model outputs
When effectively communicating uncertainties associated with research findings to multiple audiences, there is a fundamental tradeoff between the rigour demanded by other experts and the digestibility/usability demanded by decision makers and lay audiences. For example, this tradeoff has been well-documented in the literature on the IPCC’s uncertainty communication framework (e.g., Janzwood & Millar 2020). What fellow-modelers/analysts want/need is usually different from what policymakers want/need. The way that model outputs are communicated in this article (e.g. 84% confidence that the 100 millionth dollar is more cost-effective) leans towards rigour and away from digestibility/usability. A typical policymaker who is unfamiliar with the modeling tools used in this analysis may assume that an 84% probability value was derived from historical frequencies/trials in some sort of experiment - or that it simply reflects an intersubjective assessment of the evidence by the authors of the article. Since the actual story for how this value was calculated is rather complex (it emerges from a model derived from the aggregation of the outputs of two sub-models, which both aggregate various types of expert opinions and other forms of data) - it might be more useful to communicate the final output qualitatively. 
This strategy has been used by the IPCC to varying levels of success. These qualitative uncertainty terms can align with probability intervals. For example, 80-90% confidence could be communicated as “high confidence” or “very confident.” >90% could be communicated as “extremely confident.” There are all sorts of interpretation issues associated with qualitative uncertainty scales - and some scales are certainly more effective than others (again, see Janzwood & Millar 2020) but it is often useful to communicate findings in two “parallel tracks” - one for experts and one for a more lay/policy-focused audience.
Placing the article’s findings within the broader context of global priorities and resource allocation
Recognizing the hard constraints of word counts - and that a broader discussion of global priorities and resource allocation was likely “out of scope” - this article could be strengthened (or perhaps simply expanded upon in future work) by such a discussion. The critical piece of context is the scarcity of resources and attention within the institutions making funding decisions about civilization-saving R&D (governments, granting organizations, private foundations, etc.). There are two dimensions worth discussing here. First, R&D activities addressing risks that are generally considered low-probability/high-impact with relatively long timelines (although I don’t think the collapse of global agricultural would qualify as low-risk - nor is the likely timeline terribly long - but those are my priors) are competing for scarce funding/attention against R&D activities addressing lower-impact risks believed to be shorter-term and more probable (e.g., climate change, the next pandemic, etc.). I think most risk analysts - even hardcore “long-termists” - would agree that an ideal “R&D funding portfolio” be somewhat diversified across these categories of risk. It is important to acknowledge the complexity associated with resource allocation - not just between X-risks but between X-risks and other risks. 
Second, there is the issue of resource scarcity itself. On the one hand, there are many “high value” candidate R&D projects addressing various risks that societies can invest in - but only a finite amount of funding and attention to allocate between them. So, these organizations must make triage decisions based on some criteria. On the other hand, there are also a lot of “low” or even “negative value” R&D activities being funded by these organizations - in addition to other poor investments - that are providing little social benefit or are actively increasing the likelihood/magnitude of various risks. I believe that it is important in these sort of discussions about R&D prioritization and resource scarcity to point out that the reosource pool need not be this shallow - and to identify some of the most egregious funding inefficiencies (e.g., around fossil fuel infrastructure expansion). It should go without saying— but ideally, we could properly resource both resilient food and AGI safety research.
","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",65,,,,,,,,,70,,,80,,,,,,80,,,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",3.5,3.5,,,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",,,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,Not qualified,,,Not qualified,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Long term cost-effectiveness of resilient foods for global catastrophes compared to artificial general intelligence safety,https://unjournal.pubpub.org/pub/y2a1lbzv,International Journal of Disaster Risk Reduction,"50_published evaluations (on PubPub, by Unjournal)",,,2025-07-07T08:51:58.834-04:00,
Long Term Cost-Effectiveness of Resilient Foods for Global Catastrophes Compared to Artificial General Intelligence Safety,https://linkinghub.elsevier.com/retrieve/pii/S2212420922000176,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Anca Hanea,,,"I really enjoyed reading this paper and I did learn a lot as well, so thank you for putting it together. It is a very clearly presented, very dense piece of research. My area of expertise however is risk and decision analysis under uncertainty. I am a probabilistic modeller with loads of experience in structured expert judgement used to quantify uncertainty when data are sparse or lacking. My evaluation will therefore not cover the application area as such, as I have no experience with catastrophic or existential risk. I assume the cited literature is appropriate and not a non-representative sample, but I did not spend time verifying this assumption.
At a first read, both the title and the abstract left me wondering if the present analysis compares cost-effectiveness (of resilient foods) with safety (of AGI), which would’ve been a strange comparison to make. However, after reading the very clear (and dense) Introduction, things became very clear. The only minor comment I have about the Introduction is that it sounds more ambitious than what the results provide with respect to the second objective.
The Methods section is well organised and documented, but once in a while it lacks clarity and it uses terminology that may or may not be appropriate. Here’s a list of things Ii found a bit confusing:
Terminology 
the first sentence mentioned “parameters” without the context of what these parameters may be (sometimes random variables are called parameters, some other times the parameters of a distribution are referred to as parameters, etc)  
The probability distribution of the “expected cost effectiveness”. Is “expected” in this context meant in a probabilistic sense, i.e., the expectation of the random variable “cost effectiveness”?
The submodels for food and AGI are said to be “independent”; is this meant in a probabilistic way? Are there no hidden/not modelled variables that influence both?
The “expert” model was quite confusing for me, maybe because “Sandberg” and the reference number after “Sandberg” don’t match, or maybe because I was expecting a survey vs. expert judgement quantification of uncertainty. As I said (structured) expert judgement is one of my interests (Hanea et al. 2021).
In the caption of fig 2, “index nodes” and “variable nodes” are introduced. Index nodes are later described, but I don't think I understood what was meant by “variable” nodes. Aren’t all probabilistic nodes variable?
Underlying assumptions/definitions
Throughout the methods section I missed a table with a list of all variables, how where they measured, on what sort of scale, or using what formula, where were they quantified from (data, surveys, literature +reference, and if taken from other studies, what were the limitations of those studies)
Some of the parameters of the, say, Beta distributions are mentioned but not justified
The structure of the models is not discussed. How did you decide that this is a robust structure (no sensitivity to structure performed as far as I understood)
What is meant  by “the data from surveys was used directly instead of constructing continuous distributions”?
The arcs in Fig 1 are unclear, some of them seem misplaced, while others seem to be missing. This can be a misunderstanding from my part, so maybe more text about Fig.1 would help.
It is unclear if the compiled data sets are compatible. I think the quantification of the model should be documented better or in a more compact way.
The Results section is very clear and neatly presented and I did enjoy the discussion on the several types of uncertainty.
It is great that the models are available upon request, but it would be even better if they would be public so the computational reproducibility could be evaluated as well. 
Some of the references are missing links in the text, and at least one does not link to the desired bibitem.
","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",80,60,90,,,,70,90,50,80,70,90,85,65,95,73,50,95,85,70,90,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4,3.5,3,5,3,5,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",,,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,"Many components to rate, composite uncertainty in my rating :| many choices left unexplained (on the other hand there were so many choices that it would’ve been hard to justify all), the majority seemed reasonable. Validity and robustness for me are hard to assess, but I based my numbers on the discussion about uncertainties and the sensitivity analysis.",,"Both excellent, but because the paper is so dense it’s sometimes hard to follow.",The large uncertainty and a reduced score for replicable is due to the various data sources/references used in the quantification of the models. Many can be recovered from other papers which may or may not be replicable/straight forward. A table with all data sources and functional relationships would’ve been useful. Also a bit more info on model E would help reduce my uncertainty here.,,Biased by the knowledge that it has been published and my trust in the reviewing process.,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Long term cost-effectiveness of resilient foods for global catastrophes compared to artificial general intelligence safety,https://unjournal.pubpub.org/pub/y2a1lbzv,International Journal of Disaster Risk Reduction,"50_published evaluations (on PubPub, by Unjournal)",,,2025-07-07T08:51:55.353-04:00,
Long Term Cost-Effectiveness of Resilient Foods for Global Catastrophes Compared to Artificial General Intelligence Safety,https://linkinghub.elsevier.com/retrieve/pii/S2212420922000176,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Alex Bates,,,"This is a very interesting paper on an important and neglected topic. I’d be surprised if I ever again read a paper with such potential importance to global priorities. The authors motivate the discussion well, and should be highly commended for their clear presentation of the structural features of their model, and the thoughtful nature in which uncertainty was addressed head-on in the paper.
Overall, I suspect the biggest contribution this paper will make is contextualising the existing work done by the authors on resilient food into the broader literature of long-termist interventions. This is a significant achievement, and the authors should feel justifiably proud of having accomplished it. However, the paper unfortunately has a number of structural and technical issues which should significantly reduce a reader’s confidence in the quantitative conclusions which aim to go beyond this contextualisation.
In general, there are three broad areas where I think there are material issues with the paper:
 The theoretical motivation for their specific philosophy of cost-effectiveness, and specifically whether this philosophy is consistent throughout the essay
The appropriateness of the survey methods, in the sense of applying the results of a highly uncertain survey to an already uncertain model
Some specific concerns with parameterisation
None of these concerns touch upon what I see [as] the main point of the authors, which I take to be that ‘fragile’ food networks should be contextualised alongside other sources of existential risk. I think this point is solidly made, and important. However, they do suggest that significant additional work may be needed to properly prove the headline claim of the paper, which is that in addition to being a source of existential risk the cost-effectiveness of investing in resilient food is amongst the highest benefit-per-cost of any existential risk mitigation.
Structure of cost-effectiveness argument
One significant highlight of the paper is the great ambition it shows in resolving a largely intractable question. Unfortunately, I feel this ambition is also something of a weakness of the paper, since it ends up difficult to follow the logic of the argument throughout.
Structurally, the most challenging element of this paper in terms of argumentative flow is the decision to make the comparator for cost-effectiveness analysis ‘AGI Catastrophe’ rather than ‘do nothing’. My understanding is that the authors make this decision to clearly highlight the importance of resilient food – noting that, “if resilient foods were more cost effective than AGI safety, they could be the highest priority [for the existential risk community]” (since the existential risk community currently spends a lot on AGI Risk mitigation). So roughly, they start with the assumption that AI Risk must be cost-effective, and argue that anything more cost-effective than this must therefore also be cost-effective. The logic is sound, but this decision causes a number of problems with interpretability, since it requires the authors to compare an already highly uncertain model of food resilience against a second highly uncertain model of AGI risk.
The biggest issue with interpretability this causes is that I struggle to understand what features of the analysis are making resilient food appear cost-effective because of some feature of resilient food, and which are making resilient food appear cost-effective because of some feature of AI. The methods used by the authors mean that a mediocre case for resilient food could be made to look highly cost-effective with an exceptionally poor case for AI, since their central result is the multiplier of value on a marginally invested dollar for resilient food vs AI. This is important, because the authors’ argument is that resilient food should be funded because it is more effective than AI Risk management, but this is motivated by AI Risk proponents agreeing [that] AI Risk is important – in scenarios where AI Risk is not worth investing in then this assumption is broken and cost effectiveness analysis against a ’do nothing’ alternative is required. For example, the authors do not investigate scenarios where the benefit of the intervention in the future is negative because “negative impacts would be possible for both resilient foods and AGI safety and there is no obvious reason why either would be more affected”. While this is potentially reasonable on a mathematical level, it does mean that it would be perfectly possible for resilient foods to be net harmful and the paper not correctly identify that funding them is a bad idea – simply because funding AI Risk reduction is an even worse idea, and this is the only given alternative. If the authors want to compare AGI risk mitigation and resilient foods against each other without a ‘do nothing’ common comparator (which I do not think is a good idea), they must at the very least do more to establish that the results of their AI Risk model map closely to the results which cause the AI Risk community to fund AI Risk mitigation so much. As this is not done in the paper, a major issue of interpretability is generated.
A second issue this causes is that the authors must make an awkward ‘assumption of independence’ between nuclear risk, food security risk and AI risk. Although the authors identify this as a limitation of their modelling approach, the assumption does not need to be made if AI risk is not included as a comparator in the model. I don’t think this is a major limitation of the work, but an example of how the choice of comparator has an impact on structural features of the model beyond just the comparator.
 More generally, this causes the authors to have to write up their results in a non-natural fashion. As an example of the sort of issues this causes, conclusions are expressed in entirely non-natural units in places (“Ratio of resilient foods mean cost effectiveness to AGI safety mean cost effectiveness” given $100m spend), rather than units which would be more natural (“Cost-effectiveness of funding resilient food development”). I cannot find expressed anywhere in the paper a simple table with the average costs and benefits of the two interventions, although a reference is made to Denkenberger & Pearce (2016) where these values were presented for near-term investment in resilient food. This makes it extremely hard for a reader to draw sensible policy conclusions from the paper unless they are already an expert in AGI risk and so have an intuitive sense of what an intervention which is ‘3-6 times more cost-effective than AGI risk reduction’ looks like. The paper might be improved by the authors communicating summary statistics in a more straightforward fashion. For example, I have spent some time looking for the probability the model assigns to no nuclear war before the time horizon (and hence the probability that the money spent on resilient food is ‘wasted’ with respect to the 100% shortfall scenario) but can’t find this – that seems to be quite an important summary statistic but it has to be derived indirectly from the model.
Fundamentally, I don’t understand why both approaches were not compared to a common scenario of ‘do nothing’ (relative to what we are already doing). The authors’ decision to compare AGI Risk mitigation to resilient foods directly would only be appropriate if the authors expect that increasing funding for resilient food decreased funding for AI safety (that is to say, the authors are claiming that there is a fixed budget for AI-safety-and-food-resilience, and so funding for one must come at the expense of the other). This might be what the authors have in mind as a practical consequence of their argument, as there is an implication that funding for resilient foods might come from existing funding deployed to AGI Risk. But it is not logically necessary that this is the case, and so it creates great conceptual [confusion] to include it in a cost-effectiveness framework that requires AI funding and resilient food funding to be strictly alternatives. To be clear, the ‘AI subunit’ is interesting and publishable in its own right, but in my opinion simply adds complexity and uncertainty to an already complex paper.
Continuing on from this point, I don’t understand the conceptual framework that has the authors consider the value of invested dollars in resilient food at the margin. The authors’ model of the value of an invested dollar is an assumption that it is distributed logarithmically. Since the entire premise of the paper hinges on the reasonability of this argument, it is very surprising there is no sensitivity analysis considering different distributions of the relationship between intervention funding and value. Nevertheless, I am also confused as to the model even on the terms the authors describe; the authors’ model appears to be that there is some sort of ‘invention’ step where the resilient food is created and discovered (this is mostly consistent with Denkenberger & Pearce (2016), and is the only interpretation consistent with the question asked in the survey). In which case, the marginal value of the first invested dollar is zero because the ’invention’ of the food is almost a discrete and binary step. The marginal value per dollar continues to be zero until the 86 millionth dollar, where the marginal value is the entire value of the resilient food in its entirety. There seems to be no reason to consider the marginal dollar value of investment when a structural assumption made by the authors is that there is a specific level of funding which entirely saturates the field, and this would make presenting results significantly more straightforward – it is highly nonstandard to use marginal dollars as the unit of cost in a cost-effectiveness analysis, and indeed is so nonstandard I’m not certain fundamental assumptions of cost-effectiveness analysis still hold. I can see why the authors have chosen to bite this bullet for AI risk given the existing literature on the cost of preventing AI Catastrophe, but there seems to be no reason for it when modelling resilient food and it departs sharply from the norm in cost-effectiveness analysis.
Finally, I don’t understand the structural assumptions motivating the cost-effectiveness of the 10% decline analysis. The authors claim that the mechanism by which resilient foods save lives in the 10% decline analysis is that “the prices [of non-resilient food] would go so high that those in poverty may not be able to afford food” with the implication that resilient foods would be affordable to those in poverty and hence prevent starvation. However, the economic logic of this statement is unclear. It necessitates that the production costs of resilient food is less than the production costs of substitute non-resilient food at the margin, which further implies that producers of resilient food can command supernormal profits during the crisis, which is to say the authors are arguing that resilient foods represent potentially billions of dollars of value to their inventor within the inventor’s lifetime. It is not clear to me why a market-based solution would not emerge for the ‘do nothing’ scenario, which would be a critical issue with the authors’ case since it would remove the assumption that ‘resilient food’ and ‘AGI risk’ are alternative uses of the same money in the 10% scenario, which is necessary for their analysis to function. The authors make the further assumption that preparation for the 100% decline scenario is highly correlated with preparation for the 10% decline scenario, which would mean that a market-based solution emerging prior to nuclear exchange would remove the assumption that ‘resilient food’ and ‘AGI risk’ are alternative uses of the same money in the 100% decline scenario. A supply and demand model might have been a more appropriate model for investigating this effect. Once again, I note that the supply and demand model alone would have been an interesting and publishable piece of work in its own right.
Overall, I think the paper would have benefitted from more attention being paid to the underlying theory of cost-effectiveness motivating the investigation. Decisions made in places seem to have multiplied uncertainty which could have been resolved with a more consistent approach to analysis. As I highlighted earlier, the issues only stem from the incredible ambition of the paper and the authors should be commended for managing to find a route to connect two separate microsimulations, an analysis of funding at the margin and a supply-and-demand model. Nevertheless, the combination of these three approaches weakens the ability to draw strong conclusions from each of these approaches individually.
Methods
With respect to methods, the authors use a Monte Carlo simulation with distributions drawn from a survey of field experts. The use of a Monte Carlo technique here is an appropriate choice given the significant level of uncertainty over parameters. The model appears appropriately described in the paper, and functions well (I have only checked the models in Guesstimate, as I could not make the secondary models in Analytica function). A particular highlight of the paper is the figures clearly laying out the logical interrelationship of elements of the model, which made it significantly easier to follow the flow of the argument. I note the authors use ‘probability more effective than’ as a key result, which I think is a natural unit when working in Guesstimate. This is entirely appropriate, but a known weakness of the approach is that it can bias in favour of poor interventions with high uncertainty. The authors could also have presented a SUCRA analysis which does not have this issue, but they may have considered and rejected this approach as unnecessary given the entirely one-sided nature of the results which a SUCRA would not have reversed.
The presentation of the sensitivity analysis as ‘number of parameters needed to flip’ is nonstandard, but a clever way to intuitively express the level of confidence the authors have in their conclusions. Although clever, I am uncertain if the approach is appropriately implemented; the authors limit themselves to the 95% CI for their definition of an ‘unfavourable’ parameter, and I think this approach hides massive structural uncertainty with the model. For example, in Table 5 the authors suggest their results would only change if the probability of nuclear war per year was 4.8x10^-5 (plus some other variables changing) rather than their estimated of 7x10^-3 (incidentally, I think the values for S model and E model are switched in Table 5 – the value for pr(nuclear war) in the table’s S model column corresponds to the probability given in the E model). But it is significantly overconfident to say that risk of nuclear war per year could not possibly be below 4.8x10^-5, so I think the authors overstate their certainty when they say “reverting [reversing?] the conclusion required simultaneously changing the 3-5 most important parameters to the pessimistic ends”; in fact it merely requires that the authors have not correctly identified the ‘pessimistic end’ of any one of the five parameters, which seems likely given the limitations in their data which I will discuss momentarily. I personally would have found one- and two-dimensional threshold analysis a more intuitive way to present the results, but I think the authors have a reasonable argument for their approach. As described earlier, I have some concerns that an appropriate amount of structural sensitivity analysis was undertaken, but the presentation of uncertainty analysis is appropriate in its own terms (if somewhat nonstandard).
Overall, I have no major concerns about the theory or application of the modelling approach. However, I have a number of concerns with the use of the survey instrument:
First, the authors could have done more to explain the level of uncertainty their survey instrument contains. They received eight responses, which is already a very low number of responses for a quantitative survey. In addition, two of the eight responses were from authors of the paper. The authors discuss ‘response bias’ and ‘demand characteristic bias’ which would not typically be applied to data generated by an approximately autoethnographic process – it is obvious that the authors of a survey instrument know what purpose the instrument is to be used for, and have incentives to make the survey generate novel and interesting findings. It might have been a good sensitivity analysis to exclude responses from the authors and other researchers associated with ALLFED since there is a clear conflict of interest that could bias results here.
Second, issues with survey data collection are compounded by the fact that some estimates which are given in the S Model are actually not elicited with the survey technique – they are instead cited to Denkenberger & Pearce (2016) and Denkenberger & Pearce (2018). This is described appropriately in the text, but not clearly marked in the summary Table 1 where I would expect to see it, and the limitation this presents is not described clearly. To be explicit, the limitation is that at least two key parameters in the model are based on a sample of the opinions of two of the eight survey respondents, rather than the full set of eight respondents. As an aside on presentation, the decision to present lower and upper credible intervals in Table 1 rather than median is non-standard for an economics paper, although perhaps this is a discipline-specific convention I am unaware of. Regardless, I’m not sure it is appropriate to present the lowest of eight survey responses as the ‘5th percentile’, as it is actually the 13th percentile and giving 95% confidence intervals implies a level of accuracy the survey instrument cannot reach. While I appreciate the 13th percentile of 8 responses will be the same as the 5th centile of 100 samples drawn from those responses, this is not going to be clear to a casual reader of the paper. ‘Median (range)’ might be a better presentation of the survey data in this table, with better clarity on where each estimate comes from. Alternatively, the authors could look at fitting a lognormal distribution to the survey results using e.g. method of moments, and then resample from the new distribution to create a genuine 95% CI. Regardless, given the low number of responses, it might have been appropriate simply to present all eight estimates for each relevant parameter in a table. 
Third, the authors could have done more to make it clear that the ‘Expert Model’ was effectively just another survey with an n of 1. Professor Sandburg, who populated the Expert Model, is also an author on this paper and so it is unclear what if any validation of the Expert Model could reasonably have been undertaken – the E model is therefore likely to suffer from the same drawbacks as the S model. It is also unclear if Professor Sandburg knew the results of the S Model before parameterising his E Model – although this seems highly likely given that 25% of the survey’s respondents were Professor Sandburg’s co-authors. This could be a major source of bias, since presumably the authors would prefer the two models to agree and the expert parameterising the model is a co-author. I also think more work is needed to be done establishing the Expert’s credentials in the field of agricultural R&D (necessary for at least some of the parameter estimates); although I happily accept Professor Sandburg is a world expert on existential risk and a clear choice to act as the parameterising ‘expert’ for most parameters, I think there may have been alternative choices (such as agricultural economists) who may have been more obviously suited to giving some estimates. There is no methodological reason why one expert had to be selected to populate the whole table, and no defence given in the text for why one expert was selected - the paper is highly multidisciplinary and it would be surprising if any one individual had expert knowledge of every relevant element. Overall, this limitation makes me extremely hesitant to accept the authors’ argument that the fact that S model and E model are both robust means the conclusion is equally robust
Generally, I am sympathetic to the authors’ claim that there is unavoidable uncertainty in the investigation of the far future. However, the survey is a very major source of avoidable uncertainty, and it is not a reasonable decision of the authors to present the uncertainty due to their application of survey methods as the same kind of thing as uncertainty about the future potential of humanity. There are a number of steps the authors could have taken to improve the validity and reliability of their survey results, some of which would not even have required rerunning the survey (to be clear however, I think there is a good case for rerunning the survey to ensure a broader panel of responses). With the exception of the survey, however, methods were generally appropriate and valid.
Parameter estimates
Notwithstanding my concerns about the use of the survey instrument, I have some object level concerns with specific parameters described in the model.
 The discount rate for both costs and benefits appears to be zero, which is very nonstandard in economic evaluation. Although the authors make reference to “long termism, the view that the future should have a near zero discount rate”, the reference for this position leads to a claim that a zero rate of pure time preference is common, and a footnote observing that “the consensus against discounting future well-being is not universal”. To be clear, pure time preference is only one component of a well-constructed discount rate and therefore a discount rate should still be applied for costs, and probably for future benefits too. Even notwithstanding that I think this is an error of understanding, it is a limitation of the paper that discount rates were not explored, given they seem very likely to have a major impact on conclusions.  
A second concern I have relating to parameterisation is the conceptual model leading to the authors’ proposed costing for the intervention. The authors explain their conceptual model linking nuclear war risk to agricultural decline commendably clearly, and this expands on the already strong argument in Denkenberger & Pearce (2016). However, I am less clear on their conceptual model linking approximately $86m of research to the widescale post-nuclear deployment of resilient foods. The assumption seems to be (and I stress this is my assumption based on Denkenberger & Pearce (2016) – it would help if the authors could make it explicit) that $86m purchases the ‘invention’ of the resilient food, and once the food is ‘invented’ then it can be deployed when needed with only a little bit of ongoing training (covered by the $86m). This seems to me to be an optimistic assumption; there seems to be no cost associated with disseminating the knowledge, or any raw materials necessary to culture the resilient food. Moreover, the model seems to structurally assume that distribution chains survive the nuclear exchange with 100% certainty (or that the materials are disseminated to every household which would increase costs), and that an existing resilient food pipeline exists at the moment of nuclear exchange which can smoothly take over from the non-resilient food pipeline.
I have extremely serious reservations about these points. I think it is fair to say that an economics paper which projected benefits as far into the future as the authors do here without an exploration of discount rates would be automatically rejected by most editors, and it is not clear why the standard should be so different for existential risk analysis. A cost of $86m to mitigate approximately 40% of the impact of a full-scale nuclear war between the US and a peer country seems prima facie absurd, and the level of exploration of such an important parameter is simply not in line with best practice in a cost-effectiveness analysis (especially since this is the parameter on which we might expect the authors to be least expert). I wouldn’t want my reservations about these two points to detract from the very good and careful scholarship elsewhere in the paper, but neither do I want to give the impression that these are just minor technical details – these issues could potentially reverse the authors’ conclusions, and should have been substantially defended in the text.
Conclusions
Overall, this is a novel and insightful paper which is unfortunately burdened with some fairly serious conceptual issues. The authors should be commended for their clear-sighted contextualisation of resilient foods as an issue for discussion in existential risk, and for the scope of their ambition in modelling. Academia would be in a significantly better place if more authors tried to answer far-reaching questions with robust approaches, rather than making incremental contributions to unimportant topics.
Where the issues of the paper lie are structural weaknesses with the cost-effectiveness philosophy deployed, methodological weaknesses with the survey instrument and two potentially conclusion-reversing issues with parameterisation which should have been given substantially more discussion in the text. I am not convinced that the elements of the paper which are robust are sufficiently robust to overcome these weaknesses – my view is that it would be premature to reallocate funding from AI Risk reduction to resilient food on the basis of this paper alone. The most serious conceptual issue which I think needs to be resolved before this can happen is to demonstrate that ‘do nothing’ would be less cost-effective than investing $86m in resilient foods, given that the ‘do nothing’ approach would potentially include strong market dynamics leaning towards resilient foods. I agree with the authors that an agent-based model might be appropriate for this, although a conventional supply-and-demand model might be simpler.
I really hope the authors are interested in publishing follow-on work, looking at elements which I have highlighted in this review as being potentially misaligned to the paper that was actually published but which are nevertheless potentially important contributions to knowledge. In particular, the AI subunit is novel and important enough for its own publication.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",40,20,60,,,,50,60,40,30,20,60,60,40,75,70,40,75,90,60,95,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",2,2,1,2,1,2,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",,,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),See main review,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,"Very major limitations around the survey method, and implementation of certain parts of the parameter sensitivity analysis. However many elements of a high standard","The paper itself makes an important argument about resilient foods, but I don’t know if the additional element of AGI risk adds much to Denkenberger & Pearce (2016)","Major limitations around the logic and communication of the theoretical model of cost-effectiveness used in the paper. Minor limitations of readability and reporting which could have been addressed before publication (such as reporting 95% CIs without medians, and not reporting overall cost and benefit estimates)","Provided models are shared with any reader who asks, I couldn’t ask for more here. Limitations of survey replicability (particularly E model) prevent perfect score","I’d be surprised if I ever read a paper with more relevance to global priorities, although as mentioned there are a few version of this argument circulating such as Denkenberger & Pearce (2016)",,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Long term cost-effectiveness of resilient foods for global catastrophes compared to artificial general intelligence safety,https://unjournal.pubpub.org/pub/y2a1lbzv,International Journal of Disaster Risk Reduction,"50_published evaluations (on PubPub, by Unjournal)",,,2025-07-07T08:46:58.816-04:00,
Banning wildlife trade can boost demand for unregulated threatened species,"https://ideas.repec.org/p/osf/socarx/s6gwu.html#:~:text=The%20results%20show%20that%20bans,the%20benefits%20of%20trade%20bans","This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Evaluator 2,,,"Note from The Unjournal: We made some very minimal corrections to spelling, punctuation, and grammar below.
A generally well-written/reasonably well argued paper addressing an important implication of the wildlife trade - the indirect, and often incidental effect of trade bans on the sale of species that may be sought in markets by buyers as alternatives. The paper argues that bans on the trade of species of conservation concern has spillover effects into the trade of closely related species to meet market demand - the premise is straightforward and often talked about but there have not been that many studies to my knowledge that explicitly tests such as hypothesis. Well done to the authors for putting this together.
The paper provides a timely case study of the nature and broader consequences of trade bans in the context of the wildlife trade, and why these consequences need to be more closely looked at after their implementation. Data presentation and analytical framework based on the application of SDID appears sound - and the authors have also gone on to conduct sensitivity analyses.
I would recommend the publication of this paper with some minor revisions, including tightening the language at many parts of the paper for clarity and coherence, and also more caveats for the (public) dataset used.
Principal claim - trade bans have spillover effects into the sale of species not specifically targeted by the ban per se. The paper tests this hypothesis and found what I would consider to be reasonable evidence/support (although the volume of the species sold are relatively small in my view). That said, I haven’t seen many studies that have investigated the causality of policy changes on trade of specific species, so I find it interesting to see this being demonstrated here.
My confidence on the claims made - 70-75%.
Analytical approach is sound and novel (this is the first time I have seen the use of SDID to this sort of analyses), but I would recommend more explicit recognition of the limitations that would come with such a dataset (do you think there is leakage, sale of the banned species through alternative markets). Blanket bans can drive several types of outcomes in the trade of wildlife, and in many parts of the world where governance is weak, there is bought to be leakage into the black market (so what is reported formally may not fully capture the scale of trade) - this would need to be made clever.
Ideally, it would be good to explore such patterns for a large suite of species (and species that are traded in high volumes) but I appreciate that this may not always be realistically possible.
Specific comments:
P2: More background to the online wildlife trade should be given in the intro - for context setting. Suggest to provide examples of species and species groups popular in the online trade. I find that the intro currently reads very generically, and not particularly informative at this stage.
P6: Interested to see how you derived these numbers for the alternative taxa to be traded. Please provide citations. Also hard to define what is ‘substitutable’ - in the eyes of buyers, although one reasonable position is to provide lists of closely related species.
P8: Sounds more like you are providing policy and management recommendations, than ‘implications’. Lots of recommended steps provided here - do make sure they are well substantiated- and backed by sources
P8: Has there been any examples where a species has been substituted in a formal/management-driven way?
P8: How do you recommend that the monitoring be done, and how many taxa can you effectively monitor, to determine the nature and direction of these market shifts?
P8: Can cut away the usual discussion about how biodiversity conservation is afflicted by the lack of funds. Its well known, and does not add a lot to your discussion.
P9: Do you have a good reasoning to want to pursue collaboration beyond CITES? Could CITES provide the umbrella for these collaborations? I find the last bits of the discussion to be rather general, and not much of a value-add.
P11: What steps did you take to manually check and confirm the species names?
Minor comments
P1: ‘Regulations on the harvest and use of natural resources
P1: ‘knee-jerk’ probably captures what you mean more clearly.
P1: ‘heterogenous’
P1: What kind of modern technologies? Vague.
P2: Not clear what you mean by ‘distribution’ - of the species afflicted? Please re-word
P6: I think ‘show’ is a better word.
P6: Side or incidental effects
P6: ‘has important policy implications’
Figure 5: left panel vs right panel
P7: be specific - harms conservation by driving up demand (for the alternatives) - and increased wild harvest
P7: accentuate threats to biodiversity - the trade bans can also effectively undermine the conservation of species and species groups
P8: accelerate declines of species
P10: demand
P10: increased volumes of harvest for the trade
P10: What is the reasoning why these three species were chosen?
P11: each taxon banned","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",75,,,,,,80,,,70,,,70,,,70,,,90,,,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",3,3,,,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",I have worked in the field of biodiversity conservation for about 10 years.,"I review about 20-30 papers each year, and on average, 5-8 project proposals.",False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,Relevance to GP: 80,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Banning wildlife trade can boost demand for unregulated threatened species,https://unjournal.pubpub.org/pub/banning-wildlife-eval-summ,Conservation Letters,"50_published evaluations (on PubPub, by Unjournal)",,,2025-07-07T08:43:17.484-04:00,
Banning wildlife trade can boost demand for unregulated threatened species,"https://ideas.repec.org/p/osf/socarx/s6gwu.html#:~:text=The%20results%20show%20that%20bans,the%20benefits%20of%20trade%20bans","This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Jia Huan Liew,,,"Kubo et al assessed the possible impacts of wildlife trade bans on non-target species using an online auction dataset spanning 10-years. The authors demonstrated spillover effects in the form of increased trade volume involving closely related species. The spillover effects differed between the three broad groups studied, leading the authors to posit that spillover effects may differ as a function of demand for the banned species, as well as the availability of legal alternatives in the market. Overall, I thought that this is an interesting and topical paper that provides important support for anecdotes of unintended negative outcomes from trade bans. I was also intrigued by the authors’ application of synthetic difference-in-differences (SDID) which seemed a potentially powerful method for assessing the broad effects of policy decisions.
Despite my general appreciation of this work, I feel that the evidence supporting the authors’ overarching conclusion was not presented with sufficient clarity. This is because the modelling approach is fairly advanced, yet the details provided were too scant.
The most important component of this study, in my opinion, lies in the authors’ selection of “spillover” and “control” species, as I expect this to be highly influential on SDID outcomes. For “spillover” species, I recommend the authors better justify their selection by explaining, from a buyer’s point-of-view, why these would be realistic alternatives. The authors provide strong justification for giant water bugs (i.e., same market name), but not for salamanders and freshwater fish. “Spillover” species for the latter two were close-relatives, which could be a reasonable choice if the authors cite evidence to establish the logic that underlies a potential buyer’s decision to choose phylogenetically close alternatives in the event of a ban. As these are likely to be kept as pets, perhaps other species traits (e.g., appearance, size) that may not necessarily be linked to phylogeny may be more important? To clarify, I do not believe that the authors’ approach is wrong. I do, however, suggest the authors better explain their selection process.
Relatedly, “control” units were defined as “trades in the same categories as banned species, excluding potential spillover species” (Page 12, Paragraph 2). This is too vague for readers to follow and potentially replicate. I could not deduce what the term “categories” refer to. The identities of top control units were detailed in Fig S2 and Fig S4, but the texts were in Japanese (Fig S2) or too small to read (S4). From what I could tell, some of the control units were congeners of the banned salamander species and selected “spillovers”. I therefore wondered about how phylogenetic relatedness of “spillover” species were ranked and how the authors decided that spillover effects would not also affect the trade of “control” species.
With my admittedly limited understanding of SDID, I am also wondering if the issues regarding “spillover” and “control” species selection could have been averted if the authors use an unrelated group of animals (e.g., turtles) to parameterise their synthetic controls, assuming this group was not subject to similar bans. This may also help overcome the potential issue of any spillover effects in the currently selected “control” units which could obfuscate the estimation of DiD values. If the appeal of SDID was the allowance for differences in trend between intervention and control groups before the ban, do control units need to be close relatives of the spillover species?
I appreciate the novelty of applying SDID, but I am concerned that there is insufficient context to ease comprehension if this work were to be submitted to journals with a broader readership. I think the description of Eq. 1 as a method to solving the “minimisation problem” epitomises my concern. I could be in the minority, but “minimisation” is not a term I encounter frequently in my reading. Therefore, I did not initially understand why there was a “minimisation problem” that had to be solved, much less understand how to solve it. I suggest the authors provide a brief explanation about what SDID (or even DiD) achieves in simpler terms (e.g., assess the effects of interventions by comparing observed outcomes against predicted outcomes representing non-intervention).
I liked the figures presented in this paper. In particular, I appreciate the clean aesthetics of the plots presented here. However, figures depicting outcomes of SDID in the main text and the supplementary section can be difficult to decipher without additional details about the application of SDID (or even of DID and SC). Without prior knowledge, the captions and text do not provide sufficient information about what the readers should look out for in the plots on the left side of Figures 3, S3, and S5. For instance, the caption mentions “arrows” indicating estimated effects, but the arrows are difficult to see on the plot. Moreover, I recommend the authors include additional information about the vertical lines representing ban enforcement, as well as the significance of trend lines representing post-ban averages and the SDID synthetic control, respectively. This will make it easier to understand what the “estimates” in plots on the right of Figures 3, S3, and S5 signify. Relatedly, the captions specify that plots on the right of these figures represent “estimates concerning trade volumes of each taxon”. In my understanding, these should instead refer to the estimated spillover effects of the ban? If my interpretation is correct, the labelling of a 0 value for estimates (i.e., vertical broken line) as “Trade (n)” is quite confusing. I recommend the use of more precise descriptions in the plot and captions.
I appreciate the concise nature of the paper. The authors did a good job of providing key information but I believe that there is some room for improvement. First, some context about the volume or relative importance of online auctions as a platform for trading in animals could help readers better understand the significance and applicability of findings to the wider wildlife trade. Second, the authors provide additional information about the relevant policies in the methods section, but this information may be better placed in earlier parts of the text to avert confusion about focal species selection. Third, I believe that the argumentation leading to the authors’ conceptualisation of spillover effects (Fig. 5) can be further developed. The authors argue that spillover effects may be diluted when more alternatives are available in the market, but they do not explain what “alternatives” mean in the context of the wildlife (e.g., pet) trade. The text (page 6) assumes that animals in the “freshwater taxon” were potential alternatives to the golden venus chub, while animals within the “salamander taxon” were potential alternatives to the Tokyo salamander. These assumptions imply that potential buyers are unlikely to consider taxonomically distant animals as alternatives to banned species, yet I am unaware of supporting studies/papers. I recommend the authors provide additional justification for this assumption, preferably by citing relevant literature.
Finally, there were several instances of imprecise or unclear writing. I list these below, along with some suggestions for the authors’ consideration:
1) Page 2: “It activated the underground market” suggests that underground markets only came into existence when CITES regulations came into effect. Perhaps consider revising to “These regulations coincided with a growing underground market”.
2) Page 2: “Even a few empirical studies have focused on introducing trade ban policies on banned species” is a confusing sentence. Consider revising to “A small number of empirical studies focus on quantifying the effects of trade bans, but the focus was on species that were the targets of the ban”.
3) Page 7: Two sentences about exotic species trade and native species policies in developed countries were quite confusing to read. I recommend editing the sentences to “An increase in exotic species trade can increase overexploitation risk in source countries and lead to population declines unless appropriate management is implemented. Developing source countries may struggle to cope with the additional management needs as they often struggle to implement robust natural resource governance”.
4) Page 9: “evidence regarding cross-country spillovers” seems to be a very serious issue but no citations were provided to help readers learn more about it. I recommend citing the relevant sources.
5) Page 9: “We suggest the development of a database comprising banned and non-banned species” is a vague statement that may cover all known species. I recommend the authors be more specific, perhaps by narrowing the statement down to species known to be in the trade.
In conclusion, I believe that this is a very promising study with an important, policy-relevant message. However, the paper needs to be revised for clarity. In particular, additional details about the study’s modelling approach will help improve reader comprehension and strengthen the authors’ argument about the significance of spillover effects from trade bans.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",75,,,,,,50,,,80,,,70,,,50,,,90,,,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",3,2.5,,,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",13 years,~60 as a peer-reviewer or editor,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,"I do not have any experience using SDID, and I am therefore uncertain of the validity of analyses performed",,,,Relevance to GP: 65,“The taxonomic/geographical scope of the study may be a barrier to publishing in “higher quality” journals that receive more submissions.”,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Banning wildlife trade can boost demand for unregulated threatened species,https://unjournal.pubpub.org/pub/banning-wildlife-eval-summ,Conservation Letters,"50_published evaluations (on PubPub, by Unjournal)",,,2025-07-07T08:39:08.670-04:00,
The Governance Of Non-Profits And Their Social Impact: Evidence From A Randomized Program In Healthcare In DRC,https://www.nber.org/papers/w30391,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Evaluator 1,,,"This paper uses a randomized controlled trial to show that a World Bank program introducing performance pay, auditing, and feedback raised operating efficiency and reduced infant mortality in non-profit health centers in the DRC. Treatment and control health centers received equal increases in funding. A matching diff-in-diff analysis comparing treatment and control health centers against those not in the experiment shows that increased funding alone increases the number of employees and services offered but does not affect efficiency or infant mortality. Effects appear only after about 7 quarters, suggesting that the “feedback” component of the program was important for teaching health center managers how to meet the incentivized performance targets.
This is a very nicely executed paper on an important and policy-relevant topic, and it was a pleasure to read. RCTs this big are somewhat rare, and obtaining access to administrative outcome can be difficult. To combine both of these features in a context as understudied as the DRC is a real achievement. My congratulations to the authors. I think the paper is very good, and I hope the comments that follow can be useful in helping the authors clarify and improve the paper even further.
My main comment is that I would have liked more discussion of the details of the treatment, especially early on in the paper. The intro refers to “performance-based incentives and feedback” without mentioning exactly which players are receiving the incentives and the feedback. The recent literature on the personnel economics of the state (e.g. Finan Olken Pande 2017) shows that there are lots of choke points in the bureaucracy where service delivery can break down – administrators, managers, and frontline providers can each fail to perform as desired. It’s important that your readers know which of these you are taking aim at, and why you think that’s the right part of the system to target.
By the end of the paper I had learned that the incentives and feedback were provided to the managers (owners?) of the health centers. It would be helpful to have more background about these people, their world, and their incentives.
You characterized the health centers as non-profits; does this mean they are funded by donors? Are they Western or local organizations? Given that the state seems quite interested in their operation, would it make sense to think of them as government contractors? Locating the readers in the economic decisions faced by these operators in the status quo can help us interpret the program’s effect more clearly. You note that non-profit and for-profits are different, but I think it would be nice to be explicit about exactly who has different incentives, how are they different, and what predictions does that lead to.
I would have liked more detail earlier on in the intro about exactly which actors are getting the performance-based pay and the feedback, and why they are the ones who need it. Is it the managers, the owners, the frontline employees? The interpretation of the paper depends crucially on this. I’d love to see a discussion of it in the intro.
The characterization of the treatment itself confused me a bit. The paper groups “auditing and feedback” together, and suggests that the auditing in question is an audit of the feedback, meant to strengthen the reliability of the information which the health centers receive as feedback (p.1). But page 11 gives the impression that the auditing in question is being done on the information provided by the health centers to the central authority (the “community verification system”. [What incentives do communities have to report malfeasance, if their community health center will get more funding for reporting inflated numbers?] Page 11 also mentions a procurement and contracting reform that seems to have been part of the treatment. I think the paper would be well-served by a much more nitty-gritty, hard-headed explanation of the full accountability treatment and how each element of it fits into the theory of change.
Along these lines, I think the paper would benefit from a fuller picture of the possible set of mechanisms (even while I recognize that the experiment wasn’t set up to really nail down mechanisms). For example: You note an increase in the number of services performed – what are the different ways that could happen? Is it that demand increases because quality increases? Is it that nurses and doctors are more likely to show up to work? Is it that nurses are going out to drum up demand among customers? Is it that services that are already happening are more likely to be recorded? Anything you can do to distinguish between these mechanisms is of course welcome, but even just enumerating the possible mechanisms would be really helpful – remember the readers know next to nothing about the context!
It's great that you report outcomes from administrative data, but given that the treatment incentivized outcomes reported by the health centers themselves, a bit more work is in order to convince the reader that the data are reliable.
It sounds like the main outcomes you look at are NOT those on which the incentives were based (which is good!), but you should make this clearer. I didn’t find that explicitly stated until Appendix D!
It would be nice to know something about the outcomes that were incentivized. Even though it makes sense not to make these your main outcomes, showing effects on them would be an important “manipulation check” to understand if the treatment works in the expected way. (Going back to the paper again, I see that it looks like the best you can do on this is Figure A4/Table A8. But even that is helpful, and I’m glad you’ve included it.)
You mention the audits and counter audits of these incentivized outcomes – it’d be nice to see how prominent fraud was (if, of course, you have access to the data – you may not).
As a further “manipulation check” on how the program operated: it would be nice to see how the funds disbursed evolved over time, or perhaps what fraction of centers qualified for funding over time (and also for control centers if you can). This could help tell the story about how crucial the feedback/learning element of the program was – if you have the data.
Would love to see how the program affected performance on metrics that were NOT related to the incentivized ones. You cite Holmstrom and Milgrom but it would be really great to explicitly measure whether multitasking is a problem here.
Finally, I think a basic cost-benefit analysis would be really helpful here.
What’s the total cost of the program? How much more expensive is it to give out conditional cash (with auditing and procurement reforms) than unconditional cash?
How much does it cost to save each life? How does this compare with other interventions in the literature?
Small comments – mostly on framing, most simply a matter of taste.
You note that “a large share” of health services in poor countries are provided by non-profits – how big is that share? I realize it may be hard to get an extremely authoritative answer due to data limitations, but anything you can do to show what a big part of the sector you’re speaking to would be helpful.
This feature is really cool: that the control health centers receive on average the same funding as the treatment ones, but the funding is unconditional. I think it’s worth highlighting more.
You’re really interested in employees’ “motivation,” and how they might get “overwhelmed” by P4P structure. I get it; that’s the focus of the Huillery and Seban paper, and they have to figure out what’s behind a surprising negative result. But I think you don’t need to emphasize this so much, and your data don’t really allow you to speak to motivation anyway; it seems like enough to say that employees need both feedback and incentives to succeed, and neither is sufficient on its own.
The DiD on the “outside group” is really clever; I wish more RCTs did this. I love Table 6. One small caveat: I don’t think you can say that “governance alone improves operating efficiency” (p. 30) because you don’t see any health centers that get governance WITHOUT funding – it may be that funding is a necessary condition for the governance to work.
You’re focused on drawing the contrast between P4P in a social context vs a for-profit context, but I think a lot of people will be interested in this paper who care about P4P in the (social) context of public economics, especially education. My read of the evidence from P4P in education is that it seems to work pretty well in poor countries (e.g Muralidharan 2011) and that a big part of the effect is the selection of teachers (e.g. Leaver et al 2021), (Andrabi and Brown 2021). There’s also work showing, similarly to your paper, that P4P is often not enough on its own (Mbiti et al 2019). I think including a brief discussion of how your work relates to the P4P literature in education, and what makes the health context different, would expand your audience.
I think you do an admirable job motivating why the effects of this “bundled” treatment are interesting, even though ideally we would want to know which pieces of the bundle matter most. You may also be interested in another recent paper that achieved big gains by bundling a lot of things together (also in the education space, from Guinea-Bissau): Fazzio et al 2021.
There’s nothing inherently wrong with having a treatment group so much bigger than the control group, but since it’s so unusual you may want to explain why – were there originally multiple treatment arms? Were there just not very many eligible health centers relative to funding available?","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",65,55,74,,,,60,70,55,70,55,75,55,50,65,45,30,60,55,45,75,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",3.8,3.6,3,4.1,2.8,4,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",11 years,25,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,"Relevance to GP: 70, 80, 90",,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,The Governance Of Non-Profits And Their Social Impact: Evidence From A Randomized Program In Healthcare In DRC,https://unjournal.pubpub.org/pub/eval-sum-governance-nonprofits,Management Science,"50_published evaluations (on PubPub, by Unjournal)",,,2025-07-07T08:32:28.852-04:00,
The Environmental Effects of Economic Production: Evidence from Ecological Observations,https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3940045,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Anonymous,,,"[Evaluation manager/editor’s note: I made some very copy-editing corrections below.]
This paper makes an important step forward in our understanding of the relationship between biodiversity and economic activity. The first key contribution of this paper stems from the use of a novel dataset to provide a broader picture compared to previous literature of the impact of economic activity on biodiversity. The combined spatial, temporal, geographic, and taxonomic scope of the dataset allows for a more macro perspective on the GDP-biodiversity relationship compared to previous papers, and enables the authors to study the degree to which this relationship holds across a variety of contexts.
The paper’s central result that increases in GDP are associated with decreases in biodiversity is supported by a thorough exploration of the nature of this relationship in terms of heterogeneity by taxa and economic sector, distributional analysis, and dynamic effects. Moreover, the authors demonstrate that this association is plausibly causal; using state-level sensitivity to shocks in aggregate military spending as an instrument for state GDP, they present a convincing case for the argument that increases in economic activity cause a decline in biodiversity. Overall, the robustness and thoroughness of the analysis of the GDP-biodiversity relationship is good. The finding that economic activity negatively affects biodiversity is not particularly surprising on its own, but estimating the magnitude of this effect alongside extensive robustness checks as well as an IV approach for causal inference is an important contribution.
Like all studies that use biodiversity surveys, this paper faces the unavoidable drawback that the external validity of its findings is unclear, because surveys are not randomly assigned to locations and species. However, the authors provide an extensive discussion of the data and its potential weaknesses. As well as quantitative checks for any red flags that might suggest issues with the data, they also make the important point that at the very least their results are internally valid (with respect to the biodiversity surveys included in the dataset). Overall I think their discussion of the data and its inherent limitations is thorough and well-communicated, and the potentially limited external validity of the results does not undermine the importance of the contribution in terms of global priorities.
The authors build on an already important contribution by estimating the role of pollution in the GDP-biodiversity relationship as well as the mitigation of this relationship by environmental regulation. These analyses improve the overall impactfulness of the paper by providing practical insights into how to mitigate the impact of economic activity on GDP. Using a well-established IV strategy from the pollution literature, they estimate the role that pollution plays in the GDP-biodiversity relationship. Their findings shed light on the potentially important role that pollution plays in the impact of human activity on ecosystems. Finally, the authors offer an assessment of the role of environmental regulation in reducing the impact of GDP on biodiversity. Their findings suggest that the Clean Air Act may have helped to reduce the impact of economic activity on biodiversity.
As it is, I think this paper is already publishable in a good field journal, and so I do not have any comments that are necessary to address to ensure publication. However, I provide some comments and suggestions below which the authors may find helpful for further improvements to the paper. The main area of focus of these comments relates to the section on environmental regulation, which I think is the least robust section of the paper. Given the applicability of this section to real-world practice, improving its robustness would contribute to the overall impactfulness of the paper.
Specific comments and suggestions:
[Evaluation manager/editor note: I replaced the bullet points with an enumerated list.]
A suggestion to improve the abstract: The headline finding that economic activity decreases biodiversity is an important contribution but not particularly surprising or impactful on its own. Meanwhile, as illustrated by Figure 3, the paper provides some interesting insights into the nature of the GDP-biodiversity relationship, in particular that non-bird and especially mammal species are relatively more vulnerable, as are locations where biodiversity is already particularly low. Given that these insights are underpinned by the breadth of the novel data used and are therefore an important element of the paper’s novel contribution, I would suggest mentioning them in the abstract.
GDP is likely non-stationary; however, if the biodiversity outcome variables are stationary, then using a non-stationary variable (GDP) to explain these outcome variables may introduce a lot of noise into the specification. In so far as state-level GDP follows a similar trend as the national average trend in GDP, the year-fixed effects mitigate this issue. Nevertheless, state-level GDP trends are likely heterogeneous. The authors cannot include state-by-year fixed effects because these will absorb the effect of GDP, but perhaps they could reduce noise in the estimation by de-meaning the GDP variable relative to the state-specific long-term average.
I appreciate that comparing and interpreting the magnitude of the estimates in this paper is difficult, but nevertheless, some further discussion could help guide the reader to understand how large these estimated impacts are and improve the real-world applicability of the paper. For example, what do we know from ecology about the magnitude of natural year-to-year variation in population and abundance? How does the population decline associated with a 1% increase in GDP compare to population declines that followed a specific natural disaster or other extreme event (for example)?
Regarding the evaluation of the effect of the Clean Air Act on the GDP-biodiversity relationship, I would suggest providing some more detail to justify the empirical strategy and/or testing some alternative strategies as a robustness check. For example, why use the count of NA designations rather than a binary variable to indicate any positive number of NA designations? Also, why use an IV strategy rather than an alternative approach such as difference-in-differences? It looks like you could have sufficient data for a DiD-type design around 2005, when the NAAQS for PM2.5 came into effect. Sager and Singer (2022 Working Paper) might be helpful here - they provide an interesting discussion of the potential limitations of a simple DiD framework in the context of the Clean Air Act. Finally, it would be beneficial to have a more extensive discussion of the extent to which we can interpret these estimates as causal; how confident can we be that attainment only affects biodiversity through its effect on GDP?
It may be helpful to readers to see the equation for section 5.1 written out explicitly, rather than referring the reader to the previous section, to help make clear the empirical strategy and for ease of comparison with previous literature on the Clean Air Act.
Similar to the approach illustrated in Figure 3, which explores the heterogeneity in the GDP-biodiversity association, it would be interesting to explore the heterogeneity in the pollution regulation-biodiversity association. Particularly given that the availability of public funds to improve biodiversity are limited, this analysis could yield important practical insights into how to most effectively target these funds. For example, are some taxa more protected by regulation than others? Are the taxa that are most protected by regulation also those that are most vulnerable to economic activity? Does the impact of regulation on the GDP-biodiversity relationship vary over the distribution of biodiversity?","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",70,,,,,,70,,,70,,,75,,,65,,,60,,,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4,4,,,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",[blurred for anonymity: 5-10 years],None.,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,Relevance to GP: 80,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,The Environmental Effects of Economic Production: Evidence from Ecological Observations,https://unjournal.pubpub.org/pub/liangevalsum,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-07-07T08:29:23.901-04:00,
The Environmental Effects of Economic Production: Evidence from Ecological Observations,https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3940045,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

", Elias Cisneros,,,"Thank you for the opportunity to read this paper. I truly enjoyed reading it.
This work investigates the relationship between economic growth and biodiversity in the United States. It leverages an extensive database that collects an array of time-consistent species surveys at different locations to build yearly biodiversity indices for eight taxa across 50 years. Using fixed-effects regressions, the authors relate the biodiversity outcomes with state-level GDP measures in an unbalanced data set. They find a robust negative correlation between economic growth and biodiversity outcomes, with an elasticity of 1.5 to 3.5%. Although varying in size, estimates are consistent across taxa, production industries, biodiversity indices used, or alternative observational units (e.g., counties, eco-regions). The paper further delves into a causal identification of the impacts of economic growth on biodiversity using exogenous variations in military spending in a shift-share IV approach. The second part of the paper further explores two potential mechanisms, pollution and land-use change, highlighting their role in the decline of biodiversity across the United States.
The paper makes a highly valuable contribution to understanding the trade-offs between economic growth and its impacts on biodiversity. Furthermore, the authors spearhead a new spatially granular database on biodiversity measures that has received little attention from environmental and ecological economists. I am convinced that readers will be eager to follow and use the BioTIME database to understand the economic and political impacts on biodiversity.
Although I am fully convinced about the timeliness and importance of the paper, I have some comments related to framing and the empirical approach of the mechanism analysis. Nonetheless, many of the raised points can be easily addressed. If the authors pursue these improvements and others they receive elsewhere, I believe they will make a highly valuable contribution to the environmental economics literature.
Major Comments
1. The paper combines a multitude of empirical approaches and units of analysis. Although each approach has its advantage, switching back and forth between different approaches decreases the manuscript's readability. Although it is undoubtedly a matter of writing style and a question of the target journal, I believe that reordering some sections could ease the reading experience and streamline some of the arguments.
a. I suggest to order the arguments that are also linked to empirical approaches as follows: A) Correlation between biodiversity and GDP using OLS, B) Establish causality with IV; C) Robustness on measurements biases using the IV approach (instead of OLS); D) analyzing heterogeneities (e.g., by taxa, EKC, distributional, etc.) with the IV approach (instead of OLS); D) Pollution channel analysis with Wind-direction-IV E) Land-use channel analysis using OLS correlation: Though I would put this to the appendix or skip it entirely (see comments below). F) Environmental regulations channel analysis using TWFE. Though I would leave out the fuzzy regulation-GDP-biodiversity link G) Protected area channel analysis. However, I would replace it with a panel on protected area creation and use a TWFE estimator (instead of a heterogeneity analysis with interaction terms.
b. Before delving into potential heterogeneous results in section 3.2, I would address the potential biases. Section 2.3 (Title should be: “Potential sources of measurement bias”) deals with the potential measurement biases, while only section 4.1 addresses the omitted variable bias and reverse causality bias. I think it is best to address all biases next to each other. I would first describe the empirical strategy and then, directly after, the potential a) measurement biases and b) the potential omitted variable and reverse causality biases together.
c. After addressing potential biases I would create two separate chapters a) on heterogeneities (now section 3.2) and on potential channels (now sections 4.2 and 4.3).
2. After the main results, you choose to contrast two samples: All taxa vs non-bird taxa. The comparison group is missing. One should present results for either both groups, i.e., Bird taxa + non-bird taxa, or the three samples, i.e., all, bird, and non-bird. Results are mostly better for non-bird taxa, while results for the full sample are often insignificant. This bears the question if results for birds only are always insignificant or even point in a different direction.
3. Results on sectoral GDP (Table 2) are highly biased and, therefore not very informative. Regressing biodiversity on GDP is already subject to selection bias (though I think the paper is making a good point and presents a very valuable first analysis). Differentiating between sectoral GDP growth and jointly using all indicators in one regression must increase the problems of attribution, multicollinearity, reverse causality, and omitted variable bias. You do recognize these issues when discussing the positive estimate for agricultural GDP… I suggest excluding this argument (analysis) from the paper or trying a different empirical approach. Maybe it is possible to combine the IV strategy in combination with pre-period sectoral shares?
4. In the pollution channel analysis (section 4.2), I would like to see a more detailed reasoning for the selection of contributing counties. First, why are counties in a 300 km radius excluded? It would seem that close pollution sources are the most important. Second, the LASSO regression selects counties (see Figures 5a and 5b) that are thousands of miles away. Could it be that those relations are a statistical artifact and do not represent a true physical impact? I know that pollution from massive forest fires can travel large distances (e.g., Indonesia, Canada), but maybe you can back up your argument with natural science literature. On the other hand, far-away pollution sources might not pose a problem in your analysis as the IV is weighted by the inverse distance. Nonetheless, Figure 5 and your description is misleading at first sight.
5. The analysis of land-use policies (section 5.2) could be improved by using a panel of protected areas. I am unfamiliar with the expansion of protected areas in the United States. Still, if there is a significant expansion of protected areas in the vicinity of the sampling locations, that could be exploited in a quasi-experimental setting. You could use the panel of new protected areas in a similar empirical framework as the analysis on pollution attainment areas using TWFE.
Minor comments
1. I understand that many interdisciplinary journals prefer a graphical depiction of the main results, though if you choose an economic journal, I suggest showing results in Table format. I find it easier to read and understand the empirical strategy and the number of fixed effects and observations in a Table format.
a. The main results table could present the results of Figure 3a potentially using 3, 6 or 9 columns differentiating between all, bird, and non-bird taxa. It is also the set-up you choose to carry on in the text. Therefore, it might be more transparent to make the distinction right away. Figure 3b could pose as additional information for the appendix rather than a pre-step to show before summing everything up into bird vs. non-bird species.
b. OLS and IV estimates are easier to compare in Table format - Figure 4b does not easily convey how good the IV strategy is (first stage) and how small or large the difference between the IV and OLS estimates is. Table 3 is much more transparent but also mixes reduced form with IV estimates. In general, I don’t think it is not necessary to present a complete graph with a single line-plot just to show one point estimate.
2. I think the analysis of pollution regulations (section 5.1) can be cut to the TWFE estimation only. The exercises with GDP and the repeated comparison of overall vs. mechanism effects seems to massage the data a bit too much.
3. It might help to contrast the urbanization (section 4.3) and the construction-sector GDP estimates (section 4.3) next to each other.
4. Table 2 does not describe which FE are used.
5. You are missing a dotted zero line in Figure 3d
6. Figure A6 left has a wrong negative sign on the coefficient.
7. On page 15, you write, “ηt denotes year fixed effects to capture common shocks such as national recessions”. Year fixed effecs also capture common changes in federal environmental policies, regulations, laws, financing of protected areas, overall enforcement budget, etc.
8. Please provide a more straightforward argument why a state-level analysis is the preferred spatial unit. Later on, you sometimes shift the unit of analysis, which adds unnecessary to the complexity of the manuscript. I would recommend to choose one unit of analysis for the main text, as there are already many variations in the empirical strategy.
9. How is Table A.2 a panel regression if its outcome is constant over time (columns 2-6)?
10. Sometimes the paper mixes the data description with the estimation strategy: E.g., on page 9, “As previously noted, in all regressions we include … “. I would try to streamline the text for easier reading.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",88,,,,,,75,,,90,,,80,,,90,,,95,,,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4,4,,,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",I started my Ph.D. in 2012. Thereby I am already 11 years in the field.,I have reviewed 13 papers for 8 journals.,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,Relevance to GP: 95,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,The Environmental Effects of Economic Production: Evidence from Ecological Observations,https://unjournal.pubpub.org/pub/liangevalsum,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-07-07T08:23:51.264-04:00,
Does the Squeaky Wheel Get More Grease? The Direct and Indirect Effects of Citizen Participation on Environmental Governance in China,https://www.nber.org/papers/w30539,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Evaluator 2,,,"Overview
This paper presents the results of a randomised control trial examining the impact of citizens reporting violations of pollution standard (water and air) by firms in China. Specifically, if a firm is allocated to a treatment arm and goes on to violate emissions standards, the violation is reported to the regulator by a citizen acting on behalf of the experimenters. They broadly find that these reports lead to the regulator intervening more often, firms polluting less, and these effects being particularly strong when the citizen report is made public through social media (T2 vs private reports, T1). Moreover, they are able to begin to elucidate the mechanisms by which these effects are bought about (various arms within T1 which vary who the report is sent to), and show that (at least within prefecture) this is not a zero-sum game (ie there is no evidence for substantial local leakage). Clearly, issues regarding air and water pollution are incredibly important, and this paper may offer a way for citizens to reduce the damage these cause by increasing compliance. In the sense that this paper answers a large and important question, with a well-thought through and implemented large-scale RCT of a low-cost intervention it is a potentially very important contribution.
While I think that this paper will publish very well even in its current draft, there are points where I think clarification - and less bold claims - are advisable in the writing and interpretation of the results. Similarly, the econometric approach is broadly well implemented, but I think recognition of the pitfalls (primarily regarding the SUTVA assumption) might be necessary. As noted in the additional comments regarding open science, it is a shame that the pre-registration included only details regarding treatments and nothing on how the data were to be analysed, but I recognise this obviously cannot be changed at this point. I organise my comments by theme (econometrics, generalisability, interpretation of results, ease of understanding) and within these themes order from more to less significant suggested changes.
[Evaluation manager: I copy the treatment abbreviations and descriptions from the original paper for clarification below, as the evaluator refers to these in their response] 
Control Group (C): “When the CEMS data indicated that the firm violated its emission standards, we did not intervene in any way. About 1/7 of the CEMS firms were assigned to this group.”
Private Appeals Group (T1):  “...  a citizen volunteer filed a private appeal against that violation that was not observable by the public. About 5/7 of the CEMS firms …”
Public Appeals Group (T2): “ … wrote a post on Weibo  … and “@” the official Weibo account of the corresponding local EPA. … We assigned 1/7 ….”
Private Appeals to Regulator via Direct Message on Social Media Group (T1A): “...  sent a private message to the corresponding local EPA’s official Weibo account, notifying them about the pollution violation and requesting that they investigate ….”
Private Appeals to Regulator on Government Website Group (T1B): “... filed a private appeal via the 12369 website to the corresponding local EPA … 
Private Appeals to Regulator through Government Hotline Group (T1C): “... called the 12369 hotline to privately appeal to the corresponding local EPA. In the phone call, she notified the local EPA …” 
Private Appeals to Firm through Phone Call (T1D):  “… called the violating firm to privately appeal the violation. In the phone call, she notified the firm about its violation and requested that they check the issue”
Econometric approach
All prefectures contain some treated firms, but the intensity of treatment (70% or 95% of firms assigned to treatment) varies so that they can assess the “general equilibrium” effect of the treatment on non-treated firms in prefectures with a higher (95%) or lower (75%) intensity of treatment. This is clearly a very neat experimental design. However, the motivation for high/low intensity (“indirect effect”) clearly means that the SUTVA assumption across firms does not hold, such that the difference in outcomes between treated firms and control firms (those in the remaining 30/5%) captures the sum of the direct effect on treated firms and the indirect effects on control firms. I therefore find the comparisons within treatment arms (public vs private, the way private treatments are implemented), and comparisons of high vs low intensity prefectures more compelling. To better understand the impact of this SUTVA violation, it would be nice to see a plot of how control firm violations vary through time - ie does the onset of treatment lead to changes in control firms’ violations?
Relatedly, the claim that general equilibrium effects are estimated needs caveating as just local general equilibrium effects are mediated (at least as discussed in your paper) by the regulator’s capacity constraint. Of course, even including solely these local general equilibrium effects is still rather novel. But, a key problem in environmental pollution and policy (eg carbon markets) is the impact of (global) leakage (policy reduces output but relatively inelastic demand simply means that this shifts elsewhere). I don’t think that your estimates capture across prefecture leakage, and certainly wouldn’t include leakage beyond China’s border. Data on firm-level output might help mitigate this concern (ie if it shows supply actually is not constrained), and perhaps the evidence regarding the firms being fully operational is sufficient (but would need more discussion).
Much more minor comments that are easily addressed: 1) I think estimates are in effect intent to treat - the ongoing violation reporting outside of the experiment means control firms experience the “treatment” just less intensely - and this could be explicitly recognised. I think you could therefore consider using the treatment as an instrument for intensity of treatment (receiving a report conditional on a violation) in a 2SLS approach. 2) I think clustering of the standard errors in the firm-level analyses should account for the possibility errors are correlated within the citizen doing the reporting across firms/prefectures (ie some citizens may [randomly] have more or less impact across all the reports they send) 3) I think standard errors in table 6  need to be clustered at the firm level to account for multiple observations through time from different violations by the same firm.
Generalisability/impact
On P1, you give evidence that lots of different countries have made it possible for citizens to report violations, yet present little evidence that citizens then actually do use this tool. You present evidence that in China citizens do this later (P2) but understanding if your results are relevant globally would be useful - ie do citizens outside of China regularly engage in such violation reporting? I note that even if the paper is only relevant to China, the potential impact/importance of the paper is still very large. Similarly, given your evidence and the relatively low cost of implementation, it brings into question why so much cash is being left on the table, and I think this could be discussed in the concluding remarks. Perhaps part of the reason for limited wider take-up is simply how rare the hourly and near real-time data China releases is. Alternatively, is it because reporting violations is privately costly but publicly beneficial, similar to punishment in PGG?
Interpreting results
It seems to me that it is not possible to ascertain what the effect of citizen participation is through this experiment. Rather, the effects of the treatments compared with control conditions could be through some salience-of-information channel, and there is good evidence that information does matter in other environmental contexts (eg Saavedra 2023). Indeed, in the pre-registration document it reads as if there will be an information treatment separate from citizen reports (“Firms will be assigned to complaint, information, or control conditions”). This matters only for understanding what drives your results, and therefore what range of interventions might have led to similar outcomes. The impact of the Weibo likes could be 1) public pressure and therefore unrest or 2) information contained within the idea lots of people think this matters. On p27/28 you present evidence which suggests social media isn’t useful because of it involving more people per se conditional upon the severity of the violation. It would be nice to see in this observational data whether more non-treatment reports tend to be made when the violation is worse (I assume they are), in which case this would support the possibility that greater public engagement (likes, number of appeals) normally contains some information (even though in your treatment it obviously does not). Similarly, you claim that your results show “social media is an especially effective way to deliver public appeals” yet you have no comparison for delivering public appeals with anything other than social media.
Building on the idea of identifying the channels through which effects occur, my view is that you generally do a really good job of isolating channels, but this work could be explained better. As well as being better explained, I would have much appreciated you flagging that you will deal with the potential concerns when you mention the main effects. First, I was concerned about the potential for data manipulation, yet the paragraph on p26/27 details a range of robust evidence that that is not what is happening. You might also want to look at whether the data from treated and non-treated firms follows similar leading-digit distributions, applying Benford’s Law as per Cole et al. 2019 Climatic Change.

Second, the public vs private effect could operate through several channels: 
(1) firm knowledge;
(2) firms feeling consumer pressure; 
(3) public pressure on regulators; 
(4) speed through the system to the local EPA rather than through the central report body; 
(5) novelty of social media reports; 
(6) the role of central government. 

(1) can be explored through comparison to the effect observed in T1D and T1C*T1D and would appear to not be driving the result (but see comment in the understanding section RE common knowledge); (2) is dealt with through T1C and the evidence regarding whether the business is a final product producer; (4) seems unlikely given the comparator of results under T1A; (6) is dealt with by the subset of T1A which receive a threat of central government follow-up. Which leaves (3) and (5). The former is what you argue to be driving the effect - supported by the “likes” treatment, while I discuss the data needed to show if there is extra “novelty” of social media in the next paragraph.
At the moment, it is unclear how much the treatment changes the probability that a violation is reported by a citizen. In text, the paper mentions ~300k reports during the treatment period but later (P18) suggests just ~5.5k of those are actually applicable (identifying a specific violation etc). Understanding how these relevant non-treatment induced reports are split would be good: what number are private vs public? Individual citizen or NGO made? The same individuals making lots of complaints or many individuals infrequently? Finally, how are these 5478 valid appeals distributed across the 5366 real violations that occur during the treatment period - perhaps a histogram of the number of violations by the number of non-treatment valid appeals that they get would be useful.
You claim that the fact that effects persist (and if anything appears stronger) in the medium-term (ie at the end of the 8-month treatment period) suggests that if the treatments were implemented in the long term the effect would remain. This seems a little challenging. One could imagine it getting stronger if there are long-run adaptations they make, or imagine it weaken if the recipients simply get used to experiencing a high volume of appeals.
It would perhaps be good to discuss how your results fit in the wider literature regarding the impact of mandatory disclosure laws to citizens on firm behaviour (eg Bennear and Olmstead 2008 JEEM). Perhaps they would also benefit from additional consideration of how a party might reasonably increase the number of appeals - eg could making the algorithm that you developed to identify the cases of violations be useful? (which links into my previous comment RE discussing why so much cash might be being left on the table).
Ease of understanding
These comments are much more minor, in order in which they appear in the paper, and I think in general personal taste issues:
When you first use the term “appeals” early in the paper (abstract, early intro) it is unclear what this means - I would define as “appeals in the forms of reports made to the regulator if a violation of an emission standard occurs”.
MEE and MEP are confused in the first parts of the paper before Footnote 13 comes in and clarifies MEE replaced MEP
Unclear early on why there are multiple regulators - at that stage we don’t know there is a central monitor and many local regulators
I would like to see a diagram of the different actors in this space and their roles (MEE/MEP recording and publishing data, operating the hotline; local EPAs beneath this policing firms; firms; citizens). Perhaps this diagram could also include the incentives they face (official and unofficial) - eg are the local EPA legally required to investigate every violation? And defined over what timescale (hourly or daily violations etc)
How you define daily violations given the data and standards seem to be at the hourly rate is not quite clear (I assume that it is if they had any violation in a 24 hour period)
Figure 1 - would be good to have an additional plot which is a histogram plot of counts of the number of stacks/firms by number of violations that they commit. Fig 1 horizontal axis labels could also be better formatted as “1st Jan 2018” etc rather than year end and YY/MM/DD
Figure 4 - I think this would be better done as a Sankey plot. Using the same bins for where firms start, then track where the individual firms are in the distribution at the end. (At present, it is unclear if all firms are reducing their emissions a bit, or if the violation firms are reducing their emissions loads and the other firms changing very little).
Common knowledge is mentioned, but it is not clear if you’re using this in the precise definition (the firm knows, the regulator knows, and each know that the other party knows) or in a looser way (they each know, but are not specifically informed that the other party knows). If it is the former, please explain that each party was additionally informed that the other party knew (in T1C*T1D) or if the latter please use a different term.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",80,71,86,,,,72,81,63,76,68,83,67,62,78,48,42,68,91,78,94,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4.3,4.6,3.7,4.8,3.5,5,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",,,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,"This is more of a personal taste thing - I did not like the way that when reading the paper (particularly Introduction) potential pitfalls jumped at me, and rather than simple say “and this is robust to X concern” or “and as we show, we’re able to exclude X driving these results” I’m left with all of these doubts until I get to quite a bit later in the paper (latter half of results) where you quietly go dispelling most of my concerns… but I’d prefer to not have to store my concerns as I read, and instead be re-assured by you early on that you’ve done this or that check because by and large you have","While the study says that it is pre-registered, that pre registration does not include any details as to how the study will be analysed, nor anything on the hypotheses that will be tested. Only the experimental design is recorded. It was not apparent where to find the code or data to replicate the study [perhaps not unexpected given it is a WP].","Relevance to GP: 82, 92, 95","I think this paper asks a really important question, and then performs a country wide RCT to analyse the answer. While I think my concerns should be addressed - and can be relatively easily with some toning down - this will then likely make the paper less appealing to the very top journals.","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Does the Squeaky Wheel Get More Grease? The Direct and Indirect Effects of Citizen Participation on Environmental Governance in China,https://unjournal.pubpub.org/pub/jq95bapl,American Economic Review,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-30T08:53:03.881-04:00,
Does the Squeaky Wheel Get More Grease? The Direct and Indirect Effects of Citizen Participation on Environmental Governance in China,https://www.nber.org/papers/w30539,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Robert Kubinec,,"This is an evaluation of “Does the Squeaky Wheel Get More Grease? The Direct and Indirect Effects of Citizen Participation on Environmental Governance in China"", involving an RCT investigating the effects of citizen participation through social media on environmental policy enforcement. The evaluation praises the RCT as ambitious and well-execute. It highlights the sophistication of the experimental design and the importance of the findings for both academic and policy circles. However, it also critiques the handling of over-time dynamics and the panel data specifications used in the analysis, suggesting alternative methodologies for clearer interpretation. The evaluation considers the external validity of the findings, discussing the potential impact and limitations of applying the Weibo intervention in different contexts. It underscores the importance of policymakers' utility calculus and the institutional framework in determining the intervention's effectiveness. It concludes by acknowledging the study's contribution to understanding the role of social media in influencing government behavior towards environmental concerns, while also cautioning about generalizing the findings. (This abstract was not written by Robert Kubinec).","Introduction
This review of Buntaine et al. 2022 is at the request of the organization [The] Unjournal and as such emphasizes certain issues that might not be as of great import in a standard review. In particular, I will examine the policy relevance and impact of this research, which means that I will dwell on issues of external validity to a greater extent than I might otherwise.
On the whole, I found this RCT to be both ambitious and remarkably well-executed; as such the academic and policy communities both stand to learn significant new information about how social media, pollution monitoring, companies, and local bureaucracies interact. The research design is also clear and clean; we know what was randomized and the outcome measures are appropriate and relatively error-free. Even apart from the substantive findings, I believe this paper should be a part of curricula that teach RCTs as it raises the bar for the sophistication of experiments.
At the same time, increasing complexity can also lead to more complex analyses. My main fault with the paper is that the over-time dynamics of the experiment are not appropriately handled, which can make it difficult to interpret the effects of the treatment. There is substantial work on over-time variation in cross-sectional data that the authors could draw on to give us a better understanding of the dynamics of the companies under study.
In terms of broader relevance, I believe that the study’s Weibo intervention points to the role that boosted posts could affect government behavior. In some sense, this paper is the inverse of the growing literature on state-backed misinformation on social media: if nefarious actors can influence the social media discourse and consequently what social media users believe (Tucker et al. 2017), it is quite possible that altruistic organizations could influence what governments believe based on social media, leading to consequent changes in policy.
While this is a remarkable finding and one worth considering as a new tool for policy engagement, an important caveat is that the findings of this RCT depend on the policymaker’s utility calculus. In the context in China, policymakers had precommitted to learn from citizen complaints and they also believed that the legitimacy of the state depended on responding to environmental concerns. When policymakers do not share these goals, the policy intervention could either have no effect or possibly backfire on those who make the appeals. I discuss these issues in greater length after examining the study.
Research Design
As I mentioned above, I find the experiment to be remarkably well-designed. It uses a sophisticated multiple-arm approach with overlapping treatments, permitting them to maximize the different treatment nuances and consequently increase learning in the experiment. They also spent a significant amount of time to increase the validity of the treatment by analyzing pollution data from the CEMS system. This effort and attention to detail helps us know that the complaints being filed are based on actual violations rather than just raising awareness as such.
While the experiment was well-designed, the pre-registration was quite limited. We have a rough though generally accurate description of the experimental procedures and we are told that they intend to do the survey with more than 25,000 firms across 7 treatment arms. Their enrolled sample just missed this target at 24,620 firms. In general, 24,000 is still such a large number that I am not particularly concerned about power or sub-group analyses. However, that is not to say that all analyses will be well-powered, as Tom Cuningham noted about the recent Facebook mega-experiment. Compared to most experiments, though, this one will probably estimate most estimands with minimal noise and sampling error.
The limitations in the pre-registration, though, do affect how much we can learn from the experiment. We do not know what if any priors the authors’ had about effect sizes, and we lack a good understanding of the power of the design. It is a complicated design, so the power curve would require simulation, which may be non-trivial. There are R packages that offer help with this task. This lack of confirmed priors mean we have to be careful when we generalize beyond this study as we run the risk of confirmation bias; i.e., seeing these results as firmly established rather than at least partly exploratory. We can over-estimate the certainty of finished/published results while under-estimating the uncertainty of applying these interventions outside of their original context.
Estimation
My main criticism of the article’s methodology is the panel data specifications that were employed. It appeared that the authors followed a long-standing tradition in econometrics to include substantial numbers of fixed effects as a way of “conservatively” estimating treatment effects. However, the authors’ specification, so far as I can tell, boils down to a two-way fixed effects specification that has been repeatedly criticized by authors in recent years (Imai et al. 2021, Goodman-Bacon 2018, de Chaisemartin & D’Haultfœuille 2022, Kropko & Kubinec 2020). As an author of one of these studies, I may be biased against this particular specification, but I think at minimum the authors should consider implementing simpler and easier to interpret estimates.
The issue of course is that, unlike canonical experiments in which the treatment is assigned cross-sectionally and a single observation of the outcome is obtained, the authors have companies observed daily for a significant period of time. This means we have, theoretically, one counterfactual outcome for each firm and each day. Which comparison on which day is most relevant? This “repeated measures” issue is, in my opinion, an artifact of the potential outcomes framework which encourages people to think of discrete counterfactuals (Y(1) and Y(0)).
In this situation, expressing the research design as a causal graph is quite helpful. At its simplest level, the causal graph of the experiment can be expressed as the following:
T \rightarrow M \rightarrow Y
{#eq-dag1}
In this case, the treatment is randomly assigned and causes a mediator M, which subsequently causes the outcome Y. M in this experiment could be many different things, including the actions of the regulators to the appeals and the reaction of the firms to the investigations. The treatment does not directly affect either the behavior of firms or air quality directly but rather through specific causal pathways.
The authors do an admirable job in the experiment trying to ascertain what these pathways are in part by randomizing different kinds of treatments that should cause different values in M, such as by informing governments publicly vs. privately. However, my point here is that it is entirely possible to estimate the effect of T on Y without including any measures of M. Because M is caused by T, it is post-treatment and so the effect of M is subsumed in any analysis of the conditional probability Pr(Y|T).
In the context of this experiment, we are fairly certain that these mediated effects are the most relevant; it would not seem plausible that the treatment would have much of a direct effect on Y apart from the mechanisms:
T \rightarrow Y
In other words, we would not think that monitoring CEMS data would have any effect without being able to get policymakers to pay attention. As such, we do not need to be as concerned about separating the direct from the indirect effects.
For these reasons, I think the authors should perform a simple pooled analysis as a baseline specification–i.e., compare post-treatment observations for treatment to control without any fixed effects, or if including fixed effects, include firm fixed effects only (to isolate the within-firm effect) or fixed effects for the districts within which randomization was conducted (for the between-firm effect). These naive models will combine the direct and indirect effects of T on Y but are also quite simple to interpret, and therefore provide a helpful baseline before considering indirect pathways.
To understand the time dynamics better (i.e. what is involved in T \rightarrow M, I think the authors should consider models of time. On the semi-parametric side, we can look at splines and kernel density estimators (Kenny et al. 2022), and also the so-called quasi-experimental methods like multi-period difference-in-difference (Callaway & Sant’Anna 2020). The exogeneity of the randomization would make assumptions such as parallel trends more credible (Rambachan & Roth 2019), and would allow the authors to better decompose the post-treatment variation in the outcome.
Including the fixed effects as such, that is, without a clear causal model, risks inducing post-treatment bias (Montgomery et al. 2018). Each of the day \times firm fixed effects could represent some part of M that occurred post-treatment, and as such should be considered post-treatment variables. The authors’ RCT power may be affected by this in hard-to-predict ways. In general, this post-treatment bias may unnecessarily attenuate the treatment effects, and there are plenty of results in the paper that are surprisingly noisy given the massive size of the sample. Without other and simpler specifications being reported, though, it is difficult for me to say whether this might be the case.
I would also encourage the authors to consider the growing literature on sequential ignorability, especially as expressed using causal diagrams (Xu 2023). There may be many possible estimators or comparisons, but being clear about which particular comparisons are employed would help make it clear exactly what the average treatment effect is.
External Validity
These concerns aside, I do think that the experimental analysis as written produces some clear findings, especially that social media posts seem to generate more of a reaction from government officials and consequently firms in terms of their behavior. It is important to note as well that the experiment “failed” to affect some outcomes, such as firm-level outcomes in terms of pollution when the appeals were made privately (see row 1 in Table 3). Furthermore, while the treatment lowers violations, its impact on aggregate pollution is less clear (table 7), possibly because firms can meet air quality standards but still pollute substantially. As such, we learn not only that Weibo is a uniquely important mechanism for affecting government preferences in China, but also that other channels by which the government could learn apparently important information about polluters can have a very limited effect at least in terms of pollution outcomes.
The Weibo treatment has clear policy impact. In fact, if it can be implemented in China, which is notorious for controlling the information environment, it should be even easier to implement in the many countries that have much less control over social media. The impact of the policy is also high relative to the cost: boosting social media posts does not require enormous investments in capital. Highlighting complaints to policymakers that are observable to other citizens would appear to be a rewarding strategy that advocacy NGOs could use on behalf of important issues like climate change, immigration and anti-corruption.
At the same time, there are important limitations to the external validity which are not fully acknowledged in the present draft. Crucial to this strategy succeeding is the utility function of the policymaker. As the authors note, China is exceptional in its commitment to citizen concerns concerning pollution, especially for an authoritarian state which presumably does not face the same accountability pressures of regular popular elections (Slater 2012). The Chinese Communist Party, at least under the leadership of Hu Jintao, believed that the legitimacy of the state rested on mitigating pollution harms, especially those which resonated with the broader population. They imposed these goals on local bureaucrats by designing institutions like the local EPAs that have to fine and regulate local firms in order to survive and gain promotion.
Importantly, the field experiment does not have any control over this utility function. If we were to implement the Weibo treatment in an institutional framework that did not prioritize pollution harms, we would not necessarily observe such large treatment effects. Furthermore, in authoritarian regimes that do not share China’s commitment to responding to citizen concerns, social media activism could well invite repression and censorship rather than policy change (Pan & Siegal 2020). I agree with the authors’ assessment that their treatment was ethical in China, but that analysis depended on China’s particular legislation that privileged and protected this kind of citizen speech. Any application of the treatment to another state would have to do the same kind of in-depth analysis to understand whether it would be effective and whether it would expose ordinary people to harm or retaliation.
Second, the kind of general equilibrium analysis that the experiment engages in is limited to displacement effects among firms. While the experiment was appropriately designed to test this effect, I did not find it to be the kind of general equilibrium analysis we should be most concerned about with this treatment. A priori, I did not think it very likely that local firms would increase pollution after observing punishments aimed at other firms. It is possible, but depends on other firms making a series of judgments about local bureaucrats being overwhelmed by enforcement actions that I thought are unlikely to be as great as the Bayesian updating that they could be the next target.
Instead, the equilibrium analysis we need is how companies would respond after realizing that an external actor is selectively boosting social media posts. It would seem that local companies and bureaucrats would not have known that there was a campaign to send more enforcement actions; were this knowledge to become known, firms might lobby bureaucrats to ignore these requests because their external nature makes them illegitimate (Buchanan 1980). If bureaucrats are aimed at appeasing public opinion, then they might become less responsive if they see these appeals as being artificially generated by an interest group. In summary, in general equilibrium the policy may not remain effective over the long term due to counter-lobbying of companies and their business associations.
Despite these concerns, I remain bullish on the applicability of this intervention to broader contexts. The effectiveness of the treatment will likely diminish once companies respond to the change in the policy environment, but this kind of mitigation is almost a truism given what we expect rational actors to do when facing serious costs to their business. The treatment’s cost-effectiveness strongly implies that it is would be a valuable tool for policy change and enforcement. I would end though with the caution that the implementer must have a good sense of the risks of the strategy for those who post appeals on social media and whether they might face repression or censorship as a result.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",90,85,95,,,,80,90,70,90,85,95,80,75,85,70,60,80,90,85,95,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4.8,4.8,4.5,5,4,5,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",,,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,See comments [in evaluation],,,,,"Work has both academic and policy impact and represents state-of-the-art experimental techniques.
Publication decisions are very hard to predict.
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Does the Squeaky Wheel Get More Grease? The Direct and Indirect Effects of Citizen Participation on Environmental Governance in China,https://unjournal.pubpub.org/pub/jq95bapl,American Economic Review,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-30T08:45:32.875-04:00,
Do Celebrity Endorsements Matter? A Twitter Experiment Promoting Vaccination In Indonesia,https://www.nber.org/papers/w25589,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Anirudh Tagat,,"Note from David Reinstein, Evaluation Manager: This evaluator considered the May 2023 (MIT) working paper version titled “Do Celebrity Endorsements Matter: A Twitter Experiment Promoting Vaccination In Indonesia”
Brief explanation:
This paper is important for understanding how celebrities and in general, influencers could play a role in health communication in a developing country context (Indonesia). It provides a novel and carefully designed experiment where celebrities post messages on Twitter that are intended to boost immunization rates and awareness in the country.
It is grounded in a rigorous theory drawing from network science as well as the economics of networks, and makes a clear contribution to newer applications of network theory in the realm of applied economics work.
The empirical framework aims to cover various aspects of how the intervention that the authors implemented might operate, focusing in detail on potential mechanisms (as explained by their theoretical framework), as well as on two main questions: (a) what characteristics of a message have the most reach; and (b) whether seeing these messages has some downstream impact on knowledge and beliefs around immunization.
The paper is well written and clearly a very rigorously conducted experiment that has tremendous value to those working in health policy and health communication. In terms of causal inference, however, there are a few concerns that the authors could take into account to strengthen the confidence in their results, especially those surrounding the impact of these messages on knowledge and beliefs (“offline sample”).
Summary:
This paper looks at the impact of celebrity endorsements on Twitter related to immunization in Indonesia. The authors recruited 46 celebrities and 1032 ordinary citizens to participate in their study, and randomly assigned each of them to tweet a message around immunization practice during the July 2015 to February 2016. This allows authors to exploit randomized variation in the messages to study important and relatively unexplored aspects of how messages are passed on within large networks (especially networks formed on social media platforms such as Twitter). They vary the timing of the tweet, who it is originally composed by (an ordinary citizen or a celebrity), and whether or not the tweet had a source attached. This type of randomization is important because it allows one to disentangle the endorsement effect from the reach effect, an important contribution of this paper. The endorsement effect refers to the response to a celebrity directly tweeting a particular message, whereas the reach effect refers to the response when the celebrity simply re-tweets the message that was posted by an ordinary citizen. This effect is important to disentangle using careful randomization since doing so using observational data/econometric methods is challenging. Given that the authors had an opportunity to randomize, they are able to distinguish this. From a methodological perspective, this is a significant value addition to emerging work that uses social media data to study a range of issues including polarization, misinformation, and message diffusion (the topic of this paper).
The paper has some important findings:
Messages that come (originally) from celebrities are more likely to be engaged with (retweeted or liked), relative to those that come from ordinary citizens. This finding is important to understand how celebrities (and their endorsements) are central to maximizing the reach and engagement of public health messaging via social media.
The second finding is that including a source in the message (which the authors refer to in their design as the credible sourcing treatment) actually reduces the likelihood of engaging with a message. This is counterintuitive (as the authors admit), since adding an information source should ideally boost the likelihood of engagement, not reduce it. They do well to use their theoretical framework to explain that this might be on account of the lack or originality associated with a message that comes with a source. By introducing this variation randomly, the authors are better able to build on the value of celebrity endorsements, by also being able to study what type of message is likely to get the most reach and engagement when it comes to health communication in Indonesia.
Last, they find that being exposed to these messages increases vaccine-related knowledge, promotes accurate and scientific beliefs around immunisation, and has better recall (i.e., that participants are more likely to recall the associated hashtag). This last piece of evidence is actually the most critical to public health, since it suggests that such campaigns can have measurable real-world impacts, outside of the social media platform on which such interventions can be designed and implemented.
Here, I restrict my comments primarily to the causal inference aspects of the paper, and potentially ruling out other explanations for the results.
Overall, given the count outcomes data used, the Poisson regression model is a reasonable choice for estimation framework, and they also use a rigorous method to control for multiple hypothesis testing. However, they would do well to attempt to use existing methods that take into account a similar approach (e.g., Young 2018).
First, in modelling the user response to a tweet (whether it comes from a celebrity or another ordinary citizen), the authors acknowledge the potential endogeneity arising from how much the message has been retweeted or liked at the time that the user saw the message. It is of course challenging to do this, but using timestamps, there could potentially be a way to account for the immediately preceding reach or engagement that the tweet received as this could influence to a large extent whether a user chooses to like or retweet the message. They attempt an exercise similar to this in Table E.1 (in Appendix E of the 2023 working papers), but it proceeds with arbitrary linear thresholds (i.e., 5, 10, 15 tweets), whereas it should ideally enter as a continuous variable in this estimation.
It also could help link this work to other work in computer science and networks on how messages go viral (Berger and Milkman 2013), which suggests that tweet engagement is likely to be a function of emotions and may not always follow a clear chain as described in the paper. These may not be mutually exclusive per se, but I suspect that incorporating emotions in the model may be challenging as they can perhaps only be primed in the messages (e.g., an emotional appeal could be another treatment group that was randomly varied).
Second, tweet engagement is also strongly determined by the Twitter algorithm, which we have no clear idea of. This means that the fact that a user sees or engages with a tweet is not solely due to the fact that a certain number of people tweeted it, but also how the algorithm weights the importance of that tweet to audiences that may or may not have mutual network nodes with those sharing the content. The authors make a mention of how the feed works and also note changes in the algorithm in a period around their study (footnote 10), but it remains unclear if the implemented randomization translates uniformly to all Twitter users in Indonesia. This brings me to the main issue with the suggestive evidence provided on impacts of this intervention on the “offline” sample using phone surveys. The authors acknowledge the limitations of this, but I think they need to also emphasize that these are mainly correlations and cannot trace any causality back to their interventions, which is an important limitation of this study, given that the offline data relies on self-report information related to knowledge and beliefs around vaccination.
One way to overcome this is to perhaps use secondary data (if available) on child immunization per capita correlated with the regions where the hashtag #AyoImmunisasi received the most engagement. This data may not be public, but given access to the API, it may be possible to correlate the two.
Translating online engagement to offline behaviour is without a doubt challenging, and a very ambitious ask for a project that already has quite a bit of novelty and rigor in terms of experimental design and inference. However, it helps to go beyond self-report data, and also correlate with whether such knowledge and beliefs might have changed owing to the large-scale social media campaign (for which panel data from users before and after the Twitter intervention would be needed).
Two other minor things to consider in terms of the validity of the design:
Can users (including celebrities) delete the tweet once it is sent out as part of their participation in the experiment? This would also affect the extent to which it has reach or engagement. If authors could report on whether this was possible (which I suspect it was, although recruited individuals may have had to agree to not delete the content after it was tweeted).
Finally, in an older version of the paper (NBER working paper, 2019), estimation of equation 3.3 was conditioned on a sample of individuals who followed at least 3 celebrities. It was not clear how this criteria was developed. I could not find this in the most recently updated version (2023).
Overall, this paper offers important and critical information on effectiveness of public health messaging via social media. It also helps to disentangle important aspects of reach vs. engagement in social media, when those with power on these platforms send out messages, and precisely the channel through which one might expect diffusion. The offline sample results, although important from a public health outcomes perspective, are correlational in nature, and can be ideally put down as a supplementary part of the paper or bolstered with some secondary data on immunizations before and after the intervention.",,"Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",85,,,,,,80,,,90,,,85,,,80,,,100,,,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",5,4,,,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

","I am 2 years post-doc, and have been working in applied microeconomics in a developing country context (India) since 2014, and have been working on health policy and communication (with a focus on behavioural science) since 2020.","For the Unjournal, I have been managing editor on one other evaluation, and have evaluated one other paper. As a peer reviewer, I have done about 25 papers, largely in the domain of health and applied economics. I am also currently Deputy Editor at South Asia Research, where I edit papers in economics.",False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,Relevance to GP: 100,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,When Celebrities Speak: A Nationwide Twitter Experiment Promoting Vaccination In Indonesia,https://unjournal.pubpub.org/pub/alatasevalsum,The Economic Journal,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-30T08:36:13.865-04:00,
Do Celebrity Endorsements Matter? A Twitter Experiment Promoting Vaccination In Indonesia,https://www.nber.org/papers/w25589,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Evaluator 1,,"Overview
Note from David Reinstein, Evaluation Manager:undefined
This evaluator considered the February 2022 (Stanford) working paper version titled “Designing Effective Celebrity Messaging Results From a Nationwide Twitter Experiment Promoting Vaccination in Indonesia”
Overall Summary
In this paper, the authors conducted a nationwide Twitter experiment in Indonesia (pre-covid; this experiment took place in 2015-2016). The aim of the experiment was to better understand which aspects of social media campaigns are important for disseminating a message; in particular, the role of celebrity endorsement (including celebrity authorship specifically) and of referencing credible sources. I think this is a highly important and relevant topic for global priorities research, that addresses a gap in the literature (since there is significant disagreement as to whether celebrity endorsements work or not.)
The paper’s key findings were that (1) celebrity messages (regardless of their social network position, i.e. the number of people who could see the tweet) are more likely to be liked and retweeted, (2) most of this effect comes from celebrity authorship, rather than merely passing on a message, (3) citing external credible sources (perhaps surprisingly!) decreases retweets, and (4) people who were exposed to more vaccination messaging appeared to have more accurate immunisation knowledge, and reported more vaccination among friends and neighbours.
In general, I think that the methods used were high-quality; although I point to a few areas for additional clarification below. In particular, I am unsure about whether there are potential confounds in measuring the retweet/ liking behavior (for example, due to users being able to see different comments and numbers of retweets across experimental conditions, which might drive different patterns of retweeting/ liking). Regarding the offline behavior, I am happy this is included since this data is of key importance for practical application. However, I am curious whether the people who were more likely to see the celebrity tweets might be better-informed in general—it seems possible to me that this might explain some of the results seen here, although this could also reflect a misunderstanding on my part (i.e. I am not sure that exposure to tweets was truly random; see below). I also note that while the evidence is suggestive of a link between mass media campaigns and increased vaccination rates, this data is not yet clear (since the data here only speaks to the subjects’ awareness of others’ vaccination status, which the authors themselves also point out). I think that further research into how these campaigns into real-world behavior is critically needed, since it is unclear whether retweets/likes are a reasonable proxy for ‘increase in vaccination rates’ (which should be the real measure of a campaign’s success).
I point to some additional data that I would like to see. For this paper specifically, I would like to see some more detail about the celebrities who were included– should we expect that the Twitter audiences are pro-vaccination or against vaccination in general? I think we might see very diverse effects according to the celebrities’ Twitter audience (i.e. if a Twitter celebrity with an anti-vaxxer following tweeted a pro-vaccination message, I might expect fewer likes and retweets but perhaps more vaccinations from people who would otherwise never have gotten vaccinated). Understanding these effects seems to be a key step prior to real-world implementation.
I think that the statistical methods used are appropriate, and I commend the authors for pre-registering their study. However, I would love to see the code and data used for this study openly available (e.g. anonymised on DRYAD). I would also have liked to see more detail in the pre-registration analysis plan.
Overall I think this is a good paper, examining a topic of key importance. I think this study approaches a key question that is very difficult to experimentally examine, and their methods (although not perfect— I think this would be impossible to do) are fairly rigorous. I hope that further research is conducted in this area, so we have a sufficient understanding as to put some of these principles into practice- I do note some potential longterm implications of celebrity endorsement (beyond the scope of this study) that we should attend to prior to real-world application, and I emphasise that understanding the psychological mechanisms underlying these effects may be key to ensuring effective implementation/ understanding generalisability.
Methods summary
The experiment used two main interventions to examine online behavior. In the first intervention, the authors tested (1) the effect of knowing that the tweet originated from a celebrity, and (2) the effect of knowing that the celebrity authored the tweet.
For (1), the authors took advantage of the fact that Twitter’s timeline shows the identity of only two people within tweets; the originator who wrote the tweet, and the person who you follow who directly passed it to you (i.e. who retweeted it in your network). Here, the message would either originate directly from a celebrity (and be retweeted by the followers of the celebrity), or be authored by a Joe/ Jane (and be retweeted by a celebrity, before being retweeted by the celebrity’s followers). The authors examined the behavior of the ‘followers-of-followers’ of the celebrities, because in the latter case these individuals would not be aware that the celebrity retweeted the message- they would only see the name of the regular user who wrote the message, and the person who they follow that directly passed it to them (the celebrity follower). This meant that it was possible for the authors to isolate the effect of knowing that the celebrity originated the message (versus a regular Twitter user), while holding the network position constant (since celebrities will generally have larger audiences than non-celebrities, but the celebrities have ‘anonymously retweeted’ the message from the perspective of the followers-of-followers, in the latter case). The authors found that tweets originating from a celebrity had a 72% increase in the retweet/ like rate, relative to those that originated from a regular Twitter user.
One problem—which the authors spot— is that F1’s behavior (decision to retweet or not) may be endogenous- i.e. the specific F1s who decide to retweet may depend upon whether the celebrity authored the message or not. Although the equations that the authors used controlled for the log number of F1s who retweeted (and therefore the number of F2s who could potentially retweet it), it could be the case that especially influential F1s (i.e. the composition of F1s who retweet) could be endogenous and also affect F2’s behavior.
I think the authors’ methods to account for this are reasonable. They used a subset of their Joes/ Janes (who were also F1s, i.e. direct followers of a celebrity), and had some of them randomly retweet the celebrity tweets/ retweets- creating exogenous F1s. They then analysed the experiment using the subsample with exogenous F1s. Given that the results were similar to that observed in the full experiment (point value estimates were actually a bit higher. I note that the p values are a bit lower, but I assume this is due to lower sample size), the authors concluded that this effect was not creating a bias in their results. Ideally, I would have liked to have seen more info about the authors’ plan to deal with this within the pre-reg analysis plan.
I am less sure about the authors’ response to the second potential confound; the fact that the retweets show the number of times that the original tweet has been liked and retweeted (which may vary by condition, since the treatment affects the retweet count). I am unsure of the extent to this effect (i.e. whether this created a difference in retweets across treatments that was considerably > 15; if so, I would remain unsure about whether this confound was contributing to the overall effect). Apologies if this is in the paper somewhere and I missed it.
An additional factor that I think could be somewhat endogenous—and does not appear to be mentioned in the paper—is the comments beneath the post. Based on my understanding of Twitter, I think F2 would be able to see these under the tweets and retweets. I wonder if these comments (from F1s) are different (both in terms of content and volume) based upon whether the celebrity authored or retweeted the tweet, and if this affects F2’s behavior differentially across condition. I would be really interested to see some analysis of this.
For (2), the authors then used the same experimental variation, but examined the celebrity followers; who had visibility into both the tweets that were directly authored by the celebrities, and the tweets that were written by a regular user but retweeted by a celebrity. The authors found that tweets that were authored by the celebrities were 200% more likely to be liked/ retweeted than those where the celebrity retweeted another Twitter user’s tweet, implying that 79% of the celebrity endorsement effect comes from authorship specifically.
I do not see any problems with this method; this is based on F1’s behavior (rather than F2s), so there are not the same concerns about whether F1’s behavior is endogenous.
In the second intervention, the authors tested the effect of including credible sources. To do this, they randomised whether a source was included in the tweet, and examined the behaviour of the celebrity followers. The authors found that citing a public health authority reduces the retweet and liking rate by 26.3%.
I think these methods (looking at F1’s behavior) are appropriate.
Importantly, the authors also measure offline behavior. As far as I am aware, this is the first real-world data examining behavior change as a result of a celebrity mass media campaign—I congratulate the authors for collecting this data, which I think is very important for real-world application. These offline effects were measured by randomising the celebrities into two phases. A survey of the celebrity followers was conducted between these phases, and the between-celebrity randomisation was used to estimate the impact of the Twitter campaign on offline beliefs and behaviors. (I.e. based on the number of celebrities that a user follows who were randomised to tweet before the phone survey, versus after).
One potential problem I see (which could be a misunderstanding on my part) is that I think that ‘Exposure to Tweets’ is a function of both the number of celebrities that the individual follows at baseline (of the celebrities who were involved in the campaign), as well as whether the celebrities they follow happen to be randomised to tweet before or after the survey. Presumably, the individuals who follow more of the campaign’s celebrities (and were therefore more likely to see the campaign's tweets before the phone survey) exhibit different characteristics than the average Twitter user (i.e. this method may present confounds).
These individuals who follow a lot of the campaign’s celebrities may follow more people in general, and perhaps be better-informed in general than their counterparts. These individuals might also spend more time on Twitter in general than the average user. Assuming that the celebrities who took part in this campaign are more health-conscious/ accurate than the average celebrity (since they volunteered to take part in this campaign), these users might follow more accurate health sources in general than the average Twitter user, be more interested in health, or be people who are especially motivated to seek out immunisation information/ talk about health-related topics.
I am therefore unsure if this explanation could contribute to the effect that the authors observe (that people with higher exposure to tweets were more likely to have heard of the Twitter hashtag, to have improved knowledge of immunisation facts, etc).
I also note that although we have information about people’s reports of others’ vacciantion status, we can’t be sure whether ‘reports of increased vaccinations by friends/ neighbours/ etc’ actually correspond to an increase in vaccination. It could also be the case that people are simply more likely to report their vaccination status to the people who have a higher exposure to the campaign’s tweets (i.e. because they realise that person is more pro-vaccination than the average person) or the person is more likely to talk about/ remember other people’s vaccination status. However, the authors do point towards this out, i.e. by saying ‘does the campaign appear to change immunization behavior as reported by our survey respondents?’
I would be curious if more survey questions were asked and not published (can’t spot this information on the pre-registration or appendix.)
I also note that the subjects were recruited via ads (I assume that there wasn’t another viable option). I do think that people who are likely to sign up for a study based on a Twitter advertisement are likely to be more heavily engaged with Twitter than the average user–perhaps these people are more likely to be responsive to informative tweets than the average (less engaged) Twitter user.
Statistical methods summary
I think that the Poisson regression models used to estimate the effect of endorsement and authorship, and the logistic regression model used to understand the offline effects, are appropriate. I think that the statistical methods used in the paper are of high-quality, although I note that I am not especially familiar with some components of the authors’ methods (e.g. double post-LASSO)—if other reviewers have more expertise here, I would defer to their views.
One point of clarification (regarding the dummy variable used for the different types of messages included); am I correct that this only related to content (fact/ importance/ misconception correction?) If so, I wasn’t sure why this had a dummy variable, since the message content factor was already randomised. This may be a misunderstanding on my part.
I commend the authors for pre-registering their study on the AEA RCT registry. However, the analysis plan (here) is blank. I think this is an oversight since it left me unable to assess whether there were deviations from the original analysis plan (e.g. if lots more survey questions were asked and results not reported).
The data and code used to analyse this study don’t appear to be publicly available. If possible, I think that having data (e.g. anonymised and placed on DRYAD) and code available would be of significant benefit.
Relevance to global priorities
I think the topic is of high importance, and is very relevant to global priorities (although I think that more research is needed until this paper can be action-guiding in practice; see below.)
Understanding the role of celebrity endorsement in driving up immunisation is a gap in the literature, where there is significant disagreement. If celebrity endorsement does work to drive up vaccination rates, this could be a key method used to improve vaccination rates in people who might not have gotten vaccinated otherwise. Given the reach of social media, these interventions have the potential to be hugely impactful if they do have an effect upon their audience.
I would love to see more information about the celebrities who volunteered for this campaign;
A key question I have for this study regards the celebrities (and their followers) who volunteered to be a part of this campaign. Thinking about the US, vaccination hesitancy became split along political lines- I would expect that celebrities with lots of democratic-leaning followers would get a lot of retweets/likes of a pro-vaccination tweet, but probably fewer ‘counterfactual’ vaccinated people as a result. I.e. I would expect that the people who liked these tweets were always pretty likely to get vaccinated, regardless of the tweets from that celebrity. Are there likely to be similar effects going on within the Indonesian data? I am not sure if any of the campaign’s celebrities had Twitter audiences who were especially likely to be against vaccination, or who were likely to be especially pro-vaccination. I’d love to see some microdata about how people from different audiences (based on characteristics of the celebrity’s typical Twitter following) respond to these tweets; I suspect some celebrities have a far higher impact (in terms of driving up vaccination rate) than others.
Understanding ‘why this works’;
As a general point (outside the scope of this paper), I think that understanding why celebrity endorsements work (from a psychological perspective) will be key to ensuring their best use, and to understanding the generalisability of the findings from this paper. For example, is it that some people trust celebrities more than official healthcare organisations? If so, it might be best to target celebrities that are highly trusted in vaccine-hesitant communities. On the other hand, perhaps people want to be similar to high-prestige celebrities? If so, it might be best to target celebrities that people in vaccine-hesitant communities admire or want to be like. Without understanding these mechanisms, policy-makers are liable to make implementation errors that reduce effectiveness. I think that this mechanistic understanding is key to understanding whether these results will generalise to other populations.
Similarly, to what degree should we expect social media behavior to follow-through to real-world behavior? Is it the case that people retweet certain celebrities to generate a particular image on social media, or because they are more altruistically motivated to pass on accurate information? Understanding these factors may help researchers to design their campaigns appropriately (i.e. to drive up vaccination rates- note that the amount of retweeting/liking may be an inaccurate proxy of a media campaign’s success).
Regarding the finding that citation of credible sources decreases retweets;
I have a lot of uncertainty here, and highlight the need for more research. I suspect this is an area where (as above) understanding ‘what is the mechanism underlying this effect’ may be important.
Is it that including these citations makes a tweet less readable and look more boring? Is it that people on social media mistrust medical organisations in general? Is it that including these citations makes it clearer that the tweet is not in the celebrities ‘own words’ (so it looks less personal)? The best solution is dependent on which factor is driving this effect.
I am also unclear if ‘number of retweets’ is a good proxy here for the result that a policy maker would be most interested in- ‘number of additional vaccinations’. It doesn’t seem impossible to me that linking to a credible source might generate fewer retweets (e.g. if it looks more academic/ boring) but result in more vaccinations (e.g. it might be more boring, but it’s also more trustworthy- and that’s the factor that matters more for real-world behavior).
Finally, there might be longterm effects to encouraging celebrities to cite (or not cite) credible medical sources. Perhaps encouraging celebrities to cite these sources increases trust in these organisations in the long run, even if it results in fewer retweets (I have no idea, but just to highlight that there may be longterm effects here beyond the number of retweets).
Although this is outside the scope of this paper, I think it is worth considering whether there are potential long-term deleterious effects of healthcare organisations partnering with celebrities.
I think a cost-benefit analysis might well still say that the effect is likely to be very beneficial overall. But some risks include if the celebrity is then photographed going against the advice they gave (e.g. not wearing a mask, breaking social isolation rules etc)- perhaps this could result in a loss of trust against the healthcare organisation in general.
I am not sure whether (over a long-term timeframe) this kind of partnership might encourage people to rely on celebrities for healthcare information, rather than going to credible sources- which could obviously have negative effects.
On the other hand, perhaps people who update on healthcare information from celebrities would never rely upon the credible sources anyway.",,"Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",62,,,,,,55,,,55,,,70,,,45,,,55,,,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",3,3,,,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",Managing editor’s summary (to anonymize): Evaluator has been considering mass-media work only recently. They previously worked for 5-10 years as a researcher in psychology.,"Around 15 peer-reviewed papers, in psychology and [ME: a natural science field].",False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,Relevance to GP: 70,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,When Celebrities Speak: A Nationwide Twitter Experiment Promoting Vaccination In Indonesia,https://unjournal.pubpub.org/pub/alatasevalsum,The Economic Journal,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-30T08:21:50.438-04:00,
The Benefits and Costs of Guest Worker Programs: Experimental Evidence from the India-UAE Migration Corridor,https://www.nber.org/papers/w31354,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Evaluator 2,,"This is an evaluation of “The Benefits and Costs of Guest Worker Programs: Experimental Evidence from the India-UAE Migration Corridor”[undefined]. The evaluator summarizes this as follows: The key contributions of this paper are its analysis of intermediary costs of migration, migrant well-being and diversity in the workplace, expectations of job-seekers and joint comparisons of various pecuniary and non-pecuniary measures of migration. A major limitation is the lack of a welfare justification for randomisation and little to no discussion on equipoise. My suggestions largely revolve around improvements to the discussion of methodology, representativeness, attrition and representation of treatment effects.","In this paper, the authors describe their findings from a randomized experiment on jobseekers in India. In particular, the paper studies applicants to employment opportunities at two construction firms in the UAE. Data is collected from approximately 4-5 thousand workers at recruitment centers in six Indian states of a Singapore-based recruitment firm. Five out of every seven candidates who successfully clear the recruitment firm's selection process are randomly selected and offered positions. Through a baseline survey, two tracking surveys, one follow-up survey, and multiple alternative administrative data sources, the authors observe and analyze the effects of being offered an opportunity to migrate on several outcomes including actual migration, labor market indicators, indicators of well-being, indicators of work-satisfaction, religious and linguistic diversity in networks and attitudes toward democracy, religious diversity, and inequality.
The primary findings of this paper are as follows:
There is a high rate of non-compliance among workers who are offered positions, only 58% migrate to the UAE.
Treatment groups experience significant increases in earnings and hours of work and significant decreases in physical well-being at the workplace. There are no significant effects on other dimensions of well-being.
10% of gains from international migration are captured by intermediaries or migration agents.
Among those who migrate to the UAE, prior expectations of earnings compare well to actual earnings.
Treatment is also associated with increased exposure to individuals of different religions and linguistic groups and favorable attitudes.
There is little noted heterogeneity in returns to migration, except for the group of workers who have experienced migration before.
A theoretical model predicts that doubling of total offered compensation would induce full compliance among the non-complying workers (who are offered the job but do not migrate).
Evaluation
1. Methodology
A. Ethics of migration
Did the authors of this study randomly offer employment and migration opportunities to some eligible candidates and consequently randomly prevent employment and migration opportunities to other eligible candidates? Or did the authors of this study exploit an existing institutional randomization in the job market?
The authors make a rather weak argument for the latter, by describing the randomization process as ""a natural extension of an existing system in which firms request visas from the MOL and sometimes are granted all of them and sometimes fewer than they request"" (Page 10). However, footnote 21 clarifies that the two construction firms in question did indeed have a guaranteed number of visas. This appears to be a contradiction, in that in the absence of this study, presumably, the workers would not have experienced any randomization through the visa selection process. Footnote 21 further clarifies that the firms ""did have to agree to screen more applicants than they usually would need to for every position they wanted to fill"". Did the recruitment firms increase the number of screened applications by inviting more applications or by loosening their criteria for the screenings?
Without more information, it is difficult to assess the welfare costs of this study on its participants. In its current form, it appears that in the absence of this study, at least some of these control group participants would have received an opportunity to be employed and migrate to UAE and others in the treatment group may not have successfully passed screening. Both these possibilities have important implications, most importantly ethical implications but also empirical implications (See 2A1). Furthermore, the authors do not mention any IRB, nor do they describe the ethical oversight or involvement of local and other stakeholders.
Knowing what is shared in the paper in the form of previous literature on the returns to migration, it is hard to believe that authors did not have any priors about the welfare implications of employment and migration on study participants, and if that is the case, how do they justify equipoise? Taking this argument further, it is also likely that authors might have had credible priors on which groups of participants benefit most from employment/migration based on the baseline survey.
B. On selection and representativeness
The literature review discusses the contribution of this paper as better understanding selection into migration. This deserves further qualification: the paper is able to qualitatively describe general selection into migration, but the empirical contribution is rather addressing the selection into migration conditional on applying for an international job, i.e. conditional on having intent to migrate.
The experimental design mentions that the recruitment sites are a sample. If so, how were recruitment sites selected? How many recruitment sites do the partner firms have? Do they only recruit from the 6 listed states? The recruitment sites are mentioned as being close to construction training schools. Are workers mostly fresh graduates of these schools?
One issue of selection and/or representativeness that the paper does not tackle is whether the 6 states are representative of migrant workers from India to the UAE (in construction or general). It is well known that southern Indian states, particularly Kerala, have historically contributed most in numbers toward migration to UAE. The paper would benefit from further comments and comparisons on this front, potentially using data from the Kerala Migration Surveys.
The exclusion of southern states also has an implication for the analysis of friendship and linguistic and religious exposure. Specifically, given the historic pattern of migration from south India to UAE, it is likely that Indian migrants from historically non-contributing states are more likely to have increased exposure to new linguistic and religious groups through migration than others from historically contributing states. This paper has an empirical advantage on this front, and this deserves some discussion.
At the same time, one potential disadvantage of this exclusion of southern states is that it limits the generalizability of the analysis of marginal treatment effects, especially if the authors expect that heterogeneity (either in baseline characteristics or in returns) [will] be more pronounced among historical migrant groups. Just as how authors discuss that estimates may be a lower bound on the non-pecuniary costs (Page 41), because the sample is drawn from candidate interviewees, it would be helpful to discuss similar implications because of sample states.
For similar reasons, it would be useful to discuss state-based heterogeneity in the analysis of heterogeneous returns.
2. Empirics
A. Always takers and Never takers
A more detailed description of never-takers and always-takers which would help contextualize the ITT results better;
On never-takers, what is the historical percentage of applicants to refuse to take up the offer? Given that the authors mention that the recruitment firms screened more candidates for the purposes of this study, is it possible that the percentage of never-takers is more than usual?
On always-takers, is it the case that applicants that visited the survey recruitment centers also visited other recruitment centers during this recruitment period? What are the channels by which the applicants who failed the screening entered the UAE job market? Were they recruited by similar recruitment companies? Did they end up working for similar firms?
B. Attrition
An important part of this paper is how authors deal with attrition. Attrition is impressively contained and [this] is testament to what must have been painstaking work for the research assistants and authors.
In describing the observed attrition, authors could include more details. For example, what are the dimensions in which there is imbalance? While this is self-evident in Appendix Table A.1., this information should also be discussed on Page 16. Is there any proposed explanation for the observed selection into attrition? Another discussion worth having is whether attrition is observed differently for the treatment and control group, as the authors seem to assume this in bounding their results in Section 4.4. If so, what is the difference, and what are the authors' priors on how this would bias the results? A qualitative discussion of this would help [convince readers] about the usefulness of the bounding exercise carried out later in the paper.
C. Treatment effects
It is unclear why authors do not control for baseline wealth (as measured by net assets) in the specifications, given that there is imbalance in this dimension between the treatment and control group. Do other fixed effects take care of this? What is the implication of this baseline difference on the follow-up finding of no significant differences in net assets between the treatment and control group (Table 7)?
While most treatment effects reported in the paper are conservatively ITT, and authors do share with us the scaling factor for TOT earlier in the paper, it would be useful to also have a TOT/IV represented in the main results tables, especially in the context of the impact on well-being and work satisfaction.
Is it possible that workers who have received offers have fairly accurate expectations of their earnings because the partner firms/recruitment firms have communicated this information well? If so, is this an unusual characteristic of the employer firm?
Is the no-offer density of well-being bimodal in Figure 4. because of the sub-group within the control group that did migrate to the UAE?
3. Policy Implications
In addition to the national policies mentioned on Page 9, it would be good to learn about … state policies and bilateral agreements with the UAE [if there are any]. What are the specific rules and regulations governing migration from the 2022 MOU that have little enforcement? This is especially important to [help us] understand whether there is a current policy audience for the findings of this paper.
4. Editorial Comments
Recommend not using the terminology ""lower caste"".
Recommend moving Appendix A2 to main text.
On page 10, it would be helpful to clarify whether offered wages have little variation within the firm or across the two surveyed firms?
On Page 24, the authors mention that ""They are earning less in India than they expected to earn if they migrated, consistent with the idea that they only migrate for higher earnings."" This claim is more credible without the use of the word ""only"".
The term ""Indian men"" is often used to refer to the sample, but it would be more accurate to say, ""male construction workers from north India"".
Figure 5 legends mention ""Less"" of certain indicators, which is unnecessary and confusing as it appears as a double negative (for example, negative coefficient on less Fighting).
On Page 41, perhaps it should be given the lack of heterogeneity in returns on observables.
Calculations in Section 4.8 is the most interesting contribution of this paper, and it would be helpful to have more subsections in this section, delineating the model, estimates from literature, assumptions/calibrations made and final calculation.
On Page 22, the uthors mention that this study provides evidence against the idea that migrant workers in the GCC are forced to work excessive hours. However, in footnote 15, the authors describe that, in this context, the two firms that they are partnered with are large firms that are more likely to follow labor regulations. Perhaps they can qualify this statement more.
In page 3, the authors describe that never-takers have somewhat lower UAE earnings. They should clarify that these are calculated potential outcomes (given that it is only shortly before that the authors define never-takers as those who do not migrate).","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",75,60,90,,,,40,60,30,85,75,95,85,60,90,90,85,95,90,85,95,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",3.5,4.3,3,4,4,4.5,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

","Mid-career development professional, currently pursuing a PhD in economics","Journals/Grants: 0
Other peer-review: ±5",False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,The Benefits and Costs of Guest Worker Programs: Experimental Evidence From the India-UAE Migration Corridor,https://unjournal.pubpub.org/pub/guest,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-28T08:24:59.361-04:00,
The Benefits and Costs of Guest Worker Programs: Experimental Evidence from the India-UAE Migration Corridor,https://www.nber.org/papers/w31354,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Evaluator 1,,,"Overall, I find the project to be positive, and have provided detailed comments and suggestions below.
Advancing knowledge and practice
The project attempts to estimate the individual returns to temporary migration of Indian construction workers to the UAE. This is a heavily debated topic with considerable welfare implications for both the source and destination countries, for which there is a significant gap in the literature. The authors do a good job of arguing the importance, neglectedness and tractability of their research question.
Justification, reasonableness, validity, and robustness of methods
The methods are clearly justified and explained. The methods and underlying assumptions are reasonable and commonplace for an experimental set-up. The authors are open about potential sources of bias and the viability of the assumptions, and they discuss the representativeness of their sample, the attrition problem that comes with follow-up surveys, etc; and they address for these issues as they can.
However, there is a limitations in the methodology. When individuals could not be reached for follow-up surveys, their friends and family members were interviewed about the individual’s whereabouts and earnings instead (pp. 13 – 14). These observations are at an increased risk of measurement error than those where individuals themselves answered the survey. However, the authors do not discuss the possibility of measurement error here, or how to address it for the regressions where these observations are included.
Logic and communication
Most concepts are clearly defined, and the reasoning is transparent. The data, analysis, tables and figures are relevant. The authors’ conclusions are consistent with the evidence.
However, two things are unclear. First, the terms of the contract between the labour broker and the job seeker (Background section, pp. 8 – 9) were unclear. Specifically, do job seekers have to pay the contingent agent fee if they are offered a job but decide not to take it? If not, this could be a reason why some individuals in the treatment group choose to remain in India, as the new increased wage they would get in the UAE is not enough to make it worthwhile to pay the contingent agent fee. In this case, the reasons for individuals choosing to remain in India are not necessarily non-pecuniary as argued by the authors. If data is available, it would be interesting to check for heterogeneity in the effects of the UAE job offer by the level of contingent agent fee, particularly as there is considerable variation in the fee amount (depicted in Table 1, pp. 8).
Second, in follow-up surveys individuals are recorded as either: (i) having migrated to the UAE, (ii) having stayed in their home district in India, or (iii) having moved to a new district in India (discussion in pp. 15-16, follow-up survey questions in Table A.5 pp 56-57). Here, a fourth option is that individuals migrate to foreign countries other than the UAE. It is unclear whether this option is assumed away, or whether it is simply a feature of the sample that none of the individuals migrated to another country.
Open, communicative, replicable science
The methods are explained sufficiently, and the authors are open about potential limitations, discussing them at length. The authors make a variety of summary statistics and tables available. The original data is not provided, but this is commonplace in experimental set-ups.
Other comments
Suggestion: when discussing the effect of the UAE job offer on individuals’ social networks, the authors do not control for the individual belonging to a minority or majority caste or religious group within their region. This data is straight-forward to get from the IDHS (which the authors use elsewhere in the paper). Adding this variable could show some interesting patterns regarding belonging to a “minority” or “majority” group at home and attitudes towards new friend groups once one becomes a migrant and therefore the minority.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",75,70,80,,,,75,80,70,80,75,85,70,65,75,70,65,75,80,75,85,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4.5,4.5,4,5,4,5,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",[Range coded to protect anonymity: 2-5 years],"None, this is my first.",False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),See evaluation report.,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,"Relevance to GP: 60, 70, 80",,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,The Benefits and Costs of Guest Worker Programs: Experimental Evidence From the India-UAE Migration Corridor,https://unjournal.pubpub.org/pub/guest,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-28T08:24:39.421-04:00,
Legalizing Entrepreneurship,https://www.nber.org/papers/w30624,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Evaluator 2,,"This paper studies how entrepreneurship in Colombia was affected by a nationwide policy that made almost half a million undocumented migrants from Venezuela eligible for resident’s visas (PEPProgram). The authors find that having a visa increased the levels of entrepreneurship among this population. As written in the evaluation [content] below, it has a lot of strengths. The contributions are clear and are both empirical (rarely studied outcome) and methodological (new empirical approach). In the “suggestions and minor suggestions” section, some possible extensions and [critiques] are specified.","First Impressions
This paper studies how entrepreneurship in Colombia was affected by a nationwide policy that made almost half a million undocumented migrants from Venezuela eligible for resident visas (PEP Program).
The authors find that having a visa increased the levels of entrepreneurship among this population.
Strengths
It is an amazing paper to read. Very well written, especially since the context of the policy is key for understanding the logic of the methodology used.
Its contributions are both empirical and methodological. In the empirical part, it studies entrepreneurship, an outcome that has not been studied much in the literature of gains from legalizing migration. The methodological contribution is the application of the new methods that study how to get casual estimates when there is a nonrandom exposure to an exogenous shock. They translate these methods to the context of a regression discontinuity design and propose an instrument for differences between the predicted and actual realizations of the running variable. This is one of the most novel parts of the article.
Apart from the proposed methodology and its instrument, it also uses a regular panel method which provides similar results for the main specification.
Suggestions
I find it hard to make comments criticizing this paper, however, some ideas have come to mind that might be useful to think about them, either as a robustness check or a possible extension.
The article finds “similar impacts on the creation of both employer and non-employer firms.” While these employer firms create 1 to 6 new jobs and are not ‘high growth’ by the typical standards of developed countries, they still represent meaningful economic spillovers.” I wonder if it would be possible to try to study some of these economic spillovers. For example, using employer-employee matched data from the “Planilla Integrada de Liquidación de Aportes (PILA)” one could study the effect of obtaining the PEP Pardon on the likelihood of obtaining a job in these new firms created by Venezuelans. If there is an effect, it would be related to the literature on the importance of networks in the job market like for example Calvó-Armengol (2004)[undefined] or Kramarz and Skans (2014)[undefined].
In the panel specification, basically it’s like a diff in diff approach between those who received and those who did not receive PEP. It is not that clear to me why those who did not receive it see an increase (albeit much lower) in entrepreneurship (see figure 7). Did something change?
If there is also the date on where these persons received the visa, one could also use the panel data for an event study instead of a diff in diff, as a robustness check.
The treatment effect by year seems to be statistically significant until 2022 and 2021 (figure 6). I was wondering about the mechanism that might explain this lagged effect.undefined My prior is that to start a business, although having access to the banking and borrowing systems are important, having a credit history in a bank is as important and takes longer. The effects of the event study might be comparable with the findings on figure 6. However, this might also be because of the Covid.
This is speculative, but you can use the identification of Bahar, Ibañez and Rozo (2021)[undefined] at the municipal level to see aggregate effects on entrepreneurship. If it’s only the sum of firms of persons with the PEP pardon, it would be a robustness check in the aggregate of some sort. If it is new firms in general, it would include the direct effect of having more entrepreneurs and the indirect effect of having a larger pool of workers, which might benefit all firms because of potentially cheaper labor. Although, given figure 6, it seems there is no effect for Colombian native firms.
An improvement in the identification of papers like Bahar, Ibañez and Rozo (2021)[undefined] might be done also. In there, they use as an instrument the average registration day available by department. One could compute not only the actual average registration days available by department but the expected one, using the same permutations done in the paper.
Minor Suggestions
1. In the introduction, in the first page, the authors argue that in the panel data approach, they can decompose the immigrant entrepreneurship effect into a physical relocation effect and a visa effect. I firstly understood it as if the PEP-Pardon had an effect in migration towards different cities (for example, if I am going to open a business, maybe a big city like Medellín or Bogotá would be better). Then later in the text they explain that the physical reallocation effect is moving from Venezuela to Colombia. It could be my reading, but it might be worth it to recheck it.
2. Although it is a very good idea to present the assumptions in a general context… It would [also] be useful if at the end of each assumption, you could state for the case of the paper what would that mean directly. It would help the reader to think straightforwardly if the assumption is getting realized in the case of the study and to think about potential threats in a more direct way.
3. Table 8 and Table 9 show results for different sub-samples. Here, the results in the panel regression differs from the results using the IV cross-section. It would be important to explain these differences.
Main Advice
Regarding the identification, the regression discontinuity design compares outcomes among migrants who completed the RAMV, some earlier than other, obtaining a greater time window for the legalization process. Now, since both completing the RAMV earlier and having more time to obtain the visa might be endogenous to the level of entrepreneurship of the person, a source of randomness like precipitation is cleverly used. In terms of the assumptions for the instrument to work, they are well explained, and the context is a context where the instrument can work. However, the strategy is based on compliers whose PEP status was sensitive to the timing and length of the window.
This is the part of the paper that might be harder to believe. Even the people with a smaller time window had more than two months to get the PEP visa. One could think having two months or having two months and one week would not make such a difference, given the perks of being legalized in a country. Although table 3 tries to compare a classification of always takers vs compliers, one thing that could help is if the document could talk about the time people took to obtain the PEP visa. By this I mean the number of days from the date they could start the process, to the date they submitted the documents online to get the PEP visa. This would help to get an idea of how important is to have one or two weeks more, for example.
Conclusions
Overall, I find the paper very interesting, both methodologically and empirically. It is very well done, and I would think it is very likely to be published in a very good journal. I am not an expert in this field nor in the econometric procedure, but I would start by aiming, with a paper like this, for a top general interest journal and see the comments and feedback from those type of reviewers.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",90,85,97,,,,92,97,86,96,90,100,98,95,100,78,72,83,97,93,100,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4.7,4.4,4,5,4,4.7,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

","My research focuses on applied work but not particularly related to labor or migration. So I would say, not so long ago.","3 journals, 9 master thesis.",False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,"The methods are explained well enough and cite the original papers that implement the methodology. The data is not public, but it can be obtained by asking for permissions to the authorities in Colombia. They do not specify however, the procedure to obtain this data or who to contact, that would be useful for other authors.","Relevance to GP: 88, 94, 100",,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Legalizing Entrepreneurship,https://unjournal.pubpub.org/pub/legalizingentrepreneurshipevalsum,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-28T08:18:07.364-04:00,
Legalizing Entrepreneurship,https://www.nber.org/papers/w30624,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",,,"The study analyzes the effect of changes in migratory status on entrepreneurship through a policy intervention that made Venezuelan migrants in Colombia eligible for a resident visa. Causality is established by using an extension of a regression discontinuity design. The study found a positive effect on economic activity. However, it might not be precise on the mechanisms that are driving the results (for example, expanded access to credit, or subsidies?).","Main claim and overall assessment
The main claim of the paper is that improvements in legal rights for immigrants can have a positive effect on entrepreneurship, measured as the likelihood of registering a new firm. I consider that given the economic method the authors use, an extension of a regression discontinuity (RD), they can identify a causal effect of receiving a visa on immigrant investment choices, and it gives a high internal validity. The authors do not find any significant differences between the treatment and control groups (Table 2), so receiving a visa is close to a random experiment. To assess the external validity of the paper, it might be important to understand the mechanism behind this effect. For example, does obtaining a visa in Colombia give access to a range of opportunities different than in other countries?
Overall, the paper tackles a highly important and relevant topic for global priorities research, because migration flows have increased during the last years. In particular, more than 7.7 [million] Venezuelans have abandoned their country since 2018, putting pressure on the countries that receive them, and Colombia is one of the main host countries.[undefined] Therefore, a better understanding of how to promote socioeconomic integration is relevant for policy design.
Summary
In this paper, the authors study a natural experiment in Colombia that made nearly 500 thousand undocumented Venezuelan immigrants eligible for a resident visa in 2019. The aim is to study the entrepreneurship behavior of immigrants upon receiving this visa. The authors used the Registro Administrativo de Migrantes Venezolanos (RAMV), a census of Venezuelan immigrants, linked to the complete formal business registry, Registro Único Empresarial (RUES).
The paper uses two empirical approaches: an extension of a regression discontinuity (RD) design and a panel data approach. The RD extension relies on the knowledge of how the running variable is constructed to identify a source of random variation in running variable scores. They then map this back to a probability that each observation lies above or below each threshold. In this case, the design uses variation in the running variable (coming from rain) to instrument for each person's choice to apply for a visa. This application of this approach is a contribution of the paper to the literature, and it offers an econometric technique that can be applied to a broader set of questions.
On the other hand, the paper uses a panel format. This approach offers an alternative approach to the RD strategy and allows the authors to compare differences in business formation between the physical relocation effect and the receipt of a legal residency permit (the visa effect).
The main results suggest that immigrants who received a legal migratory status increase their likelihood of registering a new firm by 1.2 to 1. 8 percentage points. The increase in economic activity is equivalent to that of native Colombians four years after the visa was received. Also, the effect of receiving a visa is over twice as large as the relocation effect. Finally, the authors find evidence that the firms created by the immigrants are a meaningful source of economic spillovers.
The paper contributes to our understanding of how legal rights can change the investment choices of immigrants, as observed in their entrepreneurship. Also, the paper contributes to the literature by showing the application of an extension of a RD design.
Major comments
1. It would be interesting to delve into the mechanism behind the results. The authors provide some discussion on page 9; however, they do not provide any empirical evidence of the possible channels. For the design of better migration policies, it is important to understand [which aspects of] access to legal rights [particularly] allows migrants [to] start a business. For example, credit possibilities, subsidies, or a lower uncertainty because of the possibility of signing contracts.
2. According to the panel data approach and the results in Figure 6, the statistically significant effect on firm creation happened until 2022, with a slight increase in 2021. So, what happened with these immigrants between 2018 and 2021?
3. It would be helpful to include in Tables 2 and 3 the number of observations in each group.
4. It would be nice to develop an analysis of the assets of the firm. According to page 11, for all firms in the data, the authors obtained the number of assets. This variable might provide useful information on the type of firm created. Also, it might offer some proxy for firm productivity.
5. I [would] move the explanation that “Weather data was unavailable for six municipalities” in footnote 25 before referring to Table A2. Otherwise, it might be confusing why the sample size in Table A2 is around 250 thousand.
6. Is there any evidence of a network effect, such that people hired by these firms are also Venezuelans? I am not sure if the data allows them to study this, but it might be interesting to analyze.
7. For future work, it could be relevant to analyze the life cycle of these firms. Are they more likely to survive than firms of native Colombians? Also, given the foreign nature of the entrepreneur, are these firms more likely to become importers or exporters?
Minor comments
1. On page 17, it reads “In some settings, the mean value Xi can could be interpreted.” It should be removed the “can” or “could”.
2. Capitalize the first letter in footnote 21.
3. When the authors explain notation in Equation 2, the index should be k, not i.
4. On Equation 4, should there be a coefficient next to the ExpectedDelayi? Also, the error term should be eta, not epsilon.
5. On page 28, it reads “The coefficient for ActualDelayi is positive and […]” However, it is negative (-0.0031).
6. Sometimes the paper talks about “Columbia,” it should be “Colombia” (see for example page 23).","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",60,50,70,,,,60,70,50,60,50,70,60,50,70,50,40,60,80,70,90,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",2.5,2.5,2,3,2,3,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",5 years.,About 10.,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),"The topic is very relevant, given the increase in migration flows around the world. It is important to understand how to promote the socioeconomic integration of immigrants in host countries. The paper takes advantage of a policy intervention that increases our understanding of this matter.","Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Legalizing Entrepreneurship,https://unjournal.pubpub.org/pub/legalizingentrepreneurshipevalsum,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-28T08:09:19.028-04:00,
Money (Not) to Burn: Payments for Ecosystem Services to Reduce Crop Residue Burning,https://www.nber.org/papers/w30690,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Evaluator 2,,,"Overall Assessment
The paper Money (Not) to Burn focuses on a very significant environmental / health issue: crop residue burning in India, which contributes to air pollution resulting in tens of thousands of deaths each year. It does this by addressing whether the effectiveness of PES schemes could be improved by paying a portion of the money upfront and unconditionally. Addressing this issue is an important contribution to the literature on PES.
1. Advancing our knowledge and practice
The key contribution of the paper to PES’ knowledge and practice is to show that upfront payments can improve the effectiveness of PES by promoting compliance in a way that appears to be cost effective. This is very significant given that there is an ongoing debate whether PES schemes can be effective in addressing environmental problems. The paper would benefit from a clearer discussion of other PES research that has dealt with issues of upfront payments, participants’ trust of PES scheme implementers, and financial constraints to the participation of poor landholders in PES schemes.
The research is usefully carried out in the context of a developing country where PES schemes have faced more practical implementation challenges due to lack of funds/poverty among the land holders who are the focus of the PES schemes.
2. Methods: Justification, reasonableness, validity, robustness
The methods used are well-justified and clearly explained. I find them to be a reasonable approach to answering the questions considered by the research. The methods are also usefully deployed to address two key issues that are at the core of the implementation of PES in poorer countries: i) lack of finance on the part of the land holders who are asked to implement costly land management changes, and ii) to assess the impact of land holders’ trust that the PES payment will actually be provided.
The mixed methods approach that integrates survey questionnaires, statistical analysis and remote sensing analysis if well designed and very useful in addressing the research questions.
The description of the methods should clarify how the overall amount of the PES payment was set, given that this certainly has an impact on the uptake of the contract as well as compliance with it.
3. Logic and communication
The goals and questions of the paper are clearly expressed, and all the concepts clearly defined and referenced.
The reasoning presents a very high degree of transparency, as defined by Open Philanthropy's guide.
The research is presented in a logical way, and the arguments written in a way that is easy to follow.
The data and analysis is relevant to the arguments made.
Overall, the stated results and conclusions are consistent with the evidence. However, the statement presented in the first paragraph of the conclusion that the ‘insight is not PES-specific and is likely relevant for conditional cash transfer programs’ is a speculation as there is no information in the paper about cash transfer programs. The authors should either provide evidence of relevance of the research to cash transfer programs or delete the statement.
The following sentence in the conclusion (p. 16) also needs attention: ‘Fourth, the need for upfront payments might fade over time as trust in the program builds up.’ If upfront payments are required also to address financial constraints on the part of the participating landholders, as stated by the authors, then it is not clear how the need for upfront payments would fade. There would need to be an assessment of the relative significance of trust and financial constraints to clearly understand this issue.
The tables/graphs/diagrams are easy to understand in the context of the narrative.
4. Open, collaborative, replicable science and methods
4a. Replicability, reproducibility, data integrity
The method and its details are explained sufficiently to would enable credible replication. However, I am not in a position to assess whether the data is sufficiently accessible to allow easy replication.
4b. Consistency
The numbers in the paper make sense and they appear to be internally consistent throughout the paper.
4c. Useful building blocks
Data and results generated by the authors are likely to enable and enhance future work and meta-analysis.
5. Engaging with real-world, impact quantification; practice, realism, and relevance
As noted in the Overall assessment, the paper engages with a very significant real-world environmental health problem, and the relevance of its arguments, results and policy implications are clearly presented.
6. Relevance to Global Priorities
The topic, approach, and discussion of the application of PES to emissions for agricultural fires in India is certainly useful to global priorities research and interventions related to the fields of PES, environmental","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",85,80,90,,,,80,75,65,85,80,90,90,80,100,80,70,90,90,90,100,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4,4,,,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",,,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,,"80-90
Ed. note: These were left as confidence intervals but the range did not correspond to our journal tier scale, so they were moved here.","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Money (Not) to Burn: Payments for Ecosystem Services to Reduce Crop Residue Burning,https://unjournal.pubpub.org/pub/jackevalsum,American Economic Review: Insights,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-27T15:10:25.726-04:00,
Money (Not) to Burn: Payments for Ecosystem Services to Reduce Crop Residue Burning,https://www.nber.org/papers/w30690,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Karthik Tadepalli,,,"This paper presents a randomized evaluation of a new intervention to target air pollution: paying farmers in North India not to burn crop stubble. Air pollution is a tremendous public health concern in India, and stubble burning (in order to quickly clear the field for winter planting) is one of the biggest contributors to PM pollution in North India. Both bans on stubble burning and subsidizing alternatives have failed to gain traction, and so this paper aims to test a new intervention to see whether it can address the problem. Farmers were promised a payment for ecosystem services (PES) contract in which they would be paid if they could verify that they did not burn their crop stubble before winter planting. There were two broad treatments: standard PES, in which the payment was entirely conditional, and upfront PES, in which farmers who agreed to the contract were paid part of the contract value as an unconditional upfront payment. The authors found that the upfront PES contract reduced stubble burning by 10 percentage points, whereas standard PES had no effect. They calculate that upfront PES saves a life for $3,000-$4,400, which is much lower than the mortality cost of crop burning and much cheaper than other pollution abatement opportunities.
Assessment
This paper identifies a politically feasible and cost-effective solution to a major public health problem. Libraries could be filled with studies showing negative effects of air pollution, but there are far fewer studies that propose solutions that could actually be implemented in developing countries today, and test them with an RCT. This aspect alone makes the paper an amazing contribution to our knowledge.
Moreover, while this goes beyond the scope of the paper and into speculative territory, the upfront PES contract they describe has a lot of potential at scale. First, if it was implemented at scale and had a history of reliable payments, it would increase trust among farmers that they would actually be paid for not burning their stubble. Second, having the carrot of PES payments for not burning stubble would make the stick of punishing stubble burning more credible; it’s impossible to punish the 90% of farmers who burn their stubble, but if that percentage was 78% or even lower, it would be more feasible. Finally, scaling this intervention could increase learning about alternative crop residue management (CRM) techniques and make farmers more likely to adopt them. As a bonus, it does not require legal enforcement capacity and thus could be arranged by a nonprofit as well as by the government.
Furthermore, the results are far from a black box—the authors have a lot of data on farmer heterogeneity that can be used to get close to understanding their results. Understanding how farmers respond to regulations and incentives is important in many settings beyond stubble burning, and the authors’ data allows us to get closer to understanding farmer decision making. Even specific to stubble burning, the authors are able to focus on the difference between in-situ and ex-situ CRM techniques and thus show that ex-situ CRM is the way to go for policymakers.
In short, this paper cleanly identifies an attractive policy that solves a big problem—nothing in the next section takes away from that.
Suggestions
There are a few ways in which the paper could be improved:
The paper does not address a puzzle; why do experienced farmers have negative beliefs about alternative CRM?
75% of study-enrolled farmers have tried non-burning CRM techniques (Table A.1), and they show negative beliefs about those techniques (Table 1), but the study finds no effect of the program on winter cropping time or yields. If alternative CRM has no negative effects, why do experienced farmers have negative beliefs about it?
It’s possible that the intent-to-treat shown in Table A.8 underestimates the negative effects on yield because of the large number of never-takers (since 78% of the upfront PES group also burned their stubble). One important check would be to use randomization into the upfront PES group as an instrument for using alternative CRM, and thus estimate the effect of alternative CRM on winter yields and delays. It’s possible that this approach would find a significant negative effect of alternative CRM and thus validate farmers’ negative beliefs about CRM.
Another possibility is that the farmers who complied with the treatment (in the IV sense, not in the sense that they followed the contract) selected into compliance because of idiosyncratic shocks to their kharif harvest time. Farmers who saw their kharif harvest come early would have a lower cost of doing alternative CRM (less risk of delays), and that could have induced them to comply with upfront PES. This should be measurable - did farmers who requested checks of their fields (and thus didn’t burn) do so earlier than farmers who burned their stubble (according to either spot checks or machine learning)? This could be used to estimate a selection model similar to Suri (2011) to understand the importance of heterogeneous costs to farmers choosing whether to comply with upfront PES or not.
The paper could do more to identify the mechanism behind why upfront PES works when standard PES fails.
One way that the authors could extract more information from the existing sample is to use the kappa-weighting procedure of Abadie (2003) to compare the mean characteristics of compliers and never-takers. (Here, I mean compliers in the IV sense, not farmers who followed the contract.) In particular, kappa-weighting could be used to answer some questions that illustrate why upfront PES works:
Do compliers have less negative beliefs about alternative CRM than never-takers?
Are compliers more likely to report that cash constraints play a role in their CRM decisions than never-takers?
Are compliers more likely to trust that they will be repaid than never-takers?
Understanding the mechanism behind these effects is first-order, not just because they help us make sense of the results, but because they can improve targeting and cost-effectiveness of an upfront PES contract. Targeting the payments at farmers who are most likely to be marginal minimizes the payments made to farmers who will eventually burn their stubble anyway.
The cost-effectiveness calculations could include climate co-benefits from reduced stubble burning.
Venkatramanan et al (2021) estimate that stubble burning of rice, wheat and sugarcane across India caused 9.1 million tons of CO2-equivalent emissions (CO2e) in 2017, based on methane and nitrous oxide release.
With the authors’ estimate that 8.8 million acres of farmland are burned each year, this means that burning one acre is responsible for 1.034 tons of CO2e.
Since upfront PES results in an acre unburned for $34-$51, this means that upfront PES averts a ton of CO2e for $34-$51, which is below nearly all estimates of social cost of carbon.
Thus, upfront PES has benefits that exceed costs even without considering air pollution. It is worth including these estimates as part of the cost-effectiveness calculation.
The paper does not account for machine learning prediction error in its inference.
The main outcome of interest, crop burning, is mostly not measured directly but rather is the prediction of a machine learning model. While it is common practice to treat machine learning predictions as perfectly measured, it is not a good practice and can severely bias the results. Critically, this is non-classical measurement error. Although economists are used to thinking about classical measurement error in the dependent variable as being innocuous, prediction error in this case is non-classical in the sense explained below.
This particular kind of error causes problems for both point estimates and inference. The first issue is that it causes attenuation bias in the estimates: many plots are randomly misclassified, so the treatment will mechanically be less correlated with any particular classification. (See these notes, page 11 for more details.) This would make the authors' estimates conservative, which is not a problem for their main conclusion - though it is still desirable to fix, and would show their treatment to be even more cost-effective.
However, the second issue is a problem for their conclusions. The conventional standard errors they use are too small to capture the true uncertainty in the treatment effect estimates, because they fail to take into account prediction errors by the machine learning model. This could potentially cause them to over-reject the null of no treatment effect. (Although, the net effect of this and the attenuation bias above is ambiguous.)
As a simple intuition check, imagine that the machine learning model simply classified plots at random. Then the treatment effect estimate would be uncertain not just because of sampling randomness, but because of the randomness in which plots were classified as burned or non-burned (which obviously affects the estimate). However, the standard errors would not take into account the second type of randomness and would thus be too small. 
The model in this paper obviously does better than random chance, but there is nothing special about that example; as long as the model's accuracy is not 100%, there is uncertainty over which plots were classified accurately. The standard errors do not recognize this kind of uncertainty - they effectively assume that the model's accuracy is 100%, and thus artificially shrink variance in the treatment effect estimates.
This issue can be fixed in different ways. If the authors don't care about the attenuation bias (which cannot reverse their conclusions) and only want to fix the inference issues, the simplest approach is a bootstrap, using different subsamples of the ground truth burning data to train different models and using the distribution of treatment effects to construct standard errors. But my non-expert reading of the statistics literature is that the efficient approach is the one constructed by Chakrabortty and Cai (2017), which will improve both estimation and inference.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",90,75,95,,,,80,95,70,80,70,85,90,88,92,80,75,95,95,90,100,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",5,4,2,5,3,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",Two years,Zero,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,"Robustness checks and testing mechanisms needed, but possible for the authors to do with the data they have.",,,“Replicable” in the normal sense doesn’t apply to a working paper whose underlying data is not yet public (but probably will be after publication). ,"The authors mention that this project is in collaboration with the Punjab government; I think it would be great for the world if they worked more with the government or nonprofit funders to try and scale up this intervention.
Relevance to GP: 80, 95, 100","I don’t know how much the peer review process values the best attributes of this paper (political feasibility, scalability of intervention, cost-effectiveness).
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Money (Not) to Burn: Payments for Ecosystem Services to Reduce Crop Residue Burning,https://unjournal.pubpub.org/pub/jackevalsum,American Economic Review: Insights,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-27T14:42:58.795-04:00,
Accelerating Vaccine Innovation for Emerging Infectious Diseases via Parallel Discovery,https://www.nber.org/system/files/working_papers/w30126/w30126.pdf,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Tim Colbourn,,"This paper tackles an important global priority area. The main limitations are the premise being restricted to private sector investment, and parameter values used. A societal perspective including (up to 100%) publicly owned efforts, and different reasonable parameter value assumptions, including optimising number of vaccine candidates in relation to expected outbreaks and infections, would alter the conclusions. Calculating cost per QALY gained, and in relation to current healthcare expenditure, would further strengthen the paper.","My comments are focused on the premise of the paper, and the assumptions and parameter values used – it seems likely that different reasonable assumptions would alter the conclusions of the paper. In this regard the paper would benefit from a societal perspective and an expanded sensitivity analysis, which is explained and reported more clearly in the paper.
The premise of the paper is that “the goal is to create a sustainable business model for addressing EIDs effectively”. If the paper was focused on a broader goal of “addressing EIDs effectively”, i.e., looking beyond private sector business models and also including public sector (government funded) provision (including hypothetical 100% public sector efforts with no private sector involvement, i.e., those driven purely by missions to address EIDs effectively), it might very well come to different conclusions as cost parameter values could be very different including there being no need to produce a profit for private businesses.
Related to the above, an analysis of “market failure” to produce vaccines for EID would be useful. This could follow from Section IIA, which already touches on these issues, and would likely point to the need for public sector efforts, including 100% publicly owned efforts oriented to addressing EIDs and securing human health rather than narrowly to making financial profits.
Following from the above if Net Present Value (NPV) was calculated from the perspective of society (e.g. valuing health gains based on willingness to pay for QALYs or opportunity costs of existing government health expenditure in terms of cost paid per QALY gained – this is around £12,000 in the UK), rather than narrowly from the private sector investors perspective, the results and conclusion would be completely different, and likely highly recommend the vaccines are created and trialled, especially if there is some portfolio optimisation at the outset (see points 5 and 6 on this below).
Page 7: “number of EID outbreaks prevented” – this seems to assume vaccines will prevent an outbreak rather than mitigate the impact of the outbreak? No details are provided on this point, or the “social impact” in general and how it is calculated.
End of page 7, beginning of page 8: “we do not.. .. perform any portfolio optimization” – as the authors acknowledge in the previous paragraph, this (as done for CEPI) is likely to be beneficial, so should really have been done in this paper as another option they look at – one that would likely have more positive results. The authors only crudely look at portfolio size in their sensitivity analysis and find a large effect – why not optimise the portfolio? This could at least be easily done in relation to the expected benefits shown Table 1 (see below point).
Table 1 – how did you decide Nvac for each disease? For example, why are there 23 candidates for West Nile Virus, which has an expected annual probability of outbreak of 10% and average number of infections of 500, whereas there are only 7 candidates for Lassa even though the annual probability of outbreak is 100% and the expected number of infections is 300,000. Changing these would likely have a large impact on your results.
Table 2 narrative: Alpha-tech of 1.2, a 20% increase in PoS (probability of success) of mRNA technology is modest, and in the sensitivity analysis you only vary this to 30% and find decreasing rather than increasing NPV likely because you don’t optimise the portfolio - the numbers of vaccine candidates (Nvac) for each disease (see above point on Table 1) could be chosen in relation to the expected benefits. If this was done and an increased PoS tested it’s possible there could be significant NPV.
Bottom of page 12: why assume the vaccine price is $20 in your main analysis, when you recognise this is less than the list price of all adult vaccines in the US so likely to be an underestimate? Why not use the average list price? In the sensitivity analysis you then talk about prices being above $100 in the US sometimes, and that only $78 is needed for your results to show positive NPV. So this would alter your conclusions, yet in the abstract you don’t mention this sensitivity analysis, or indeed any of your sensitivity analyses, alter your conclusions. The point about low-and middle income countries not being able to afford it seems out of place with the premise of your paper requiring private sector profits.
End of part IV, page 14: “social impact” – how is this defined? No details or justification are given in the methods section.
Results – “the vaccine megafund will require $9.5billion funding from the public sector to generate positive financial value for investors” – this may be a price worth paying for human health security, though going back to points 1-3 above, if the system is wholly publicly owned, the concerns of private investors don’t need to be taken into account, especially considering the public sector could pay for the vaccines ($20 per dose) anyway.
Results: “Using even the most conservative “quality adjusted life year” estimate (e.g., Neumann, Cohen, and Weinstein, 2014), the lives saved and socioeconomic losses avoided by the vaccines far exceed the negative financial value of the megafund.” – It would be good to show estimates of QALYs gained, and methods used to calculate them, and factor them into the overall results e.g. using cost-per-QALY willingness to pay estimates (assuming public healthcare provision) or opportunity cost per QALY gained of current healthcare expenditure (see point 3 above). This would likely make a convincing case for (public) investment in the vaccine megafund i.e. would result in an opposite conclusion of the paper to the current one.
Table 4 footnote – an annual discount rate of 10% is very high. Typically 3-5% is used in health economic evaluations: see Table 1 of https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5999124/. [undefined]
Sensitivity analysis B: “However, due to cannibalization between vaccines targeting the same EID and the stochastic nature of EID outbreaks, the ultimate revenues increase by a much smaller amount than the investment.” - this result seems to be heavily dependent on the assumptions about cannibalisation, which are not explained in the paper. These should be explained, and varied. Also, if the premise of the paper included a wholly publicly funded system (see points 1-3 above) then the problem of cannibalisation could disappear.

Sensitivity analysis D: what is the assumed pHCT and what is it based on? This is not shown or explained in the paper, though from the appendix (Table S1) it’s apparent HCT is only increased to 30%. If pHCT approaches 100% surely the results would change and there may be positive NPV? This should be shown.

Discussion – the authors note the utility and reduced costs of adaptive and platform trials. These should ideally be modelled in the paper.

Discussion: “A robust and multi-criteria optimization framework is needed to ensure that their value to society is not compromised by optimizing financial returns for the investors.” – as per earlier comments this should ideally be part of the model in this paper.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",60,50,70,,,,50,60,40,60,50,70,60,50,70,60,50,70,60,50,70,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",3,3,2.5,3.5,2.5,3.5,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",[Range coded for anonymity: 15-20 years],Around 130,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),If all my comments were addressed and the paper completely redone then it would score a lot higher,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,"Relevance to GP: 70, 80, 90 ‘The topic is highly relevant as pandemics and infectious diseases are a top global priority. The premise with the limited focus on private sector investment is somewhat flawed though as per comments 1-3 of my review, as is the assumption of no attempt to optimise of the portfolio of vaccines at the outset (e.g. in relation to expected benefits) as per comments 5-6 of my review.’",This is as it stands. Should be top tier 5.0 (50) if all of my comments are addressed and the paper is completely redone.,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Accelerating Vaccine Innovation for Emerging Infectious Diseases via Parallel Discovery,https://unjournal.pubpub.org/pub/accelvax/release/3,,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-27T12:07:17.104-04:00,
Accelerating Vaccine Innovation for Emerging Infectious Diseases via Parallel Discovery,https://www.nber.org/system/files/working_papers/w30126/w30126.pdf,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Richard Bruns,,"This is an evaluation of ""Accelerating Vaccine Innovation for Emerging Infectious Diseases via Parallel Discovery"". [Extract from the evaluator’s summary follows.] This is an advance on the literature, a promising foundation for future research. In its current form I do not find it convincing as a model of the future of vaccine development, and I am very skeptical of its repeated claims that the portfolio will have a 66% chance of preventing a major ‘Disease X’ pandemic, because the paper does not provide enough information about how it simulates the development of these vaccines for a previously unknown pathogen. However, it provides some insight in its current form (it shows that challenge trials, while potentially helpful, are not sufficient to solve the problem), and there are several minor extensions that could make it much more useful.","Summary Review
This is an advance on the literature, a promising foundation for future research. In its current form I do not find it convincing as a model of the future of vaccine development, and I am very skeptical of its repeated claims that the portfolio will have a 66% chance of preventing a major ‘Disease X’ pandemic, because the paper does not provide enough information about how it simulates the development of these vaccines for a previously unknown pathogen.
However, it provides some insight in its current form (it shows that challenge trials, while potentially helpful, are not sufficient to solve the problem), and there are several minor extensions that could make it much more useful. In particular, Table S1 should be promoted to the main body of the paper, discussed more, and expanded. This table tells us what is more or less likely to make a vaccine portfolio happen, and should be expanded to include more policy choices, such as reforms to decrease the cost of trials.
Specific points:
Modeling Doses
The doses per outbreak should be a variable, ideally one different for each disease, rather than fixed at 10 million. For each known emerging infectious disease, there is a probability that it will become a major global pandemic, but also a probability that it will be smaller or can be contained. The following should be researched, possibly via expert elicitation:
For each disease, what is the 90% CI of the number of vaccine doses that would be required to contain the outbreak with a ring vaccination approach? For example, the 2018 Kiva Ebola outbreak was contained with about 100,000 vaccines.
What is the probability that containment fails and the disease becomes a global pandemic?
Much, perhaps most, of the returns are driven by the ‘Disease X’ vaccine sales. The paper does not fully explain how this is modeled, but from what I can see, I suspect that it is wrong to include it in the portfolio. At minimum, it should include a sensitivity analysis where the Disease X vaccines are removed from the results.
Modeling Disease X
It seems from Page 13 that the Disease X vaccine is not developed to completion of phase 2 in the absence of an outbreak. However, the paper never states how and when the vaccine is developed, and how it relates to the rest of the portfolio. I suspect that there is a special logic for an accelerated trial after the Disease X outbreak starts. But if this is the case, then Disease X should not be included in the portfolio, because it is a completely separate program and research track.
A better modeling approach would be to use a viral families approach, as described by CSET and CHS:
https://cset.georgetown.edu/publication/viral-families-and-disease-x-a-framework-for-u-s-pandemic-preparedness-policy/ [undefined]
https://centerforhealthsecurity.org/sites/default/files/2022-12/180510-pandemic-pathogens-report.pdf [undefined]
Disease X is likely to come from one of a small number of vaccine families. If there is an approved vaccine for another virus in that family, it will be easier to develop one for Disease X. For example, if Disease X is a paramyxovirus, and a Nipah vaccine has been approved, that Nipah vaccine might be effective against Disease X, and it will be easier to develop and produce a new vaccine specifically for Disease X.
The simulation framework for Disease X should be:
If Disease X happens, randomly choose which viral family it comes from.
If there is no vaccine approved or in the pipeline for any disease in that family, there is no cash flow from the portfolio, because any development will be de novo.
If there is a vaccine in development, or approved, for a disease in that family, an expedited development and trial can begin. The development of the mpox vaccine can be used as a case study for this.
There is a probability that the existing vaccine can be used immediately for Disease X (likely with a lower effectiveness, but still providing cash flow and social value).
The number of vaccines sold is variable. Some Disease X outbreaks may be large, but others may be small, like mpox. (Expand the definition and probability of ‘Disease X’ to account for this larger definition, such that there is still a 1% chance of a major outbreak.)
Modeling Costs
The cost per dose is key to profitability, but [this is] modeled with little rigor. In the primary specification, they assume a fixed $20 per dose based on very little information. They could easily extend the model to market-based pricing, where a company could charge more if it has a monopoly and less if there is competition. The simulation should be upgraded so that if only one vaccine candidate has been successfully researched, the price is higher. (Even so, Table S1 shows that even with a price of $100 per dose, the returns are not high enough to compensate investors for the risk in the absence of other reforms.)
Presentation and Discussion
The main takeaway from this paper is that we need regulatory reforms to make vaccine clinical trials cheaper. However, the paper authors have very little discussion of this, and tend to treat clinical trial costs as a law of nature rather than a policy choice.
The ‘Average Number of Infections’ column in Table 1 is extraneous and misleading, having no impact on the simulation. They always assume that 10 million doses will be sold for all vaccines.
The bin size on Figure 3 is much too large, and the histograms reveal very little.
Extensions
There are many virus families that are candidates for a Disease X, such as picornaviruses, that are not associated with the Emerging Infectious Diseases described in this paper. However, they are associated with seasonal respiratory viruses, and there may be a market for them. For example, many schoolteachers might like to take an annual rhinovirus vaccine. The model should include them.
Vaccines in these virus families will, if successful, generate some annual cash flow, while also having a chance of being useful against Disease X.
The cost per clinical trial should be a variable input, and Table S1 should show profitability with lower trial costs. While there may be some reforms that simply improve efficiency, other reforms may generate tradeoffs in the form of slightly lower safety and efficacy, and the costs and benefits of these expedited and cheaper trials can be modeled. As a first step, however, this simulation can easily show how much cheaper clinical trials would have to be for the portfolio to be profitable.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",40,20,60,,,,50,60,30,35,10,50,30,10,50,50,30,70,60,50,80,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",2,2,1,3,1,4,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",[Range coded to preserve anonymity: 10-15 years],Several dozen,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Accelerating Vaccine Innovation for Emerging Infectious Diseases via Parallel Discovery,https://unjournal.pubpub.org/pub/accelvax/release/3,,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-27T11:57:29.393-04:00,
Irrigation Strengthens Climate Resilience: Long-term Evidence from Mali using Satellites and Surveys,https://academic.oup.com/pnasnexus/article/3/2/pgae022/7604271,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",,Adam Smith,"This was a very interesting paper with evident care taken to use novel data sources for careful analysis of irrigation and climate resilience. This paper uses detailed data on the exact boundaries of irrigated sites in Northern Mali combined with remotely-sensed data on farm production, water availability,  vegetation condition, precipitation, and temperature to estimate the impact of these small-scale, river-based, irrigation investments on agricultural output.A particularly novel aspect of this paper is that it estimates the relationship between irrigation and agricultural outcomes in a conflict zone, where traditional data gathering is likely to be a challenge. The authors use Two Way Fixed Effects to estimate these results, following recent work on heterogeneous treatment effects in TWFE. 
The authors demonstrate that Normalized Deviation Water Index (NDWI), which is interpreted as water available to crops, significantly increases after irrigation is introduced, with no evidence of a significant pre-trend. Similarly, the authors find a large increase in Normalized Difference Vegetation Index (a measure of plant green-ness or vegation health) during the rainy seasons after irrigation was introduced. This is consistent with results of qualitative work suggesting rice yields increased significantly. This is also complemented by increased nutrition from DHS for children living near the irrigation projects. There are some negative effects on child nutrition and agricultural employment more than 4km away from the project. Irrigation also appears to be related to conflict, with sites experiencing significantly less conflict events immediately at the perimeter, with a slight increase in conflict further away.
",,"Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",75,69,85,70,60,75,75,85,70,80,70,90,75,70,80,80,70,90,75,65,85,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",3.6,3.5,3.2,4.2,2.9,4,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",8 years.,10+,True,,"Process was clear, template was helpful.","Yes I would, it is a good project and a few changes could make it stronger.",,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.","The authors argue that small-scale, river-based, irrigation investments in Northern Mali significantly increase agricultural output. ",,"I strongly believe that irrigation did increase agricultural output, for those areas where irrigation was placed.","What would make this claim stronger is a robustness check to understand how much of this agricultural output is being moved from surrounding areas into the new irrigation scheme to understand the effects on the treated area overall. I discuss at greater length in the main evaluation, but I would like to more carefully quantify the effect on total agricultural output in an area with a larger buffer around the irrigation system itself.","For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.","For the “Claims” rating, I think the results could be closer to the claim that this paper addresses climate “resilience”. The claim that these results represent resilience would be stronger if linked to climate shocks rather than overall agricultural productivity.","The methods used here are reasonable and appropriate, though the literature on Two Way Fixed Effects is rapidly changing and other more recently-introduced methods might also be considered here.","This paper provides new and interesting knowledge, and the application of remote sensing to generate data during a period of conflict is a clear advance where other traditional field data methods would not have been feasible. This is a notable advance to knowledge with clear practical implications.",,Methods are clearly explained and links to data sources are precise.,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",Unjournal_BenYishayEtAl.tex,The implication here is that investments in small-scale irrigation would increase agricultural output even in conflict-prone areas.,True,"Yes, it would be interesting to discuss further with people who may have different areas of expertise than my own.",,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
","Development economics, with some work on irrigation.",Irrigation Strengthens Climate Resilience: Long-term Evidence from Mali using Satellites and Surveys,https://unjournal.pubpub.org/pub/evalsumirrigationresilience/,,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-16T10:26:34.486-04:00,
Misperceptions and Demand for Democracy under Authoritarianism,https://www.nber.org/papers/w33018,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Korhan Kocak,Korhan Kocak,"Why authoritarian leaders continue to receive support despite openly agitating against democratic institutions is one of the most important puzzles of the last few decades. This paper argues that the answer is partly lack of knowledge about the costs of authoritarianism. Using an online and a field experiment, the authors show that informational treatments that remedy this lack of knowledge can have substantial effects on both beliefs and voting behavior. ","Turkey, Russia, and Hungary experienced substantial falls in various democracy indices as their leaders, Erdogan, Putin, and Orban, dismantled democratic institutions over the last three decades. Still, they receive strong support from large swaths of their respective publics, handily winning mostly free — though unfair — elections. They are by no means alone. Much of the democratic world is experiencing a backlash against democratic, majoritarian, and inclusive institutions. For academics, this presents a frustrating puzzle: despite robust evidence in favor of democracy — not only purely as a system of governance, but the economic growth it and its bedfellows bring — why would voters support parties that want to dismantle it? 

This paper tests a simple, albeit understudied, answer to this question: not everyone has read Why Nations Fail. Using an online and a field experiment, the authors show that a simple informational treatment that informs voters about the state of democracy and press freedom in Turkey and their importance in good governance can have dramatic effects on votes — expressed intentions for the online experiment and neighborhood level vote shares for the field experiment. “Dramatic” here, if anything, is an understatement: the estimated change is a 4.4 percent increase in the vote share of the opposition. Neither the turnout intentions in the online experiment nor the actual turnout shows significant changes: not a surprise given the high turnout rates in Turkey. The authors show that the largest changes happened for respondents and neighborhoods with the largest misconceptions (in favor of the government), which they interpret as evidence that the effect is via persuading government supporters that maybe the costs of keeping Erdogan in power is larger than they think. They also find that the effectiveness of the treatment depends on the reliability of the message (e.g. citations, referring to a scientific consensus) as well as the affiliation of the canvasser (i.e. partisan or non-partisan), but not so much on the complexity of the message. 

This is an excellent paper that asks an important question, offers a reasonable answer, and provides credible evidence in its favor. The work done, especially for the field experiment, is impressive. The analyses are well-done and very accessible. I have a few concerns and suggestions, but none of them are major.

I would like the authors to elaborate more on the ethics of the experiments. The estimated effects are massive, large enough ex ante to change the outcome (although ex post we know they didn’t). Currently, the only discussion is that the authors reached out to all major parties (but only the opposition responded, which is not surprising given the content) and that the opposition parties were planning to canvas anyway and the authors just helped improve the message. I think a more robust discussion of the studies’ ethics is warranted. There is an extensive literature in both political science and economics that focuses on the ethics of experiments, in particular regarding election interference. The paper should refer to such studies and justify its interventions. That the present intervention is purely informational is insufficient as a justification: as scientists we tend to agree with the scientific method and the ensuing consensus — others may describe the treatments in this experiment as propaganda. Consider the flip-side of this experiment: AKP canvassers can cite information from a pro-government think tank that finds press freedom in Turkey has improved dramatically to improve their vote shares. Would we then have a “hopeful interpretation—accurate and high-quality information can bridge gaps between ideologically divided communities of voters” (p. 25)?

My second point is methodological. Both experiments use as their outcome variables the post-treatment observations: beliefs or voting intentions in the online experiment and vote shares in the field experiment. Both experiments also have some imbalance. I am concerned about the ex ante differences in the outcome variables between the control and the treatment groups. This concern is exacerbated by what the authors write on page 19, subsection 4.6: “the magnitudes of the informational treatment become somewhat smaller when we control for the pre-treatment values of the dependent variables on the right-hand side of the regression.” Why is not the specification where the pre-treatment values are controlled for the main specification? This is done in the field experiment, but not the online experiment. Even better, why don’t the authors use the difference between post- and pre-treatment values instead of only the post-treatment values. This would be straightforward for the online experiment; I concede that I see no obvious way of doing this for the field experiment.

Third, and related to the above, I would like the authors to calculate, if possible, the persuasion rates of their treatment (DellaVigna and Kaplan, 2007). A number of papers in this literature calculate this quantity and it would be very helpful in comparing the findings here to other contexts and interventions.

Minor, but I don’t know what “reached tertiary education” (p. 17) means. Please clarify whether this refers to any post-secondary enrollment, degree completion, or another threshold.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",85,70,95,90,80,94,75,90,60,90,75,95,90,80,95,30,15,50,90,80,95,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4.1,4.3,3.4,4.4,3,4.7,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",10+ years,50+,True,4 hours,,Sure,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",Voters’ support for authoritarian regimes comes in part from their lack of knowledge about the costs of authoritarianism. Informational interventions that remedy this can have a substantial impact on vote shares (to the tune of 2.4 pp).,,I strongly believe it.,"The outcome measure should not be post-treatment means but the difference between pre-treatment and post-treatment. Otherwise, I think there are sufficient robustness checks. ","For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,"Presumably, the authors will make replication code and data available once they publish this paper.",,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,The most important implication of the paper’s findings concerns canvassing efforts of opposition parties is semi-authoritarian regimes: small changes in their messaging (i.e. talking about scientific consensus on the importance of press freedom and other democratic institutions) can dramatically improve their effectiveness. This would come at little to no additional cost.,False,Sure,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
","Political economy, media and politics",Misperceptions and Demand for Democracy under Authoritarianism,,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-13T04:29:54.578-04:00,4
"When do ""Nudges"" Increase Welfare?",https://www.nber.org/papers/w30740,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",,,,,"Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",,,,,,,,,,,,,,,,,,,,,,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",,,,,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",,,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,When do “Nudges” Increase Welfare?,https://unjournal.pubpub.org/pub/welfarenudgesevalsum,American Economic Review,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-12T12:29:36.026-04:00,
"When do ""Nudges"" Increase Welfare?",https://www.nber.org/papers/w30740,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",,,"The present paper reports on nudges failing to improve utilitarian welfare because they increase higher-order choice distortions. While the paper is highly complex in practically all regards, their general idea is important for public policy. The welfare losses implied by nudges may not be large, and real-life policymaking emphatically does not proceed by maximizing utilitarian welfare, but this paper could represent a step forward in how we think about policy incidence over the whole distribution of behaviors.
","This is a review of ‘When Do “Nudges” Increase Welfare?’ by Allcott et al. (September 20, 2023 version). I review this version over the NBER version because it is more recent.
Summary
The authors show theoretically how nudges change prices in equilibrium and how their economic effects differ from those of taxes. Even when nudges change average behavior in beneficial ways, they may increase the variance of distortions, decreasing welfare. They use two artefactual incentivized experiments to demonstrate these theoretical findings.
Comments
This paper appears to be a very substantial contribution to the study of nudges and similar behavioral interventions. While the theoretical analysis hinges on the assumption that taxes are optimal, there is of course a large literature on the heterogeneous effects of policy. To pick a random example, Manning, Blumberg, and Moulton (1995)[undefined] show how different utilizers of alcohol react differently to increases in alcohol taxes. That policy has heterogeneous effects is by itself not a novel insight, but it is nonetheless important.
The most significant finding in this paper is in section 1.4, equation 6. That nudges can influence prices in equilibrium is a result of high practical relevance. Previous work may have implicitly assumed nudges to be “too small.” Section 1.4 would deserve a more central place in the paper. Policymakers simply cannot know (Hayek 1945)[undefined] what the boundaries of interventions are, even when their effects are thought to be local.
The presentation of the paper is slightly difficult. In the whole paper, highly technical and complicated language is used. But simple is better than complex, and complex is better than complicated. An example is “The remaining bars present conditional ATEs within the sample of participant-by-product pair observations with below-median or above-median bias or externality estimates” (p. 22). In other cases, the paper uses unknown terms such as “highly statistically insignificant” (sic, emphasis mine, p. 22). The introduction is already quite technical; section 1, in my opinion, is appropriate. Section 2 is almost unintelligible. This is because the experimental design is never clearly stated. Even though the “cars” and “drinks” experiments share many features (as hinted at in section 2 after the heading), readers require clarity to understand the multiple elicitations and the exact procedure that subjects underwent. The understanding of the “cars” experiments is complicated by the unceasing use of brand and product names that are not necessary for understanding. Section 3 is highly technical; many econometric aspects could be moved to the appendix.
Once the experiments are understood, they certainly make sense. However, I do have issues with how subjects would perceive certain aspects of them. Most striking is the WTP elicitation for car leases if gas is free. This seems difficult to make sense of intuitively. Because this setup is so unusual, the experiment will inevitably elicit numeracy, and any elicitation also be affected by cognitive uncertainty (Enke and Graeber 2019)[undefined]. Bias is only identified under the strong assumption that such considerations are not relevant. While recent work has demonstrated that demand effects may not matter much or at all in experimental designs (De Quidt, Haushofer, and Roth 2018)[undefined], they may still be significant here due to the multiple elicitation.
In general, I have to praise the transparency the researchers use when making their assumptions. Even though sections 3 and 4 are highly technical—and contain some of the densest writing I have ever encountered—I believe the authors take assumptions and models seriously. On the one hand, the models described in section 3 are clearly in reduced form. On the other hand, the authors make substantial assumptions to identify otherwise structural parameters such as \text{Var}\lbrack\tau\rbrack. This positions their econometric model somewhere between “reduced form” and “structural” approaches; it also means that this paper uses a “model” approach, where many objects of interest are not directly observable through the “design,” but only implicitly contained.
It should be clarified that some of the effects seem quite small, especially for cars. The abstract already says that “labels […] may decrease welfare.” The authors effectively use figures to communicate their ideas, but they should seek to find plausible numerical configurations that allow them to make clearer statements about the effects of nudges in their examples. It is important to note that this paper represents a “proof of concept”—and that is a true strength of this work. It is still important to provide readers with tangible, real-life ideas about the effect of some sample interventions.
In general, I do not believe it to be appropriate to drop observations without preregistration. (At the time of review, a full and transparent preregistration was not publicly available.) This point becomes even clearer when the authors write that “excluding these outlying responses is conservative” (p. 15). I obviously understand the wish for precision in estimation, but if it doesn’t matter, then don’t do it. The preregistration should always be followed. If there is no preregistration, then no data should be dropped.
In general, if a difference is “not statistically significant,” then no conclusion should be drawn as if it were. The manuscript (p. 22) contains several instances of suggestive phrases that frame the experimental results in terms favorable to the authors’ interpretation. That is not permissible. However, I am not sure whether section 3.2.2 is necessary. Its many caveats paired with overconfident statements certainly make for a confusing read. The entire paper could benefit from clarifications and simplifications.
Global priorities
How does this paper help us deal with global priorities, such as statecraft, governance and democracy? It certainly highlights the challenges of policymaking. While the paper itself represents a “proof of concept” that nudges can diminish welfare through unexpected channels, we have to ask ourselves what this implies. On the one hand, optimal policies are shown to be difficult (and the paper’s application of optimal tax theory should not be taken as a prediction that taxes always work optimally).
However, it was recognized early by the public finance literature that policymakers can insert their own views into policymaking, even if such insertion is justified on the presumption of asymmetric information (e.g., the case of “merit goods:” Musgrave 1959, 1956)[undefined][undefined]. Furthermore, Peltzman (1976)[undefined] considered a situation in which policymakers work to improve their own lot. Recent research by Ambuehl, Bernheim, and Ockenfels (2021)[undefined] extends to paternalism, with paternalists imposing their own preference. To that extent, the present paper does not necessarily provide a description of policymaking processes as they currently exist; and because the authors (understandably) miss the true motivations for governance and intervention, this paper is unlikely to improve what is globally important. Their demonstration is still significant, as it shows how optimality is easily missed even with well-intentioned, utilitarian policymakers.
Open science and replicability
The authors provide a full replication package. Furthermore, their experimental design—once fully understood—is easy to replicate materially. The authors are transparent about their assumptions vis-á-vis econometrics (despite the substantial complexity of their analyses). In this regard, the authors made laudable efforts.
Conclusion
The present paper reports on nudges failing to improve utilitarian welfare because they increase higher-order choice distortions. This is a highly original contribution. While the paper is highly complex in practically all regards, their general idea is a curious addition to what we know about public policy. The welfare losses implied by nudges may not be large, and real-life policymaking emphatically does not proceed by maximizing utilitarian welfare, but this paper could represent a step forward in how we think about policy incidence over the whole distribution of behaviors.
Further discussion in response to management notes/questionsundefined
Managers’ notes: We normally provide evaluators with a set of ‘bespoke evaluation notes’ in advance. We neglected to share these in advance, but the evaluator graciously responded to these afterwards. We excerpt these below, leaving out those questions which the evaluator already addressed above.
Managers: Is a model in which government adjusts taxes/subsidies optimally a relevant one for this discussion? …
Evaluator: This article is embedded in a recent literature that tries to compare the relative efficacy of taxes vis-á-vis alternative policy measures. One of the earliest works that clearly articulates this line of research is Loewenstein & Chater (2017, reference in original paper), which has been followed up by various theoretical and empirical investigations. To properly compare the performance of alternative policy measures (such as nudges), it is necessary and proper to fix the alternative that nudges are compared against at the optimal level. Of course, taxes may (and do) reduce welfare; they are emphatically not neutral. But this literature seeks to “steelman” taxes and ascertain whether nudges hold up to the theoretical optimum achievable with taxes.
Managers: …Don’t we think of nudges usually exactly in places where we have reason to believe behavior can have an impact, and where we see no other easy way to steer people in the right direction, e.g., as adequate taxes are politically infeasible?
Evaluator: I do not see it that way. Governments everywhere have many tools at their disposal; nudging is a recognition that there may be additional “knobs” available. The selling point of nudges is that they have minimal cost yet still are effective. If high taxes on sugary drinks or cars are politically infeasible, then nudges compare favorably, but that is only one possible alternative. Nudging may simply be used to steer a different kind of consumer. In fact, the recognition of the heterogenous effects of policy tools is one of the major contribution by the authors.
Managers: “We apply our framework to randomized experiments evaluating automotive fuel economy labels and sugary drink health labels ...” These experiments were run on a particular pool of participants, asking questions about particular contexts. Are the inferences made from these participants in this context meaningful?…
(If so) do the results in this context meaningfully generalize in ways that will help inform (globally) important real-world policies? 
Evaluator: See the main review. The experiment is necessarily highly stylized and artefactual. Insofar as the authors attempt to verify theoretical predictions, the experiment is conceptually sound. However, as stated in the main review, the design is quite complex and the presentation could be much improved.
Neither the experiment nor the theoretical exhibition can help with global priorities because governments are by their nature ignorant of welfare; and welfare is indeed not what is optimized.undefined
Managers: There were some real incentives to choose according to one’s true preferences (in the drinks case?) or to get state WTP ‘correctly’ as imputed from their earlier survey responses (in the autos case).  Were these incentives strong, meaningful, and correctly aligned?
Evaluator: The incentives have a typical size for experiments, but they are not as “high stakes” as real-world decisions. Conceptually, the idea of rewarding participants according to their consistency is sound, but the complexity of the experimental design may inhibit rational responses in the first place. Issues with MPLs [multiple price lists] are well-known. However, such a skew would be systematic and independent of treatment. Sadly, the authors did not present any discussion as to this inherent cognitive load imposed by their design.
Managers: [Please consider issues surrounding ] the statistical modeling of the results, statistical inferences, structure of any structural (?) model used to derive welfare implications; consistency with pre-registration/pre-analysis plan
Evaluator: The most important issue I see is that the authors never give an example for plausible parameter values. To gauge the importance of welfare losses of nudging, we need to know some values. Indeed, the example that they give hints at tiny losses (section 4.4). See the main review.
The statistical model is quite sophisticated and indeed almost of a structural nature. This makes it susceptible to modelling choices and “researcher degrees of freedom,” but we cannot know to what extent. I was not able to find a meaningful preregistration or preanalysis plan. This must count against the authors, as do the significant exclusions that they applied seemingly without reason (see section 2.1, “ Excluding these outlying responses is conservative”). If sample sizes and the exclusions were preregistered (and perhaps even the analyses committed to via a preanalysis plan), I will happily withdraw this point. Nonetheless, all assumptions are transparently made and a replication package is provided.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",85,70,95,,,,80,90,50,15,10,20,85,75,90,95,90,100,,,,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",,,,,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",,,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),"The theoretical analysis has very high quality. The experiments could be improved so that they are clearer to subjects. The econometric analysis is state-of-the art, albeit highly complex and thus potentially less robust.","Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,"The methods appear to be appropriate; the experiment is simple enough, albeit with some issues. I am not an econometrician and thus unable to judge the intricacies of the highly complex mixed effects model or their implementation. However, the general approach appears legitimate.","Since this project does not consider real-life policymaking, its impact for global priorities is severely limited. Nonetheless, they highlight challenges even under utilitarianism as a motivation of policymakers.","Theory-wise, everything is clear to me. Regarding the analyses, tables and figures certainly help, and all concepts are generally clear enough. Nonetheless, all aspects of this project are highly complex and the paper could benefit from more clarity. But this does not invalidate the logic. Regarding statistical analyses: it may be that there is a better way to do them, but I do not know of such a way.",,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,When do “Nudges” Increase Welfare?,https://unjournal.pubpub.org/pub/welfarenudgesevalsum,American Economic Review,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-12T12:29:32.156-04:00,
How Much Would Reducing Lead Exposure Improve Children’s Learning in the Developing World?,https://www.cgdev.org/publication/how-much-would-reducing-lead-exposure-improve-childrens-learning-developing-world,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Evaluator 2,,"This non-systematic review seeks to explore the relationship between lead exposure and children’s learning outcomes by updating existing meta-analyses. Its key strengths are 1) consideration of standardized test scores for reading and mathematics 2) use of a specification curve, and different ways to assess the impacts of publication bias. Critically, a quality assessment is missing and some control choices are unmotivated. This evidence captured in the review itself is likely not causal, though the authors do examine a broader literature on the cognitive implications of lead exposure that certainly goes beyond correlational associations.","This non-systematic review seeks to explore the relationship between lead exposure and children’s learning outcomes by updating existing meta-analyses. Its key strengths are 1) consideration of standardized test scores for reading and mathematics 2) use of a specification curve, and different ways to assess the impacts of publication bias. Critically, a quality assessment is missing and some control choices are unmotivated. This evidence captured in the review itself is likely not causal, though the authors do examine a broader literature on the cognitive implications of lead exposure that certainly goes beyond correlational associations.
Strengths
I view the inclusion of a specification curve very positively. This method allows authors to examine whether their conclusions are sensitive to changes in analytical approach, such as subsampling or use of controls. In a way, this allows for researchers to report their own robustness replication alongside their main ‘conventional’ result (e.g. meta-analytic pooled effect). It might be useful for readers to contextualize this with current wide-scale efforts in economics to mass reproduce research in top journals, which showed that 70% of effects remained significant after a robustness replication, and about half diminished in size (Brodeur et al., 2024)[1]. In other words, the inclusion of this specification curve should offer us confidence that we are looking at robustly negative and significant impacts of lead, albeit small in magnitude.
The authors have not discussed their results in this manner, but since lead exposure can be due to macro-level policies or trends, and as such populations will be exposed to lead based on these, it may be appropriate to consider these population-level effects; as such even small effects are very important for police (Rose, 2001)[2]
I am very supportive of the consideration of measures beyond IQ. IQ has been criticized as a challenging measure for young children and in some, particularly non-Western, cultural contexts. Standardized test scores for reading and mathematics are a nice addition with added applied utility, i.e. they are directly relevant for education.
Publication bias - strong approach, especially when reporting multiple ways to account for publication bias. As is typical for such efforts, the overall pooled effect does moderately decrease. (Though I do wish this corrected-for-publication-bias effect (based on either method) would be provided in numeric terms in the report.)
Critical points
Note I see this paper as a non-systematic review and so consider it to be unfair to assess it against the standards of systematic reviews, e.g. consideration of systematic databases rather than Google Scholar’s, reporting standards such as inter-rater reliability measures - see PRISMA reporting standards for me). At the same time, the non-systematic approach does mean studies could have been missed or selected in a biased manner. One way to have improved on this without an extreme time commitment is to have done a systematic search for systematic reviews in a couple of targeted databases (i.e. using ‘systematic review’ as a search term). There are benefits of searching for new studies systematically as well.
The most serious concern I have is that there is no formal quality assessment (risk of bias) screening. Without this, we have no sense of whether or not this meta-analysis perpetuates the ‘trash in - trash out’ problem (see Egger et al. 2001)[3]. By using potentially biased data, erroneous conclusions or mis-estimation can be perpetuated and solidified. This may or may not be the case here but there is no way to tell. For instance, we have no sense of study population, recruitment strategy, attrition, power justification, robustness or validity of measures used etc.
Inclusion criteria: studies that “have blood lead level measures from the same individuals, whether contemporaneously or from different points in time.” - I assume if different time points, it would still be required that blood lead level is measured before IQ/ reading/ math, though I cannot find whether this was the specific inclusion criterion
Extraction: “We extract any coefficient relating maternal or child blood lead levels…” there is no further mention of maternal lead beyond p.6, does this mean no studies used maternal measures or whether lead at the maternal or child level was treated the same? If the latter, I would be concerned and would prefer such a choice to better and more transparently motivated.
“We exclude results which include blood measures for multiple ages in a model separately, as this has a different estimand: the effect of exposure at a particular age, relative to exposure at another age.” - this sounds strange to me, given that likely the first available age would be comparable to all other included studies where there is bone lead measures and cognition measured fitting the inclusion criteria for time of measurement (“contemporaneously or from different points in time”)
I note in passing that from the 47s studies, 29 are in high income countries, which might suggest need for more data in LMICs - cannot make this as a strong claim, since this is not a systematic review.
In passing as well, some figures need better labeling, e.g. include effect type directly in label.
Specification curve - minor; perhaps could have been done in a more principled way - e.g. controls reported in stepwise manner (first each on their own, then combined), a subsampling based on country more direct comparison of high-income vs LMICs
The second most serious concern I see is around the reporting standards. Particularly, the choice of controls is discussed very briefly and is not theoretically motivated for the reader. For instance, SES could be not only a confounder but could modify the neurotoxic effects of lead exposure. Inappropriate modeling re: SES/ other controls can lead to underestimates of the effects of lead or missattribution to the wrong risk factor (See Bellinger 2008[4] for more on this specifically or Judea Pearl on causality).
Overall, taking these concerns above - lack of quality assessment, the lack of motivation for controls, and the more minor questions about timepoint inclusions - I do not personally take the evidence base captured in the review as causal. Most of the data is observational as well.
At the same time, the discussion in Section 4 (‘Assessing the role of unobserved confounders’, starting p.21) is strong and helpful in focusing particularly on what kinds of work exist more broadly on the cognitive impacts of lead, and how much different studies can speak to causality (e.g. animal studies, natural experiments).
Additional note (from correspondence)
Evaluation manager
… What exactly [did you find] non-systematic about the review?…
Evaluator:
The authors searched only in Google Scholar - this is not a systematic search database but simply a search engine, and I think it's deeply misleading to use the terminology of 'systematic review' in such cases.
While Google Scholar can be used effectively as a supplementary addition to systematic databases, it itself is not systematic and cannot be searched systematically. There are further differences between systematic databases and search engines like GS- search engines  such as GS are non-replicable, results almost always vary between machines given the algorithms GS uses, you do not reliably know where GS pulls results from, there is no controlled vocabulary (eg MESH terms), you cannot be specific enough within GS and  so likely receive pages and pages of results and thus have to screen up to an arbitrary 'stopping point', whereas typically in systematic reviews all results are pulled. For instance, when I try to replicate the authors’ search strategy in section 2.1 the output I get from GS is ""About 9,330,000 results"" so it's very difficult for me to understand how the authors report that they ""found 951 potential results"" without applying some arbitrary stopping rule.
[Evaluation manager: We also ran this search and got many more results than the authors reported.]","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",75,68,82,,,,68,78,58,70,65,75,75,70,80,80,70,90,70,65,75,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",3,3.2,2.8,3.2,2.8,4.2,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",,,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,"Wide CIs - I view very positively the reporting of a specification curve, the different approaches to assessing publication bias, the consideration of different interventional approaches. At the same time, the lack of quality appraisal and the lack of motivation for controls are concerning.","Consideration of reading and maths is useful and goes beyond standard of IQ measures, which have previously been critiqued for both young children and in some non-western cultural contexts",,"Data and code are not shared, so effort would have to be expanded to replicate this, but enough data are reported in text for this to happen","Limited novelty is what detracts from my score, as the negative effects of lead on cognition are well established and without a better sense of the quality of data meta-analyzed, it is difficult for me to support any other significant contributions

Relevance to GP: 70, 80, 90",,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,How Much Would Reducing Lead Exposure Improve Children’s Learning in the Developing World?,https://unjournal.pubpub.org/pub/evalsumleadexposure,The World Bank Research Observer,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-12T12:19:44.331-04:00,
How Much Would Reducing Lead Exposure Improve Children’s Learning in the Developing World?,https://www.cgdev.org/publication/how-much-would-reducing-lead-exposure-improve-childrens-learning-developing-world,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Evaluator 1,,"Strengths include excellent explanations of how the literature was harmonized for comparison and how various analytical methods were considered. Authors make compelling cases for both. The major limitations are around the actual systematic review methodology that is employed as well as the causal conclusions drawn. Authors have not used published best-practices in the conduct of a systematic review, and have not noted this as a limitation of the work. It will be critical to add methodological limitations. There is also a lot of causal language used that cannot be supported by the type of data presented (correlational), thus the suggestion is to focus on the associations or relationships between the IVs and DVsundefined, rather than speaking about 'effects'.","This review aims to examine the relationship between blood lead levels and learning (including IQ as well as reading and mathematics). They find that reducing blood lead levels by one natural log unit increases cognitive outcomes by 0.12 standard deviations. Authors do a great job of explaining how data was harmonized to allow for comparison across studies, and in walking the reader through their analytical choices. They also do a nice job of situating their work in the context of other previous reviews. However, gold standard methodologies for conducting a systematic review are not utilized, and the potential biases that this introduces into the results are not acknowledged. There are also causal claims that cannot be supported by the type of data used in this case.
[Managers’ note: we recognize that the paper also considers instrumental-variables estimates, which have a case for a causal interpretation under a particular set of conditions.]
Major comments
Results are given as one natural log unit increase in blood lead is associated with X. It would be helpful to know, on average, what the differences are in average blood lead levels for children in LICs vis a vis MICs or HICs earlier in the intro (before you start getting into the results).
Phrases like ‘causal link’, ‘causal interpretation’ and ‘effect of lead exposure’ do not seem appropriate given the study designs. In other places, more appropriate phrasing like ‘association’ and ‘relationship’ are used. Phrasing should be tempered throughout the manuscript to reflect that this is not causal evidence. Despite arguing the causal link later in the manuscript, the data itself cannot make this claim, particularly when there are so many comorbidities to lead exposure that are also related to lower cognitive achievement (e.g., SES).
Authors need to acknowledge the search is not comprehensive. They state that they seek to identify “all studies measuring the correlation between blood levels and IQ or test scores”, but then report that only existing SRs and Google Scholar were searched.
Please provide details on whether there were exclusions based on publication year or language.
On page 7, authors discuss that they include 286 estimates from 47 unique studies. It would be useful to know the minimum and maximum number of estimates contributed by a single study.
In Figures 3 and 4, please add the number of estimates being averaged for each study. It would also be useful to include the actual weighted average effect of the RVE model when referring to it in the Figure note. Finally, in Figure 4 there is a test of group differences presented (whether blood levels had a different effect of reading versus math?). It is unclear why this test was included or why such a difference would be expected. There is also no discussion of the test of group differences in text.
There needs to be a robust paragraph about the limitations of the systematic review methodology employed here. Gold standard SR methodology requires duplicate independent screening at full text to reduce bias. It also requires that risk of bias assessments are conducted in duplicate for each included study, and that results are presented with reference to potential biases in the underlying literature. It should also acknowledge whether limitations were put on the publication year or language of the studies, and should acknowledge that the search was not comprehensive (e.g., no grey literature sources were mentioned).
Figure A3 should include the study citations. It is very surprising that Taylor (2017)[1] would weaken the effect since it is a non-significant study without a particularly high weight. It would great if the authors could confirm this is indeed the correct study.
A PRISMA flow chart should be included to illustrate each phase of the search process as well as the reasons for exclusion at full text.
It would be useful to have additional information in A1 and A2, e.g., sample size, age at lead testing, age at cognitive testing, what was the measure used for cognitive testing (for reading and math).
Minor comments
In the introduction, the second sentence of the first paragraph does not follow logically from the first. You state that 600 million children in L&MICs have elevated blood lead levels, and then you state that 3% of this comes from HIC (which are not even mentioned in the first sentence). Do you mean that 3% comes from middle-income countries?
I am not sure who the target audience is, but when sigma is first used in the text (on page one of the intro) it may be useful to explain that this is the symbol for standard deviation.
On p. 31, in the first full sentence, please add the unit of measurement for the 149 and 15 numbers.
[Note: Copy-editing suggestions were removed here; these are useful to the authors, but not to other readers.]","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",70,55,74,,,,60,82,44,80,65,85,66,58,75,61,52,73,80,73,85,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",2.5,3,2,3,2,3.5,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",[20-25] years in research and [about 15] years in systematic reviews and meta-analysis. [Range coded to preserve anonymity.],Approximately 50,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),"This is a topical area of critical importance, but some of the methodology used in the production of the review is likely to introduce some level of bias into the results.","Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,"Methods related to the actual conduct of the review are underexplained and do not appear to follow best practices. However, their justifications for their specific analytical choices as well as their explanation of how data were harmonized to allow for them to be combined in a meta-analysis were very well thought out and well described.","I think that once the methodological limitations are acknowledged and the causal language is tamped down, this work can be appropriately caveated and contribute to our understanding of the relationships between blood lead levels and cognitive outcomes.",Here some work needs to be done,"Sufficient information is given for someone to attempt to replicate the search, but I did not see any links to open access data.","This is outside of my topic area as I am primarily a review methodologist, however the policy relevance as well as the real-world relevance for children and families around the world is clear.

Relevance to GP: 77, 83, 87
Yes, this is a major global concern and should be of wide interest.","It would be of interest to a field journal, but would be unlikely to be published in a top journal due to the methodological limitation of the review process.","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,How Much Would Reducing Lead Exposure Improve Children’s Learning in the Developing World?,https://unjournal.pubpub.org/pub/evalsumleadexposure,The World Bank Research Observer,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-12T12:11:38.702-04:00,
Economic vs. Epidemiological Approaches to Measuring the Human Capital Impacts of Infectious Disease Elimination,,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Evaluator 2,,"This paper presents an interesting comparison of two methods for estimating the impact of measles on long-term health and economic outcomes: an epidemiological model to estimate variation in measles infection across cohorts and, as more standard in economics, a reduced-form model that uses variation in pre-vaccination measles mortality across place. It is a fascinating and important study that highlights the strengths and weaknesses of the two approaches. However, pushing a bit further on the assumptions and measurement issues associated with each approach and having a fuller explanation of the differences in the empirical results would make for an even stronger contribution.","This well-written paper provides an interesting comparison of two different approaches to estimating the long-term health, education, and income impacts of measles in the United States. Both approaches center around using variation in measles exposure generated by the rollout of the measles vaccine in 1963. The first approach is epidemiology-based, using state-year data on measles infections along with vaccination rates to estimate the share-ever-infected of each cohort and then using this as the independent variable of interest. The second approach, which the authors rightly identify as the main approach within the economics literature, uses variation in pre-vaccine measles severity, proxied by measles mortality, interacted with a dummy for the pre-vaccination period, as the key independent variable. Note that, following the authors, I will refer to this second approach as the reduced-form approach.
When using the reduced-form approach, the authors find significant long-term impacts of measles in the expected ways: exposure to a higher measles mortality environment was associated with lower levels of high school completion, lower income, higher levels of unemployment, and higher levels of welfare income and food stamp usage. The epidemiological approach finds noticeably different impacts, with higher shares of measles infection showing a statistically insignificant relationship with later life outcomes. Additionally, these estimates appear far more sensitive to model specification than the reduced-form results.
The paper presents compelling evidence that epidemiological and reduced-form approaches provide different perspectives on the long-term impacts of infectious disease elimination. However, to properly interpret those differences, I think the paper needs to dig a bit deeper into the impacts of the different assumptions built into the two approaches and offer more guidance as to how to interpret differences in the results. I elaborate on these points in the comments below.
[Cases vs. deaths]
One significant focus of the paper when interpreting results, and in my view one of the significant theoretical differences between taking epidemiological versus reduced-form approaches, is the difference between focusing on cases versus deaths. It is understandable that most reduced-form studies will inevitably rely on mortality statistics because the data are more readily available and more accurately measured. The underlying assumption of most of these reduced-form papers is that a worse disease environment in terms of mortality is likely highly correlated with a worse disease environment for those who survive. The paper would benefit from offering the reader more guidance about how likely this assumption is to hold both in terms of measles and more broadly.
Figure 4 (a really helpful figure for understanding the three different measures of measles exposure) suggests that infection rates and mortality rates do not necessarily move in lockstep with one another. Note how Washington and Arkansas completely flip in terms of level of measles exposure when switching from share infected to pre-vaccine mortality. The reader needs more context for how a deadly outbreak of measles differs from a less lethal outbreak and how those differences likely translate into different patterns of human capital formation. This seems essential to both understanding the empirical results but also to understanding the value of the two different approaches to measurement and when we should favor one over the other
[Cohort trend vs cohort fixed effects: why are the ‘epi’ models so sensitive to this?]
2. While the point above is primarily about the real-world aspects of more and less lethal measles outbreaks, the empirical results also make me wonder if [there] is a very different but important issue related to the econometrics of the epidemiological model versus the reduced-form model. I think both models and their assumptions are both well-motivated and well-explained in the current manuscript. However, the main results suggest that there is something missing on the econometrics side. In particular, it was surprising to see how sensitive the epidemiological estimates were to the choice of controlling for either cohort trends or cohort fixed effects. The reduced-form results were pretty much unchanged by this choice but the epidemiological results not only changed substantially but typically flipped signs. This is noted in the paper but never explained. In my view, it is essential to figure out what is going on here. What I suspect is that the method for calculating cohort exposure to measles by estimating the number of susceptible individuals in the cohort each successive year is generating a correlation by construction with the cohort trends that might be throwing off results through multicollinearity issues.
[Doubts about cohort trends in epi model]
I haven’t thought that through fully but I think the paper needs to grapple with whether the inclusion of cohort trends is compatible with the way cohort exposure is calculated.
[Puzzling ‘sign flips’ between results of epi and reduced-form models]
While the econometrics of that remain a bit out of reach for me, it is far clearer to me that the cohort fixed effects results are both easier to interpret and seem more defensible. That being the case, the paper needs to do more to explain the opposite signs on the long-term effects results for the epidemiological results compared to the reduced-form results. All of the reasoning throughout the paper relates to different magnitudes of effects, not different signs. Coming up with an explanation for those sign flips is important for establishing the credibility of the epidemiological approach (note that the power of the approach to predict future outbreaks is really convincing evidence of the credibility of the approach, evidence that I think could be emphasized even more, but explaining the sign flips still seems essential as well). The only explanation for sign flips between infections and deaths that immediately comes to mind is if there is some sort of culling effect going on, leaving a healthier but smaller population in the wake of severe measles outbreaks.
[Measurement issues for both approaches]
3. There were a few places where I would like to see a bit more about measurement issues for both measures and whether those issues are likely to generate attenuation biases in the results. For the reduced-form approach, how accurately does stated cause of death capture measles deaths? Are deaths from related complications like pneumonia included? If not, do we have a sense of how much measles deaths are being underreported? If so, do we have a sense of how much they might be overreported?
For the epidemiological approach, could you offer a bit more to get a handle on the variation in both reporting rates and immunization rates? For the reporting rates, I find the approach of allowing them to vary over time the most reasonable and would recommend that it be the main approach rather than relegated to the appendix. I’d also be curious as to how variation in reporting rates correlates with other relevant state statistics (from the map, it appears that higher reporting rates are potentially positively correlated with income and with school attendance, pretty relevant correlations). For the immunization rates, I understand the need to use national estimates given the lack of state-level data but we know that in recent years these rates vary substantially across states. From the October 18, 2019 Morbidity and Mortality Weekly Report, the national average for MMR vaccination rate is 94.7% yet some states like some states like Idaho are below 90%. That would lead to some substantial measurement error in the measles exposure estimates. It would be useful to track down earlier data on variation in immunization rates across states or to at least point to sources that document a divergence in immunization rates being a more modern phenomenon.
[Compare to Atwood (2022) more closely to unpack the source of the differences]
4. While this paper highlights the differences in measuring infection versus mortality by comparing the epidemiological model results to the reduced-form results, and does so with solid discussions, it seems equally important to dive deeper into the differences in the reduced-form results in this paper and those in Atwood (2022)[1]. As noted in this paper, the Atwood results aren’t exactly the same as they focus on measles cases rather than deaths. Given this, should we expect the Atwood results to more closely track the epidemiological results of this paper? Or is it really the methodology (epi versus reduced-form) that makes the biggest difference as opposed to the cases versus mortality, meaning the Atwood results should more closely align with this paper’s reduced-form results? Doing more to address these questions would both help distinguish this paper’s contribution from Atwood but also provide a lot more clarity about the broad goal of showing how the measurement of infectious disease impacts estimates of its long-term impacts.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",80,65,90,,,,77,83,70,76,64,85,91,81,100,96,92,100,95,90,100,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",3.3,3.3,2.6,4.1,2.6,4.1,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",[Range-coded to preserve anonymity: 15-20 years],roughly 75,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,"Relevance to GP: 77, 86, 95",,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Economic vs. Epidemiological Approaches to Measuring the Human Capital Impacts of Infectious Disease Elimination,https://unjournal.pubpub.org/pub/evalsumdiseaseelimination,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-11T12:38:20.444-04:00,
Economic vs. Epidemiological Approaches to Measuring the Human Capital Impacts of Infectious Disease Elimination,,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Evaluator 1,,"The following abstract was generated by the evaluation manager, with the help of a chatbot. Positives: (1) Novel epidemiological method for imputing historical infection rates. (2) Interesting comparison of epidemiological vs economic approaches. Limitations/suggestions: (3) Conclusion favoring the economic approach needs stronger support. (4) Concerned about identifying assumptions for ‘sharp cohort design’ — would like to see tests for ‘no cohort pre-trends’. (5) Needs more discussion of conceptual differences between mortality and infection rates. (6) Could elaborate more on applicability to other diseases.",,"Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",,,,,,,,,,,,,,,,,,,,,,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",,,,,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",,,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Economic vs. Epidemiological Approaches to Measuring the Human Capital Impacts of Infectious Disease Elimination,https://unjournal.pubpub.org/pub/evalsumdiseaseelimination,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-11T12:35:48.488-04:00,
Effects of Emigration on Rural Labor Markets,,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Frank Gyimah Sackey,,"We prioritized the paper “Effects of Emigration on Rural Labor Markets”[undefined] because of its potential relevance to the decision to shut down the charity “No Lean Season”. Although we normally seek two or more strong evaluations, we struggled to find evaluators for this paper. We would have persisted further, but we were also advised that other work in this area seems more relevant (see discussion below). Thus we decided to release this package with only a single evaluation.","Introduction
Whilst the topic is clearly explained, authors may consider replacing the word “effects” with “impact:. An impact is an effect, but not all effects are impacts. Since authors cannot measure all effects and therefore models often developed tend to be parsimonious, using “impact” may be more appropriate in my opinion. Again, the authors, in their summary of income, wage and labour supply used “effects” 5.3, 5.4, and 5.5, and as I have indicated above that an impact is an effect I will advise accordingly.
The authors did a good job of motivating the problem statement and the objectives in the introduction which are reasonable and able to meet the objectives set based on the data available.
Literature
Whilst the [literature] review was broad, it lacked comprehensiveness. [It included] a lot of review of empirical studies, however, the theoretical framework was not adequate. I expected, for example, that the Harris-Todaro theory and model on rural migration [would be] adequately reviewed, not just in passing.
Methodology and Data
The [theoretical] models were clear and formalize the argument given by the author as in the statement of the problem while they relate well with the objectives set. However, the models do not incorporate clearly all the aspects of reality which are very important to our audience.
Though treatment effects modelling was incorporated, a decomposition and counterfactual model should [also] have been incorporated. The Generalized Blinder-Oaxaca decomposition or the Fairlie decomposition (in the case the outcome variable is discrete) is recommended to deal with this shortfall. Again, variables that cover individual characteristics should have been incorporated in the model. It must be emphasized that the decision for a labor[er] to migrate may not necessarily [only] be influenced by transport subsidy but [also by] other factors including one’s gender, sex, marital status, education, age, etc.undefined
[On the Oaxaca or Fairlie decomposition…] measures the emigration gap between subsidy beneficiaries and those without subsidies. It will also indicate the extent to which the gap is influenced by the subsidy (unexplained factors) differences in endowments (characteristics) among the two of which the absolute values of the explained (characteristics) and unexplained (in this case, the subsidy) will determine which largely influences the emigration decision.
Analysis and discussion of results
The model being estimated is clear and relates with the objectives. Though the author found no usefulness in determining whether the migration experience of those travelling from a high intensity village is more successful than those from a low intensity village between migrants and non-migrants, this could have been observed with migrants alone.
[Clarification] On page 20, authors indicate “ITT estimates report average effects combining migrants and non-migrants, and are therefore not useful for determining whether the migration experience of those traveling from high density villages is more successful than that of migrants from low density villages.” Though this is not part of the objectives such analysis could have been observed with migrants alone.
The author also could have examined differences in individual endowments among the migrants and non migrants as the willingness to migrate may be determined by individual characteristics and not the transport subsidy alone. Again, comparison between those who are offered subsidies and those who were not should have included decomposition and counterfactual analysis as well as measuring the wage gap among them to critically examine the push factors apart from using the transport subsidy as a sole factor.
… what if other factors, other than the subsidies, or including the subsidies could be the driving force of emigration?","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",70,50,90,,,,70,90,50,70,50,90,70,50,90,70,50,90,70,50,90,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",2.5,2.5,1,3,1,3,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",14 years,40,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),"The paper sought to examine the effects of emigration on rural labor markets, specifically, how outmigration transforms rural labor markets through the subsidized transport cost as an intervention mechanism in its experimental approach. Using a sample of 5,792 potential seasonal migrants across 133 villages, authors observed that transport subsidies increased beneficiaries' income that generate spill-over effects. The paper contributes immensely to the research on migration and migration and labor studies.","Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,"Though the author found no usefulness in determining whether the migration experience of those travelling from high intensity village is more successful than those from low intensity village between migrants and non-migrants, this could have been observed with migrants alone.",The paper provides practical insights on interventions and adds useful value to research on interventions.,"The author also could have examined differences in individual endowments among the migrants and non-migrants as the willingness to migrate may be determined by individual characteristics and not the transport subsidy alone. Again, comparison between those who are offered subsidies and those who were not should have included decomposition and counterfactual analysis as well as measuring the wage gap among them to critically examine the push factors apart from using the transport subsidy as a sole factor.","The sources of the data are clear, while the analyses are well explained with clear labelling. Replicating the analysis is possible, however, the modeling and the analysis is so broad that one might lose focus, in relation to the objectives.","The paper is very relevant to the real-world situation while its policy implications are consistent and implementable. Though the set-up assumptions are valid, the literature review leading to the assumptions should have had the Harris-Todaro theory and model on rural migration was adequately reviewed with its assumptions to add more insights to the set-up assumptions. Again, the analysis, especially, regarding the result tables were too many that the average audience my lose focus. Though some of the tables have been placed as an appendix there is the need to streamline and summarize the models and the tables in a brief but in a parsimonious manner in order to lose important results.

Relevance to GP: 50, 70, 90 
Though the topic is clearly explained, useful to the global audience and has high impact interventions, the author may consider using the word ""Impact"" instead of ""Effects.""",Top B-journal/Strong field journal,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Effects of Emigration on Rural Labor Markets,https://unjournal.pubpub.org/pub/evalsumemigrationonlabor,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-11T12:12:26.685-04:00,
Willful Ignorance and Moral Behavior,https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3938994,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Romain Espinosa,,"This study is an outstanding work that will become a major reference in the empirical literature about information avoidance and meat consumption. First, the authors make here a highly valuable contribution to the economic discipline and, in particular, to the research on dietary transitions by determining the causal impact of information provision on individuals conditional on their a priori willingness to get informed. This is an important research point both for economics in general (where the question of information avoidance has been discussed intensively over the past five years) and for dietary transitions, where researchers and society struggle to induce dietary changes (changes which are needed in light of the environmental, health, and ethical issues arising from large scale meat consumption in developed countries). Second, the authors came up with a lab & field design with real consumption choices, which has been relatively rare in the empirical literature on dietary changes. Following individuals outside of the lab is a key element [for considering] issues like displacement or long-term effects. In addition, looking at effective dietary choices is central, as the attitude-behavior gap in nutrition is a major limiting factor for this research literature. Here, the authors also show that the effect of watching the video on effective consumption choices is short-lived. Third, on the estimation side, the authors propose a neat empirical strategy to elicit the conditional impact (addressing the issue of self-selection), discuss the potential problems at length (e.g., displacement of meat consumption), clearly expose the deviations from their pre-analysis plan, and carefully thought about numerous design issues (e.g., the alternative video). Overall, I am very supportive of this work.","This work addresses the question of information avoidance related to meat consumption. More precisely, it analyzes to which extent individuals reluctant to get informed about farmed animals’ rearing conditions change their meat consumption when they are effectively exposed to the information they seek to avoid. The authors investigate this question by offering participants in a lab experiment (N=330) the possibility to watch a video showing the rearing conditions of intensively farmed pigs and eliciting their reservation price to accept watching the video. Using a multiple-price list design with random price selection, they identify the causal effect of watching the video for different reservation prices. The authors analyze the conditional treatment effect for both in-lab meal choices (voucher) and out-of-the-lab meal choices (university canteens). They provide evidence suggesting that information avoiders are more likely to change their meat consumption after exposure to the video than information seekers.
Feedback
While I find the work extremely well done, I report here some comments that arose while reading the manuscript. I hope that some of these comments can help the authors in improving their document or can bring some ideas for their future work.
Major commentsundefined
[Experimental design: interpretation of WTA/WTP elicitation]
My first major comment concerns one of the core elements of the paper, namely the willingness-to-pay (WTP) for watching the video. At first, I did not understand that the prices were relative, i.e., they were opportunity costs of watching the video. The authors write that they are ‘relative prices’ in the body of the manuscript, but it became clear what they mean by that only after looking at the instructions.
While this is not a concern for the validity of the paper, I have two behavioral concerns about the wording here. First, I think that it is important to underline in the main body of the manuscript that the prices are opportunity costs. I would suspect participants to give different answers if they had to pay EUR 8 to avoid watching the video about animal farming compared to the current situation where they would not gain EUR 8 if they decided to watch the alternative video. Actively paying money to avoid watching a video is a more active behavior. It relates to the status quo and loss aversion. The initial endowment would also matter here. The experimental design captures information avoidance as a more passive phenomenon than what the readers could understand from the current wording.
Second, I am not sure that what the authors measure is a WTP. Here, the authors offer participants to receive some amount of money if they agree to watch a video. It is much more like compensating people for doing a task than asking them to actively pay to watch the video. Thus, I feel that this is closer to a willingness-to-accept (WTA) than a WTP. I think that it is of particular importance in the case of information acquisition where there are different behaviors at stake: actively looking for information, passively accepting the information, actively avoiding information (and possibly passively avoiding information, if this makes sense).
Again, these questions do not affect the results of the paper, but I see them as important to understand what is measured and what we learn from it.
[Belief elicitation — design/methods, interpretation]
My second major comment relates to the beliefs and belief updating. First of all, we can note that the belief items are not as precisely elicited as the behaviors of the participants (which is fine given that the focus is on behaviors). However, I would suggest being more careful when discussing the beliefs given this. First, the beliefs are stated by the participants. The authors cite one of my [papers] (Espinosa & Stoop, 2021).[undefined] As we show in this work, there are significant differences between incentivized and non-incentivized reported beliefs in the case of meat consumption. If people engage in motivated reasoning somehow after watching the video to limit cognitive dissonance, if the video makes the belief question more salient, if on the contrary cognitive dissonance becomes impossible after watching the video, the authors might misestimate the impact on beliefs of the treatment effect.
Second, I also think that the belief items are relatively vague such that it is unclear what conclusions we can draw from it. Note that I did not find the precise wording of the belief questions in the paper nor on the PAP (https://www.socialscienceregistry.org/trials/5015). I think that only evaluating beliefs on a 1-to-5 Likert scale about the ‘pigs’ living conditions’ is not the most efficient design if we really care about understanding what cognitive process happens here and what participants learn from the video. These questions seem to reflect a relatively general evaluation of the pigs’ welfare rather than accurately assessing the knowledge of their living conditions. While I do not see this point as a threat to the validity of the paper, I would have appreciated that the authors mentioned this issue and recognized the possible limits in terms of what we can learn from the results.
Third, I did not understand in the manuscript the effective design regarding the beliefs. On page 18, in the first paragraph, the authors discuss the difference in beliefs between information seekers and information avoiders. They say that the average belief deteriorated with the video and that belief updating is not statistically significant between the two groups. However, the authors said on page 13 that they asked the belief and preference questions before watching the video. So, how did the authors evaluate the change in beliefs? Was this question asked twice (within- subject) or conditional on exposure (between-subject)?
Assuming that the authors evaluated the beliefs twice, I might have some concerns here. One issue is that most of the participants on this question are distributed at the highest level of the Likert scale (about 70% of the participants report the maximum value looking at Figure A5). When assessing a difference between treatment groups or a treatment effect, ceiling effects are important as they can lead to considerably underestimate the difference. I would suggest using here a Tobit model to take this issue into account. (I assume that there is no issue with combining it with inverse probability weighting.) Another related issue concerns the difference in beliefs. The authors write that the difference in beliefs is 0.15 for information avoiders and 0.20 for information seekers (page 18). However, note that the difference is non-negligible (it is about 33% larger). The lack of significance for the difference does not mean that there is no difference (well-known moto: the absence of evidence is not the evidence of absence). This is particularly true in the case of underpowered tests. And as I mentioned above, this is likely to be the case here because of the ceiling effects. If we look at Table A9, we see that the average beliefs are 4.69 for information avoiders and 4.59 for information seekers. It seems that information seekers have more room to update their beliefs on this Likert scale than information avoiders (because of the ceiling effect).
Overall, I would suggest being more careful about the conclusions the authors draw about the beliefs (ex: ‘Hence, differences in baseline beliefs or in belief updating do not explain why some individuals engage in willful ignorance while others seek information.’).
[Decomposing information and emotion effects]
My third major comment is about what drives the change in behavior and what the authors measure. I think that the authors test the overall effect of the video, which could (in theory) be decomposed into two effects: a pure informative effect (people learn about the state of the world) and an emotional/affective effect of the video (people feel negative affects when exposed to the video). Of course, it is very difficult to decompose the overall effect into these two sub-effects. It might also be that such a decomposition is not relevant because the cognitive and affective processes are interconnected. However, the authors provide evidence supporting the idea that avoiders and seekers show different affects associated with the video (see Section 4.2 and Table A13). Given that (i) avoiders and seekers have similar priors, (ii) they have similar posteriors, and (iii) have different affective/emotional reactions, it seems to me that the former are more negative-affect[s] avoiders than information avoiders. So, are the authors evaluating here willful ignorance in the sense people do not want to get information that will change their beliefs or are they measuring emotional protection, i.e., avoiding exposure to information they already know but which they expect to generate negative emotions? To be fully transparent: I think that this question is beyond the current knowledge and discussions in economics and I think that, while the paper might not be able to address it, it might open the path to new research on the topic.
Relatively minor comments
I have two relatively minor comments:
First, the authors state several times in the paper that the canteens ‘typically serve meat from intensive farming’. While it might be correct, it would have been useful to have some numbers here. Ideally, I would have preferred to have the participants’ beliefs on this. In fact, in my second work cited by the authors, [undefined] we find that participants who watched such a video are likely to accuse the intensive farming industry but a large share of them then say that it is not representative of the animal industry in their country. So, an obvious protective mechanism to maintain his/her meat consumption is to say that it does not concern their own consumption. While it is not a concern for the paper (it would imply that the authors underestimate the treatment effect), I think it is an important point.
Second, on a side note, I would like to stress that increasing the costs of information avoidance (discussed at the end of the paper) could also lead to other behavioral issues. For instance, the authors mention the strategy of activists to display information on product packages or approach pedestrians on the street. Increasing the coercion level could lead to backlash effects such as reactance where people feel restricted in their choice and could start criticizing the activists rather than considering the information from the video. This effect could be even stronger for information avoiders. Thus, strategies affecting the information acquisition costs as discussed in Section 5 might induce other and possibly backfiring beha","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",88,83,93,,,,85,90,80,85,80,90,85,80,90,50,50,50,90,85,95,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4.7,4.3,4.4,5,4,4.6,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

","I have been working on animal welfare economics and meat consumption since 2018. So, about 6 years.","I've written 71 referee reports for journals, and a handful of reports for funding agencies, and reviewed about a hundred applications for recruitment committees.",False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,The experimental design is very well done and clearly exposed. The empirical methods are sound and the authors present convincing evidence about the most immediate robustness concerns.,"This work is very informative about the challenges that empirical research on dietary changes faces. It combines several strengths that are seldom jointly present in other empirical works: revealed preferences, lab and field data, and randomization at the individual level. The results will help NGOs and policymakers in supporting/stimulating dietary transitions and will benefit the scientific community (positive relationship between information avoidance and information elasticity).",The paper is written clearly and all arguments are well presented.,"I have not seen any link to the data and codes. I assume that the data are too sensitive to be shared (individual-level consumption data with identifiers). So, this category would not be applicable. (So, I indicated 50.)","Very relevant as such videos are widely spread and viewed in society. The only departure from reality might be the 360° dimension.

Relevance to GP: 100, 100, 100 ‘The topic of the paper (industrial animal farming) is one of the greatest challenges our societies currently face. Congratulations to the authors for addressing such an important topic.’","I fear that some editors in top economic journals are still reluctant to publish works related to animal welfare. So, I think there is a risk of this paper being underpublished.","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Willful Ignorance and Moral Behavior,https://unjournal.pubpub.org/pub/evalsummoralbhvr/,SSRN Electronic Journal,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-11T11:46:07.440-04:00,
Willful Ignorance and Moral Behavior,https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3938994,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Joshua Tasoff,,The paper is professionally written and the experiment is well-designed. The main result – avoiders of a VR animal advocacy message exhibit larger treatment effects on consumption than information seekers – is novel and important. I find the claims about the dynamics of the treatment effect to be less supported by the evidence. It is more speculative.,"The authors conduct an experiment on the effect of a virtual-reality (VR) video depicting the course of a farmed pig’s life, from birth to slaughter. Student subjects have their demand for this video elicited with real stakes in a positive and negative domain. Subjects are then conditionally randomly assigned to either watch the VR or watch a control VR video. There are two main outcome variables. After watching the video subjects choose whether they would like to receive a voucher for a meat meal or a vegetarian meal, redeemable at the campus canteen. Subjects’ meal choices are also observed for the beginning of the academic year to March (onset of [the Covid] pandemic). The analysis period is from two weeks prior [to] three weeks post-intervention. The authors find that the video reduces the preference for the meat voucher by 15.6 [percentage points]. The key result is that this effect is driven by the avoiders, with an effect on them of 34.1pp (relative to 7.7pp ns for seekers).undefined Results are consistent, if less precise, using the consumption data. Finally, the authors present evidence on [the] mechanism. They show that avoidance is uncorrelated with beliefs about animal intelligence or factory farming and preferences for animal welfare or meat. Information shifts beliefs slightly. But it seems that avoiders self-report much more sensitivity to violence and experience more negative affect from the video. The evidence suggests that the intervention operates through an emotion channel.
There is much to like about this paper. I think understanding information preferences in the domain of farmed animal welfare is important because information is readily available and actionable, but there seems to be very little action on the part of consumers and voters. The experiment is well-designed, and the data set is substantial. The main result, that avoiders exhibit larger treatment effects, is consistent with several behavioral economic models, but not “rational”/expected-utility; so this is of basic scientific interest. It is also of policy interest because it suggests that pecuniary incentives may be required for an effective messaging campaign. The authors also push the claim that the field effects only last about one week and then attenuate. This may or may not be true, but I do not find the evidence on attenuation particularly convincing which I discuss below. The paper is already well-polished, and I think ready to be published in a high-profile economics journal.
Comments
[Field results, dynamics, statistical methods, sample window]
Perhaps my biggest concerns are about the field results. I believe that there is an effect in the time window, but I do not understand the choice of this window. Why not use the full sample window? Won’t this have greater power?
a. Why not use a panel regression or a difference-in-difference estimation where the observation is whether a specific meal is meat or not? This is the approach used in Jalil, Tasoff, and Vargas-Bustamante (2020, 2023)[1][undefined].
b. The analysis appears underpowered to estimate any sort of dynamics. How many meals are in the analysis window? How many meals are in the entire data set? It would help to have this reported in Table A.14.
i. If there are on average 3 meals in the two weeks prior to the intervention, that implies about 1.5 meals/week. If there are about 28 weeks in the sample with 261 subjects that implies about 10,000 meals in the whole sample. However, if you’re considering a window five weeks long, that implies a sample of about 2,000 meals. I think it is difficult to estimate dynamics from a sample so small.
1. Indeed, Figs 3 and 4 show that 95% confidence are quite wide, and the immediate post-treatment mean is within the confidence intervals of the mean a few weeks later.
2. Jalil et al (2020)[1] came to the incorrect conclusion from their larger sample (~50,000 meals) that the treatment effects attenuated over time. But they discovered in their 2023 follow-up that there was no attenuation when they expanded the window over three years (~100,000 meals).
ii. What would happen, if rather than using a moving average, which multiply counts the same observations, you estimate treatment effects for specific post-windows: week 1, week 2, week 3 or month 1 vs. month 2?
c. That said, I still think there is value in the field study. I am surprised that there is any significant effect from such a short 5-minute intervention. I think this shows robustness as well as showing the effects persist outside of the lab. It is truly impressive that such a quick intervention can last for weeks. That is my takeaway, I’m less credulous about the claims on the dynamics.
[Belief, emotion and preference questions]
2. The battery of belief, emotion, and preference questions is a valuable addition to the paper. I often wonder how much interventions such as this shift behavior through the belief channel vs. more of an emotional/attentional channel. I think this paper presents very nice evidence on the latter (PANAS and distaste of violence). This stands in contrast to the other studies in this area that seem to focus on information content that is less emotionally-charged: Schwitzgebel, Cokelet, and Singer (2020, 2021)[3][4] and Jalil, Tasoff, and Vargas-Bustamante (2020, 2023)[1][2].
[IPW estimator vs standard approach]
3. As far as design goes, I am not familiar with the IPW [inverse probability weighting] estimator, but I understand that the assignment is random conditional on WTP (but unbalanced). I wonder how well this method performs compared to alternative designs in experimental economics in which assignment is random for a large fraction of subjects and […] a small fraction (5 or 10% usually) are placed in an “incentive compatible group” where they get assignment based on choices and chance. Usually, this IC group is thrown out. I wonder which method is more efficient. Or perhaps the obvious best approach is to use the above method (random assignment) and use the IPW on the IC group onl","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",90,75,96,,,,60,80,20,75,33,92,75,33,92,75,33,92,75,33,92,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4,3.4,3.5,4.3,2.8,4,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",10-15 years,In the vicinity of 100,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,The methods seem reasonable. I'm not an expert on econometrics and the IPW estimator so I was unable to assess this.,I think this is more of a basic science paper combined with treatment effects on what particular intervention.,,,,"Unfortunately, it is not yet clear whether animal-welfare economics has enough buy-in from the profession to be considered a subject of interest. I think there is a greater than average risk that the paper would be under-placed.","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Willful Ignorance and Moral Behavior,https://unjournal.pubpub.org/pub/evalsummoralbhvr/,SSRN Electronic Journal,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-11T11:33:22.317-04:00,
"Ends versus Means: Kantians, Utilitarians, and Moral Decisions",,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",David Hugh-Jones,dhj,"This paper reports an experiment on deontological (ends versus means) preferences using incentivized experiments, including a “trolley problem” where subjects can choose to save real lives. Non-consequentialist preferences are common, but are situation-specific and not linked to conventional social preferences such as altruism. The experiment is well-executed, and results are credible and interesting. Questions remain whether economic lab experiments are the best tool to characterize subjects’ ethical views.",,"Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",70,45,85,75,60,90,70,80,60,65,50,80,65,55,75,70,50,80,50,40,60,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4,4.3,3.5,4.8,2.5,5,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",18 years,About 60,True,1 1/2 days,I know there’s a revised version in the works... it was fine for me,Sure,I’ve written separately about this!,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.","About 25% of people make deontological decisions in a “trolley problem” with real consequences, i.e. they do not intervene to stop the saving of one statistical life (via money donations to a charity), even though doing so would enable the saving of three statistical lives elsewhere.",,"In the exact same experimental setup, but across a broader population e.g. a random sample of Europeans, I’d have a 80% credible interval of 10-40% deontologists. Across a more broader set of real-life means vs. end dilemmas with serious consequences, that would be wider, but I’d give an 80% probability that any given dilemma would have at least 5% of deontologists.","A similar experiment with a random population sample could estimate this parameter in a more interesting population. Ultimately, interaction with political and ethical philosophers might be more important for determining how useful the above number is.","For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",unjournal ends means review.docx,"If you think that public policy should be informed by the electorate’s moral preferences, then this result could be quite important for funding priorities. For example, it might go against using foreign aid as a tool for foreign policy, even when that would maximize global welfare. However, not all philosophers would agree (a) that it should be so informed or (b) that it should be informed in this particular way.",True,Sure,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",experimental economics,"Ends versus Means: Kantians, Utilitarians, and Moral Decisions ",https://unjournal.pubpub.org/pub/evalsumpopintuitions/release/5,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-11T07:29:09.528-04:00,12
Replicability & Generalisability: A Guide to CEA discounts,https://docs.google.com/document/d/1eJBSmNG-iRJ-twoHaoztQUmB4pmEHut8_oT3xgtldnI/edit#heading=h.7om527x8q9t7,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Max Maier,,"The proposal makes an important practical contribution to the question of how to evaluate effect size estimates in RCTS. I also think overall the evaluation steps are plausible and well justified and will lead to a big improvement in comparison to using an unadjusted effect size. However, I am unsure whether they will lead to an improvement over simpler adjustment rules (e.g., dividing the effect size by 2) and see serious potential problems when applying this process in practice, especially related to the treatment of uncertainty.","First, I want to say that I think the proposal is a good starting point on the important topic of how to shrink study estimates in practice and I appreciate the opportunity to review this important work. I think this topic is difficult to tackle in a numerically quantifiable way and the author made a very important contribution by starting this process. Further, I think all points outlined below can be addressed and I would be happy to see how this guideline develops further. 
Major comments 
[1. Uncertainties are compounded at each step] 
My main concern is that in each step of the evaluation, there is quite a lot of uncertainty about what is the correct estimate to derive for the shrinkage in this step. Therefore, using this multi-step process with very uncertain estimates in each step may actually worsen the estimates in comparison to using a simpler rule (e.g., ½ of the primary study effect size). The reason is that at each step of the process, an additional possibility for error or misspecification is introduced. I created a small simulation to illustrate this process. For example, assuming the distribution of true shrinkage factors is beta(2, 2) and the six-step process is on average unbiased (i.e., the same as the true shrinkage), but in each step error from normal(0, 0.2) is added, I find that using ½ leads to better replicability assessment than following the 6 step proposal here (I am attaching the R script with my report). 
Manager note: this script is given at the bottom, and we have also hosted this R script in a Google Collab here
Of course, the simulation depends a lot on the assumption about the distribution of true shrinkage and error and with other assumptions, I would likely find the opposite. I am not proposing that ½ is certainly better. Instead, I am only proposing that it is a serious possibility worthy of consideration that the 6-step process will introduce much more noise to be worthwhile overall. Note also that this could be tested empirically by having Founders Pledge members predict the replicability of already replicated studies using the tool and then comparing their forecasts against ½. The problem discussed here is also amplified by two other major concerns discussed below. 
[2. Adjustments should consider uncertainty] 
All the adjustments only operate on point estimates, and the uncertainty is largely ignored. I think it would be very valuable to take uncertainty into account in this approach. For example, researchers could specify a range of values for the plausible effect size and the different plausible adjustments and then do the other steps (e.g., power analysis) for this range. Then you could use a probabilistic tool such as Squiggle to get to an estimate for the final shrinkage that takes uncertainty into account. The reason why I am emphasizing this is that I think when selecting between multiple donation or investment options (which I assume is ultimately the goal of the cost-effectiveness assessment), a selection only on point estimates can introduce a variety of serious problems, in particular, a selection in favour of more speculative and less effective interventions (see for example here). 
I know that you could argue as a counterpoint that the proposed guidance already shrinks more speculative effects more; however, I think that is the case only for effects that are likely to be null rather than effects that have a wider range of plausible effect sizes, which is what I am referring to here and which is a separate issue. For example, if one effect is likely between 0 and 1 and another likely between 0.4 and 0.6, these two would be treated similarly in the proposed analysis. I would really recommend reading the blog post (if you are not familiar with it already) and thinking about this issue, as this may otherwise introduce big problems in the later point estimate-based selection of effect sizes.
[3. The worked example needs more explanation and detail] 
The worked example, which would be an opportunity to give more specific guidance on how to get from the general guidelines to specific estimates for the amount of shrinkage, is very short, and it is often unclear how the numbers are derived. This is especially important as the theoretical steps in the main part are quite abstract and, therefore, I imagine that the worked example will be crucial for other researchers to actually be able to apply this method in practice. Currently, several things are not explained in enough detail in the worked example; I list most of them below, but I would recommend reworking this in general and adding much more detail.
Step 1: Are those 16 studies that you found […] exactly the same effect or related effects? If it is exactly the same why not use a meta-analytic estimate across all of them? Also, how do you arrive at the 50% shrinkage to those studies? Could there be a problem of an infinite regress, where technically you would need to go through the stepwise process to adjust those studies, which then again requires primary studies to get an initial guess and so on?
Step 2: You refer to Figure 9 to get the adjusted estimate due to the interaction of power and publication bias but it would be good to give more detail on how to read of the values. Further, Figure 9 is between Figure 3 and Figure 4 in the manuscript I would order the Figures so the ascend from 1 to 9. 
How did you estimate that social desirability increases the estimate by 10%? Where does this number come from? Is there any work that can be referred to on how much social desirability would usually inflate effect sizes?
Step 5: If it is likely that you did not sufficiently discount the estimated effect size in the baseline estimate then why did you not use a lower baseline estimate to begin with?
Step 6: Needs more detail on how exactly you arrived at the 5% shrinkage.
It would be good to also have an example for a Type-S adjustment. Maybe for a different study if this one is well-powered already.
Minor comments
 I am wondering whether for the internal validity guidance for researchers step 1 and step 5 essentially constitute a way of updating on the prior twice? The reason is that one first informs once estimate for the power by once prior intuition and then discounts the effect size if the expected effect size (and consequently power) is low and then discounts this already discounted estimate again in step 5. This seems to violate ideas of Bayesian updating, where the same data should not be used twice.  
 I think 2b(ii) in the internal validity guidance for researchers could be more specific. For many people without extensive statistical training in the social sciences these signs may be hard to spot, so I would add at least a short list of some of the common issues (e.g., hypotheses that seem like researchers only came up with them after seeing the data, extensive subgroup analyses without adjustments for multiple comparisons, p-values just below the .05 threshold)
 For 2b(i) I would emphasize to always go back to the preregistration and check that people said what they did. I know it already says “did the researchers stick to this”, but I would really emphasize this, as people deviate from the preregistration without saying anything very often and therefore just noting that there is a preregistration does not really add a lot of robustness without actually checking i.
Further for 2b(i) I would also mention registered reports: if the article is a registered report that would increase my confidence that there is little researcher bias quite a lot, much more than only for a preregistration
For point 3 it took me a moment to realise that those are the example numbers from above rather than some intrinsically meaningful numbers. I think that could be addressed with a simple rewrite.
Calculate the total discount, as 1/ the product of the individual discounts above. So here, this would be 1/ (1.2 * 1.1 * 1.15) = 0.66. This suggests that your total adjustment is 66% x [study estimate of effect size].
 Calculate the total discount, as 1/ the product of the individual discounts. Using the example numbers above (20% inflation due to power, 10% inflation due to XXX …), this would result in an adjusted estimate of 1/ (1.2 * 1.1 * 1.15) = 0.66. This suggests that your total adjustment is 66% x [study estimate of effect size].
The Bartos et al[undefined] paper on comparing meta-analyses is now peer-reviewed. The appendix also includes a matched K-analysis comparing meta-analyses of similar size, which should help with some of the concerns about meta-analysis type. However, I agree that there could be differences between the Cochrane meta-analyses in medicine and psychology and economics that are based on journal articles.
 On p.18 “If the ratio of the standard deviation to the mean is >1, this suggests that the data is noisy.” I am not aware of this guidance or where it may come from, it would be good to provide some reference and explanation.  
  On p.17 “One difficulty of this method is the estimation of an unbiased sample size”
Should this mean unbiased effect size?
 For Figure 4 I would increase the Figure labels.
 Another source of evidence for Table 1 could be comparing registered reports to unadjusted effect sizes in the same field
Kvarven, A., Strømland, E. & Johannesson, M. (2019)[undefined]
 I think while the internal validity is often mostly dependent on statistical issues that can be identified by following the guidance in the document, external validity assessment would usually require much more domain expertise, so it might be good to recommend consulting with an expert in the domain area of the study for the external validity assessment.
 The first time you introduce the power calculation I would probably add a few notes on the problems of post hoc power calculations, just to make completely sure that users are aware of this and are not tempted to use the provided effect size estimate for the power analysis.
 In several places parentheses are missing for equations. For example 1/ 1.2 x 1.2 x 1.05 = 66%, should be 1/ (1.2 x 1.2 x 1.05) = 66%
Despite this mixed evaluation, I want to emphasize again that I think the proposal was a good and important starting point on a topic that is also very difficult to tackle. I think all of the issues raised here are addressable, and I would be very happy to read a revised version in the future.
Additional content (R script for simulation)
#this vector stores the distance between the estimates from the six step process and the true values
dist_process <- c()
#his vector stores the distance between 1/2 and the true values
dist_half <- c()

epsilon <- 0.2 #noise added in each step
n_steps <- 6 #number of steps

for(i in 1:1000){
  #sample true value
  true <- rbeta(1, 2, 2)

  #add noise for each step (this could be rewritten more simply based on sum of normals)
  estimate <- true + rnorm(1, 0, epsilon)
  for(j in 1:(n_steps-1)){
    estimate <- estimate + rnorm(1, 0, epsilon)
  }

  #add bounds between 0 and 1
  if(estimate < 0){
    estimate <- 0
  }
  if(estimate > 1){
    estimate <- 1
  }

  #store the results from this simulation run
  dist_process <- c(dist_process, abs(true - estimate))
  dist_half <- c(dist_half, abs(true - 0.5))
}","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",40,10,60,,,,10,40,0,50,20,70,30,10,50,,,,50,10,80,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",,,,,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",~3 years,~10,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,"When applying the procedure, the conclusion seems very vulnerable to small chances in assumptions about how specific factors (e.g., social desirability) influence effect sizes. Further, there are no guidelines provided for how to determine the impact of these specific factors.",,At several points a more detailed explanation is required.,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Replicability & Generalisability:  A Guide to CEA discounts,https://unjournal.pubpub.org/pub/evalsumtemplateapplied/,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-08T11:38:48.577-04:00,
Replicability & Generalisability: A Guide to CEA discounts,https://docs.google.com/document/d/1eJBSmNG-iRJ-twoHaoztQUmB4pmEHut8_oT3xgtldnI/edit#heading=h.7om527x8q9t7,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Evaluator 1,,"Pro:
Raises important points and brings them to wider attention in simple language.
Useful for considering individual RCTs.

Con:
Not clear enough about intended use cases and framing.
Writing should be clearer, shorter, more purposeful.
Guidelines need more clarity and precision before they can be genuinely used.
I think best to reframe this as a research note, rather than a ready-to-use ‘guideline’.
Unclear whether this is applicable to considering multiple studies and doing meta-analysis.
","Summary
Overall I agree on the importance of the points that the author raises. It’s very good to bring them to wider attention. I will mainly focus on the ways to improve this text, but that should not take away from the value already present in this text. For example, I will critique clarity of text in several places, but overall it’s very good. It’s just about making improvements at the margins.
Crucially, it’s hard for me to assess the usefulness of this note, because I do not know what a typical user of this note already knows. However, the note itself does not really define its readership and situations where it should be applied. I think it would be great if the author started the note by stating this very clearly. For example, there are several sentences early in first person singular, but I couldn’t quite parse them. Does the author mean the team at FP, effective altruist decision makers, people working in global health more broadly? 
I’d recap the main problems as follows:
To this reader it was not clear who the intended audience is and the scope of situations where this report applies felt poorly defined. I think for the intended end user (and here again I am guessing who that is) these guidelines won’t be clear yet. I think they are more of a list of considerations that should be applied when evaluating individual (more on that below) RCTs.
The document is clearly work in progress. I think more care is needed to prepare figures and tables, as well as drafting the guidelines more precisely before they could genuinely be used. They would also need more (and more comprehensive) case studies. Alternatively, this document could be reframed as a research note or a case study, dropping the “guidelines” framing.
I think various methodological revisions are best done only after there is more clarity on scope and purpose of the doc. However, I have tried to list various suggestions throughout my review.
I think the very last sentence of the report (“I think this work is likely to be most helpful for establishing discounts for ‘risky’ interventions that are comparatively less well-studied”) would work well right at the beginning. It is important to recognise that different situations will require different approaches.
Details
I believe that throughout the document every time effect size is mentioned, it is implied that the author is interested in deriving an average effect size (as opposed to, e.g. calculating a probability of the effect exceeding some threshold or working with the entire predictive distribution). If that is the case, it would be good to state this very clearly right at the beginning. It would be even better to explain why. I assume this implicit assumption will be clear to nearly all readers, but even then it is worth pointing out what is in scope of this report and making people consider the alternatives. 
I am unsure about the way that Type S errors are incorporated into the practical instructions. First of all, why would you check for it AFTER assessing adjustments to effect size? I think this would feel somewhat confusing to a hypothetical reader. More generally, I think this type of “type M” and “type S” error dichotomy makes sense in the context of evaluating quality of various domains of scientific literature more broadly (“look, studies make this type of errors this often”), but if your focus is on evaluating a single RCT, then I think it is most natural to frame the internal validity adjustment as a single process. 
Relatedly, why say “baseline estimates” and not priors? I am somewhat agnostic about this choice, but just curious? Is it because the intention is to keep this as simple as possible and the worry is that readers will be confused by Bayesian concepts? Or does the author think that it is not a good idea to view the problem through a Bayesian lens in these situations?
I would be more careful with claims about what is underappreciated: I think the problems described in this text have been part of the “zeitgeist” for about a decade now (look at publication years and consider that each of these were a result of at least a few years of work on these topics). I grant that this is a minor comment, but it is part of the more important question of who this report is from and how it frames its message.
Some writing is a bit unclear or redundant:
A good example of repetition is how pages 7-8 repeat much information from the beginning of the doc, which is then explained again on page 9. At that point it feels like internal replicability is being explained for the third time. Then a repeat of what's already been explained (1.5 pages earlier, in fact) ""I have split Internal replicability into two components"". On the other hand, some material that belongs together is then strewn around: pg 10 and pg 12 have different references to low power in large sets of published studies. I’d scrap nearly all of the intro to avoid the repetition and make the note much shorter. Note that the Gelman and Carlin paper is only about 10 pages, with a couple of case studies. 
I believe in several places I saw reference to “a discount”, but the author then referred to the adjusted value (after applying the discount)
I don’t think there is a need to capitalise “Internal replicability” or say things like “increases inflation”. “The ‘winner’s curse’ means that the filter of 0.05 significance level means that the under-powered studies”.
In the part early on which starts “I split Internal replicability”, there is talk about Type M errors and adjustments. Sometimes these terms to be used interchangeably in ways that may not be intuitive to people who have not heard these terms before. I think a very careful explanation of “error” and “adjustment” terminology may be helpful to some non-technical readers.
Some statistical language is a bit imprecise (although this is a very minor point) power as ""probability that hypothesis test will successfully identify an effect"" or ""due to low power, the study does not successfully detect the 0.1 or 0.2 effect sizes"". It would be better to say this in the language of hypothesis testing. 
I think there is a way to phrase things with more formal rigour, without complicating things, e.g. ‘rejecting null hypothesis’ instead of ’identifying an effect’
Some commentary on guidelines 
I think the main blind spot of this document is what happens when there are multiple studies. There is some discussion of that and a mention of “averaging effects” after adjusting each RCT separately, but I find it a bit confusing. There are whole systematic literature reviews and evidence synthesis/meta-analysis literature journals. I don’t know whether that is in the scope for this report, so I won’t go into that, but I think this either has to be put “out of scope” of this document or completely reworked by referencing the existing approaches better. 
I think a proper treatment of this topic would require a whole separate review. At this stage I think it’s best to assume that this is not in the scope of the report. But here are some pointers in case the author decided it should be in scope: 
I’d start from reviewing the Cochrane handbook and grading of evidence as it is done in meta-analyses. I think the publication bias assessment methods cited are quite new and I am not sure they are a genuine improvement over what already exists in the literature (this is an expression of ignorance, not skepticism), so it may be good to first stick to standard methods used to assess publication bias. There is also a lot of debate in the literature on whether to downweigh low quality studies or simply exclude them: any potential guidelines would have to address that problem. 
Next, the author would have to consider an operational problem: at what stage is it better to outsource a meta-analysis to experts rather than do it “in-house”.
Lastly, when dealing with multiple studies, the division between internal and external validity is no longer clear cut: in the use cases that author has in mind you would typically be averaging on studies where effects can be expected to be heterogeneous. So the relevant adjustments may have to happen within a meta-analysis model. My personal feeling given this very abbreviated “tour” is that in such situations it gets too complex for an “operationalised” approach, but I am perhaps bound to be biased that way.
I think the worked example is great and a must. But if these are really guidelines, then there should be several worked examples, of different levels of “difficulty” (highlighting how the rules can be applied in diverse and ambiguous cases). I think it is imperative to making these widely usable.
Still thinking about the case study, it’s better to calculate power yourself even if a study has a sensible looking pre-registered power calculation. For the example used in the article, power/MDE depend primarily on the number of women randomised into the trial and how many of them use contraception in absence of treatment. Both of these quantities are of course authors’ predictions made before the trial was conducted and may differ a lot from what actually happened.
If researchers are really supposed to use Fig 9, then there should be a table from which they can read the right value. 
I do not like that “Hawthorne effects” are assessed twice. I think there should be more consistency.
I don’t understand the role of the sanity checks recommended. I think it’s better to just be familiar with individual studies and studies and assess their merits on a case-by-case basis, or at least this is how I try to do it in my practice. There is, I presume, little value in using some discount across all studies to bring them closer to zero: even if you believe that “most published findings are false”, but your funding process is in fact based on reviewing published findings and a fixed budget, then applying some skepticism correction indiscriminately to all studies will not change your decisions.
The report talks about “attending to” sample differences, but I feel like detail there is lacking.
I think you need a mechanistic model for external validity (how different population factors impact the effects; how implementation factors impact the effects).  For example, if you think that future implementations are going to differ in terms of take-up, then you need to postulate a simple model of how you think take-up impacts the effects of the intervention. I don’t think a “blanket” prior on how much variance there is across studies will suffice. In my experience subject-matter experts can provide domain-specific heuristics (“if take-up halved, then I think the effect size would roughly halve, because x, y, and z”) which are sufficient to postulate such a model. A lot of it is also common-sense. On the other hand, heterogeneity in effects in large body of literature is itself heterogeneous. Moreover, these mechanistic models are also amenable to statistical analysis in cases where you can obtain extra data. Or the relevant work has already been done by other authors (to give a made up example, for modeling effects of malaria vaccine roll-out you can find a whole set of epidemiological modeling papers of how benefits scale as a function of coverage).
On that note, I’d like much more detail in section 3b, Step 1. What is the list of factors compared between RCT and target population? Why was the discount 20%? ","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",25,0,50,,,,,,,30,20,80,30,10,70,10,0,50,70,50,95,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",,,,,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",I have worked as a statistician [for about a decade].,"I am not sure, sorry. I’d guess 20.",False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),"I am not sure what to compare this to, because I am unclear about scope. So please don’t take these ratings seriously.","Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,"I think this is a report summarising some methodological literature, so I won’t assess on this metric","Pro: It talks about important concepts that may not be known to decision makers

Con: I think it needs more work (to have workable guidelines) before it can actually impact practice. It would be good if this report could be related to existing literature on grading evidence etc.","Pro: right level of detail for decision makers, good use of plain language to communicate the concepts.
Con: A lot of repetition, needs more clarity in some places. Figures and tables need reworking to be useful.","I think the main “replicable” part of this paper are guidelines, and as I indicated, I think these need to be revised to apply to particular cases. I don’t think an average reader would be able to use them right off the bat.",I think this is very relevant to global priorities research! Question marks over focus,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Replicability & Generalisability:  A Guide to CEA discounts,https://unjournal.pubpub.org/pub/evalsumtemplateapplied/,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-08T11:31:08.686-04:00,
"The Long-Run Effects of Psychotherapy on Depression, Beliefs, and Economic Outcomes",https://www.nber.org/papers/w30011,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Evaluator 2,,This is a high quality paper that extensively studies the effects of therapy. I make a few small suggestions for analyses to explore the economic effects and the perceived efficacy of therapy.,"This paper studies the long-run effects of therapy on depression, using two randomized trials of psychotherapy in India. The main result is that therapy reduces the likelihood of depression by 11 percentage points, five years after treatment. The evidence for this claim is strong, coming from follow-up data on randomized controlled trials with no evidence of problems in balance or attrition; deviations from the pre-analysis plan are clearly explained. The paper investigates the mechanisms of the causal effect, studies economic effects, and elicits beliefs about the perceived efficacy of therapy. The focus on long-run effects is important for informing policy.
The authors find no effect of therapy on economic outcomes like employment and earnings. This is explained by the sample being mostly women combined with social norms in India preventing female labor market participation. This leaves open the possibility that therapy affects economic outcomes for men, which is not tested. From Table A.1, there are roughly 70 men in the HAP trial follow-up (N=391 and proportion female of 0.82), which could be a large enough sample to test for an interaction effect. The authors note that the economic outcomes are not corrected for multiple-hypotheses testing and should be viewed as exploratory, so further exploratory work on the economic effects for men seems appropriate.
The authors report that therapy increased perceived effectiveness of therapy in both trials, despite the THPP trial being ineffective. One explanation, proposed by the authors, is that patients are not able to distinguish correlation from causation. Depression was reduced in the treatment and control groups in THPP at the same rate, presumably due to a common cause: pregnancy and childbirth (the sample is pregnant women). Hence, because their depression improved while receiving therapy, treated patients mistakenly believe that therapy caused the reduction in depression.
Since the treatment group beliefs about efficacy are actually higher in THPP than HAP, we might think that experimenter demand effects are playing a role. This is consistent with THPP being a longer trial than HAP 1 (6-14 sessions over 7-12 months in THPP compared to 6-8 weekly sessions in HAP), and hence being more likely to leave participants with fond memories of the trial. However, the evidence is mixed, since demand effects predict higher perceived efficacy in the treatment group relative to the control group, which is true for beliefs about efficacy at 1 year and 5 years after treatment, but does not hold for 3 months after treatment (Figure 4).undefined
Given the importance of understanding how perceptions of therapy affect take-up, it seems worthwhile to dig deeper into these results. The authors test for heterogeneous treatment effects for the effect on depression (Table A.3), and could repeat this analysis for perceived efficacy.undefined For example, how does the effect of therapy on perceived efficacy vary by baseline expected efficacy?
The authors claim that $7 per month of depression averted is “remarkably cost-effective”, but it is not clear how to place this number in context. For example, it would be useful to know the cost-effectiveness of other therapy interventions. Moreover, it would also be useful to compare therapy against a cash transfer benchmark. Similarly, the discussion of policy-relevance could be expanded. How generalizable are the effects of a basic therapy like behavioral activation (which encourages engaging in enjoyable activities)? Do the effects reported here depend on characteristics specific to India?
The authors collected expert forecasts using the Social Science Prediction Platform. As shown in Figure 8, the interquartile range of experts’ predictions contains the actual estimate in only 3 out of 6 outcomes. This is beyond the scope of this paper, but for knowing how seriously to take these forecasts, it would be helpful to see a track record or some form of validation.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",92,81,100,,,,90,97,83,90,80,100,79,69,89,88,82,95,86,79,93,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4.7,4.6,4.4,5,4.1,5,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",[4-8 years — range coded to protect anonymity],10,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,"Relevance to GP: 82, 92, 100",,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,"The Long-Run Effects of Psychotherapy on Depression, Beliefs, and Economic Outcomes",https://unjournal.pubpub.org/pub/evalsumpsychotherapy,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-08T11:24:35.687-04:00,
"The Long-Run Effects of Psychotherapy on Depression, Beliefs, and Economic Outcomes",https://www.nber.org/papers/w30011,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Evaluator 1,,"This paper’s most important contribution is the evidence for sustained positive impacts on mental health after 5 years (one of the longest follow-ups in the field), after short, lower-cost (community based) therapy. As critical comments, I don't believe the 'depressive realism' task was well selected, I see opportunities to better contextualize the effects for the reader as well as opportunities to improve figures and make stronger policy recommendations.","Manager’s note: We added the emphasis (bold text) below
Main contributions
This work has many strengths, including:
Evidence for positive impacts on mental health following therapy 4-5 years later, one of the longest follow-ups in the field
Consideration of wider outcomes, including nulls for economic outcomes - sheds important light on [the] limits of what therapy cannot do
Very valuable to see low-cost and relatively short-duration therapeutic approaches be effective in the long run - the fact that these interventions were community/ peer-delivered is great both in terms of cost savings but also in terms of community empowerment
The collection of experts’ opinions is not often seen and I think nicely complements the surprising, positive nature of resultsundefined
The two main policy-relevant findings are (1) that therapy has sustained positive effects in the long run, thus highlighting therapy as relevant for future global health prioritization, and (2) that therapy can be delivered in lower-cost and shorter durations that are impactful. There is existing evidence on the second point, so this triangulation is welcome to see in shaping a robust body of evidence. On point one, future replication will be needed to better understand these effects, but this is a promising result in an overall well done study.
Critical points
One of the main results is that, after pooling the 2 trials, the treatment groups had 0.15 SDs lower PHQ-9 scores relative to the control,undefined at a p = 0.08. This is the result I’m least confident in and most interested in seeing replicated. While p <.1 is a convention for significance in economics, it is not in many medical and epidemiological fields, and I too prefer a more conservative cutoff <.05, especially for novel or surprising effects. Importantly, recent large-scale robustness replications of work published in top journals suggests effects at the p >.05 but <.1 level were least likely to replicate and effect sizes would frequently diminish very [notably] (approximately halve I believe; see Brodeur et al. 2024)[undefined]
Decomposition of PHQ: good to see this. Sometimes in practice specific symptoms are measures with singular or very few items, making these less robust measures overall. It’s hard to get a sense of how robust each measure was from the current write-up.
Measure of mental health: would have been better in my opinion to survey a wider battery of mental health measures, rather than just depression, especially since this is the focal point of the paper. In some ways I think the language of the paper may overstate benefits and under-specify their domain, we’re really looking at depression and most likely targeting behavior and mood here. I appreciate the difficulty of administering many measures in LMICs especially, but there are now good and validated and even short measures that can further capture broader mental health measures. (e.g. GAD-2 or 7).
Would be nice to see Cronbach’s alphas or similar for PHQ-9.
Very brief mention on missing data - could this be reported more clearly, how much data was missing, was it at random, which measures were affected, was any of it at high degrees? Something like Little’s MCAR could be a quick start here.
With some uncertainty, but it seems to be that baseline mental health might have mattered, considering HAP adults had moderately severe depression whereas the THPP participants had ‘lower (moderate)’ depression. This might be interesting to unpack, as recent interventions have found no heterogeneity based on baseline symptomatology (Barker et al. 2022)[2], but other meta-analytic work has suggested that interventions seem to be more effective when baseline problems (Bower et al., 2013)[3]
It was very interesting to read the discussion around the THPP and lack of stronger effects, particularly around high rates of spontaneous recovery. I wonder if more specific measures for postpartum depression at pre- and post- would help in the future.
On the note of population characteristics, I imagine the authors would be underpowered to test for gender differences given women are more widely surveyed across two interventions. This should be a target for future work, as well as capturing a more general population. As above, pregnancy is associated with particular patterns of mental health risk and recovery that make generalization difficult.
I found it difficult to get a sense of population characteristics beyond what is in the tables, which was brief - e.g. a sense of income?
Overall, I think the findings need to be better contextualized. The authors define them as ‘substantial’ but what does that really mean? A few ideas to make this clearer to readers could be:
The classic epidemiological argument that at population level this matters a lot (Rose 2001)[4]
Reference and directly compare effects targeted by other comparable interventions, perhaps such as behavioral change - e.g. around substance misuse, sexual risk, bullying etc., preferably in the long run; and then also around other primarily general-health-focused interventions such as e.g. obesity, ADHD. There are many systematic reviews on these topics and my reading is that mental health interventions are as tractable and comparable/ not too far behind to general physical health interventions. This might not be clear to a reader or someone in policy, the latter of whom might be facing difficult decisions on triage and funding prioritization.
Further on the population-level point: I would have loved to have seen a stronger conclusion based on the evidence on how we should address the inverse care problem. Therapy may seem [to be] an individual-level approach to health, but when scaled to a governmental health system, it will be more impactful, and that’s where effort should go. See further Chater & Loewenstein 2023[5] on why individual-level framed interventions and recommendations misdirect funding.
More detail on why pooling the two trials is appropriate and what limitations that may have would be welcome for readers without a clinical background. The shared treatment approach allows this, but the very different populations examined jointly could be a difficulty for precision in interpretation and understanding mechanisms. For instance, the authors’ broad level interpretation is that HAP drives the overall effects but certainly someone could say it’s population dependent, e.g. not being pregnant that contributes or matters more than the intervention.
A little bit more on why behavioral activation matters would be good. For instance, it is not clear from the current writeup why treatment primarily targeting behavior and then affecting mood would be expected to change different types of cognitive outcomes, including some of the examined ones.
If tasks were financially incentivized, was it clear (any formal test?) which was linked to receiving or not the financial payment? From past experience in global health, instructions around payment in the context of experiments can be very hard to understand for participants.
A few points on the bead bracelet making task. Overall, I don’t think this is a suitable task for testing for depressive realism, though it is an interesting way of showcasing the Dunning-Kruger effect. These points are relatively more minor, as I don’t see this as the main focus of the paper, and indeed the authors do not frame it as such. This is also a controversial literature, where many of us have different views, though there is a building consensus from reviews that if there is such an effect, it is likely more constrained to certain circumstances and domains rather than universal.
Deficits in self-perception, negative views of self, rumination are fairly established for depressed individuals.
Tasks should not be self-relevant if we want to isolate the effects of such deficits from perception about others or the environment.
Tasks where there is no clear standard for reality, such as relative positioning to others, are a weak benchmark (Moore & Fresco, 2012[6] - see this review on this point and further critical points)
If participants were informed they could get a job later reporting overconfidence is not illogical - how was this accounted for?","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",77,70,84,,,,75,82,68,83,76,90,77,74,80,35,20,50,83,76,90,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4,4,3.5,4.5,3.5,4.5,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",~5 in global mental health specifically,~50,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,"Overall, this is a very sound paper, including two tightly controlled RCTs that seem well balanced and not impacted by attrition. Consideration of a wider dashboard of mental health measures would have strengthened the work. I'm unsure the bead bracelet task is suited to testing the question of depressive realism. Further replication in a more diverse sample would improve my confidence in the results.","The findings are relevant to global health funding prioritization particularly within mental health as a cause area. They are especially important as a contribution given the lower cost, due to training members of the community, the relatively brief duration of therapy, and the sustained long-run positive effects. At the same time, I believe the authors could draw out more explicit policy recommendations. I've tried to outline this more in my report.",,"Sufficient data is presented for meta-analysis. Some figures and their labels can be improved, see report for further info. No code or data are shared, nor is there a statement on data availability, hence low score. I've worked on many different reproducibility studies and based on this experience find that the direct and exact computational reproduction of any work where code isn't available is difficult, as researchers rarely specify 'finer' points like estimator choice, pre-processing can be opaque to a reader using only the paper, or sometimes authors will refer to citations explaining a certain method but do not report in the paper all their specification, for instance here the double ML approach.","See above on Advancing knowledge and practice.

Relevance to GP: 80, 85, 90",,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,"The Long-Run Effects of Psychotherapy on Depression, Beliefs, and Economic Outcomes",https://unjournal.pubpub.org/pub/evalsumpsychotherapy,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-08T11:17:19.039-04:00,
Universal Basic Income: Short-Term Results from a Long-Term Experiment in Kenya,,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Evaluator 2,,"[Strengths]: + Rigorous large-scale RCT design + Simple yet effective statistical approach + (Mostly) outstanding statistical analysis/interpretation & clear communication + Insightful analysis of cash-transfer effects in development context
[Limitations]: - Limited relevance for generalized UBI in macro equilibrium - Potential biases in self-reported data, which is insufficiently discussed - Additional socio-economic context would enhance understanding - Some statistical results and interpretations from the paper remain unclear","A. On this evaluation
My review is informed by a background in economics and policy, UBI, and the fundamentals of statistics and econometrics. An ideal complement could be a separate evaluation focusing more specifically on how the paper fits into the current development economics research landscape.
Given how the paper is presented, I evaluate it with regards to three implicit goals […]:
(i) Informing us about economic effects from UBI in general and in a development context in particular,
(ii) informing us about [the] effects [of] and the ideal design of cash transfer programs, and
(iii) advancing [the state of the art]of RCT execution & evaluation.
According to the abstract, the paper indeed examines “What would be the consequences of a long-term commitment to provide everyone enough money to meet their basic needs?”, and the preregistration registry states the goal to also inform UBI debates in wealthy countries, including as to whether economic security motivates their citizens to work more or less.
The various highlighted limitations with regards to the generalizability of the study results to potential UBI settings more broadly should not be misinterpreted as a fundamental critique of the core analysis in the paper itself; some of the limitations could be difficult to overcome within a cash-transfer RCT in a development context.
B. Subjective summary of the research
Context
The interest in universal basic income (UBI) has become increasingly popular during the past decades, with the growing inequality within most countries worldwide, and looming automation plausibly being contributors to the interest in the topic.
Given this interest, it seems a natural idea that a new RCT on cash transfers be designed in a way to elicit insights on the viability of a UBI, and, as the authors argue, with a focus on (long-term) effects on economic development and (market) income
[…added line break] given the difficulty to continuously finance a UBI in the developing world. GiveDirectly financed the study.
Study
RCT on poor villages in Kenya, to observe economic responses from cash transfer schemes of three types (“arms”): (i) one-off lump-sum, (ii) monthly for two years, and (iii) monthly for 12 years. Each arm delivered a similar cumulative income transfer within the first two years, “suﬃcient to meet basic needs”.
Early-stage evaluation two years into the program, finding a multitude of economic effects from the cash-transfers. There was “substantial economic expansion—more enterprises, higher revenues, costs, and net revenues—and structural shifts, with the expansion concentrated in the non-agricultural sector. Labor supply did not change overall but shifted out of wage employment and towards self-employment”. The expansion was concentrated “particularly in retail—indeed much of the economic story appears to have been the expansion of supply chains to meet increased local demand for goods manufactured elsewhere”. The value of land increased a lot, and “household well-being improved on some common measures (e.g. food consumption, depression) but not others (e.g. children’s anthropometrics and schooling)”.
A few differences between the arms’ effects may surprise. The authors design a model that can theoretically explain these.
The authors strike a rather positive tone with regards to implications for UBI, and I understand this is notably due to the absence of a negative effect on total working hours.
C. Strengths
Well-conducted large-scale randomized controlled trial (RCT) on relevant questions of increasing importance or interests.
Scale and spatial setup of RTC informed by careful examination of expectable statistical power of the analysis, and on expectable spillover effects between different groups.
Simple, yet effective statistical approach, well explained.
In many instances, exceptionally sober statistical interpretation of results: Regularly discussing the confidence interval (CI) ranges as opposed to only statistical significance (or absence thereof); focusing not just on statistical significance but also on economic significance of results.
Preregistration of study along with to-be-analyzed core variables.
Effective three-arm setup with different cash administration durations, enabling deeper insights into the subtle psychological and financial mechanisms at play, offering valuable insights into short-term and potentially long-term economic effects.
Treatment subjects are individual members of the households in the treatment villages. This seems a useful innovation relative to the more usual approach of treating households, whose composition can change over the study period.
Assesses a broad range of relevant outcomes, with outcome variables including, for example, psychological, financial/economic, and health results.
All in all, careful implementation from start to finish means not only reliable results are gathered, but the study can also be an inspiration for future work in similar and related domains.
D. Main claims and assessment
I address four major claim sets.
1. Claims around economic expansion, structural shifts, and labor supply: Communities receiving UBI showed significant economic expansion, characterized by increased business enterprises, revenues, and shifts from wage employment to self-employment, particularly in the non-agricultural sectors. Total labor supply did not decrease, indicating that UBI does not promote ""laziness"" but reallocates labor towards more entrepreneurial activities.
These effects are factually observed and rather robust (see Table 4). One open question is how far they may generalize. In so far as the study will be seen as supporting the idea that “UBI works” in [the] most general sense, multiple limitations are noteworthy:
1.a) The scheme did not first fiscally collect revenues from the local, treated population, for redistribution in form of an unconditional cash transfer, as is usually thought of for a UBI scheme. Instead, the money distributed was ‘manna from heaven’ for the treated population, i.e., an inflow of external cash (from GiveDirectly). A series of macroeconomic equilibrium effects expectable of a more standard UBI can therefore not be observed.
1.b) Baseline wealth and size of transfer: While substantial for the concerned households, with roughly 0.30 USD/day per person in the household,undefined the transfers received seem small even with PPP-adjustment (0.76 USD PPP per person in the household), when compared to the requirements of a materially more or less decent life, even in the villages. The households were quite poor in the baseline (85% experienced hunger, according to Table D.1). It can therefore not be known to which degree the observation that the “UBI” does not weaken labor incentives generalizes to other contexts. Arguably, it would seem natural that less poor populations, receiving a higher UBI, might more readily shift, after their broader basic needs [were] met, to unpaid activities, reducing their wage labor supply.
1. c) Desirability of shifts: A related question that the paper does not discuss in much detail, is how ‘positive’ the observed economic changes really are. Naturally, the transfers have a first-order effect of allowing the households to consume more, including, for example, more proteins. The broader “economic expansion” and “structural shifts” away from agriculture have been overwhelmingly concentrated in the retail sector. It therefore seems like, first and foremost, more goods, produced elsewhere, seem to have been consumed. This may contrast with hopes that much of the new resources would be invested [in] local production of new types of goods and services. Changes in the latter direction would have partly occurred according to point estimates, though they were not statistically significant, and seem to have overall been mostly quite small relative to the observed retail expansion (Table E.9).
2. Claims from comparison between transfer types: One of the most noteworthy differences between the three arms was that the lump-sum & long-term arms had a much larger economic expansion than the short-term arm, while the latter instead had a larger expansion in consumption. For mental health, the main difference was between the lump-sum and the two regular payment arms: mental health improved significantly less in the lump-sum arm than in the other arms.
These conclusions seem statistically robust (Tables 2 and 9). The question about the underlying reasons is interesting. Without presenting it as the only potential explanation, the authors show how the smaller economic expansion in the short-term arm can be obtained in a model with savings and borrowing difficulties where “investment projects require both a large up-front capital outlay and ongoing ﬂow investment to turn a proﬁt”. This is a relatively complex mechanism. I wonder whether a much simpler, psychological explanation could be behind the results: Receiving a larger one-off lump sum, might spur one to think seriously of how to productively invest it; receiving regular payments for the longer term, might also make one wonder how to best change one’s life. Receiving, instead, the limited extra income regularly for a known shorter period of two years, might do less of either of these two things, and may therefore instead mainly lead to an increased consumption in the short term arm.
3. Claims about psychological and consumption effects: UBI increased general well-being, reduced depression, and improved nutrition through greater food variety and protein consumption. It also increased education spending across the three arms. Impacts on health spending were mixed across transfer schemes.
These effects are readily observed in the data (Table 5). They seem mostly unsurprising, with the main question being to which degree some of the more unexpected differences across arms (e.g. health) might be due to statistical noise. After all, with the large number of parameters estimated in the study, we’d expect potentially several […] to become statistically significant, potentially even with the wrong sign, merely due to statistical coincidence.
4. Claims about price impacts: “Overall we do not reject the null that consumer prices [in nearby markets] were unaﬀected, in which case the revenue increases discussed above come entirely from increased sales, albeit with fairly wide conﬁdence intervals”.
On one hand, Table E.1 confirms that many price effect estimates are insignificant, and the authors’ explanation as to the reason why the estimates may have a large confidence interval seems compelling.undefined On the other hand, among the “Study Shares” estimates in Table E.1,undefined almost all the nine “Study Shares” effect estimates (three arms times three goods sets: Overall, Agricultural, and Non-Agricultural items) have a positive sign, three of them significant, and most with at least around one standard deviation in the positive direction, and only one with an economically meaningful negative price effect point estimate (albeit only half a standard deviation in size). The table provides less statistical information than some of the other tables (e.g. no p-values for joint tests), so I could not follow in detail how exactly the authors concluded [that there were] no significant price effects in the data; overall, from the data provided, it was not what I expected. In conclusion, from the table data, it should be noted at least that economically highly positive price effect point estimates were found, some of them individually significant and others often roughly one standard deviation large.
Two further points with regards to the price effect seem worth keeping in mind:
1. Should the price effects indeed be economically significant, one could imagine them to be mainly a temporary phenomenon. Many of the goods surveyed (sandals, fertilizer, soap, …) could arguably be supplied to the market with constant or even increasing returns to scale, once supply-chain and retail facilities adjust to the new demand level.
2. For a macroeconomically more significant UBI (nationwide or even larger regions), demand-induced price increases could, (i) in a similar ‘manna from heaven’-setup, be expected to be substantial rather independently of whether […] major price effects were found in the present study. [This is because] demand effects for many of the inter-regionally produced goods may only start to play a role once quantities demanded increase on a country-wide basis. And (ii) in a more typical UBI scheme, where regional taxes could be used to finance the regional UBI, one could potentially expect there to be limited first-order impact on average prices (as the taxes to be paid neutralize the UBI payments received by households on average). […] Again [this would be] relatively independent of whether the prices in the present study turn out to be meaningfully positive or not.
The authors refer to a forthcoming study apparently showing a tiny price effect of cash transfers[…] I cannot meaningfully comment on [this] without reading that study in detail; the results could crucially depend on time and geographic dimensions of the treatment and its measured effects.
5. Claims on land value: Land values in treatment villages increased strongly, without correspondingly large investments in the enhancement of the value of the land.
This claim appears to be well-supported by results from different surveys, see Table 6. The interpretation [makes intuitive sense]: The improved living conditions and/or the households’ increased purchasing power (and [their] search for ways to store wealth combined with [an] inelastic supply of land) may have contributed to the land appreciation […]
6. Claims on alcohol: There is evidence that alcohol problems have decreased.
This is factually true, although the evidence can be put into question by potential biases in answers on particularly sensitive questions and by harder, albeit statistically less significant, data pointing towards a different interpretation:
The alcohol data is reported in Table E.10. Alcohol is perceived as a large problem (baseline mean 3.37 on a scale of 1-5 of perception of alcohol as a problem within the community). Alcohol as a problem in the treated villages is reported to have decreased significantly, most notably with a very substantial reduction of reported daily drinkers. The point estimates on individual self-reports of drinking (columns 1 and 2) are all statistically insignificant and seem arbitrary in their direction.undefined
The authors do not discuss the last data column, about alcohol sales. Coefficient point estimates point towards a very strong increase in sales for each of the three UBI arms, two being significant at the 10% level, with net revenues more than doubling on average across the three treatment arms. The unit of the values is not indicated. On the one hand, this evidence is statistically weaker than the villagers’ reports about alcohol in their villages. On the other hand, alcohol is a particularly sensitive topic as the authors point out, so without further evidence, it would certainly seem plausible that villager reports about alcohol problems in their village are not perfectly objective. This may be particularly true if very poor respondents know they are surveyed to assess the effects on their village of a program that provides for their livelihood.
It would therefore have been interesting to see whether the authors have alternative explanations for the increased revenue figures to allow one to understand whether the reservations about the overall conclusions on alcohol here expressed may be safely dismissed or not.
E. Further important limitations
Caveats not addressed as part of the Main claims assessment above.
Bias potential: ‘Program goodwill’ bias, survey completion rates, alcohol results
Large parts of the study rely on self-reported data. This begs the question [of the degree to which] participants’ answers may be biased by goodwill towards the program[…] [This could be] both due to positive views towards [the program], as well as potentially strategically in the hope, however faint, that positive answers could increase the probability of receiving future benefits from the program or from similar programs. There are two observations in the data presented that I interpret as underlining this hypothesis:
The follow-up survey response rate (Table D.2) is significantly and substantially higher in each of the three arms than in the control, with roughly 1/3rd less non-respondents in the arms than in the control.undefined It would not seem a priori implausible that if the program motivates people to participate in the survey, they may also have some motivation to report positively about its effects. In a similar vein, estimated treatment effects could be affected if control households, informed about the treatment in nearby villages and knowing they were left out, answered surveys with biases of some sorts. On the one hand this is speculative – the authors do not discuss the topic. On the other hand, at least intuitively, it would seem rather natural for such types of biases to arise in the given setting.
Alcohol (Table E.10): Villager reports report a reduction of alcohol consumption problems in their villages (Columns 3-4). At the same time, point estimates indicate high increases in sales in all three arms (Column 5), although only significant at the 10%-level in two of the three arms (see comment on alcohol above). Although statistical noise can obviously not be excluded, a preference for reporting a positive picture about the program’s effects could immediately explain the observations in particular in this domain of the known large problem of alcohol.
Other NGOs
The authors report a significant effect on the number of other NGOs active in the treated villages, although it is not obvious (to me) how the corresponding data table is consistent with the author’s exact text.undefined
It would seem very useful to understand more about the activities of these other NGOs, and how the GD cash-transfer program affects their presence, to get a better understanding of how/whether study results may be driven by location-specific interaction effects that cannot be expected to generalize to different settings.
Information of participants/framing
The early-stage effects of a promised 12-year intervention may depend a lot on how much confidence participants have in the long-term nature of the project. This in turn depends on how the trial setup is framed/communicated. There is no information on such implementation details.
F. Minor limitations
Authors remove outliers (top percentile) in data without it being clear how this threshold was chosen.
Original datasets and code used do not seem to be made accessible.
G. Final note on UBI
Extending and summarizing reasons for limited generalizability of insights on the economic viability of typical UBI concepts, mostly due to natural limitations of an exogenously financed cash-transfer RCT:
Anticipated limited program duration (even of the longest arm).
Limited lifetime exposure to the scheme (people not having grown up/socialized under a UBI scheme).
Lack of public finance effects (requirement to absorb large domestic resources for UBI financing).
Lack of macroeconomic scale (supply) effects when UBI is administered on a national or global scale.
Further, with regards to generalizability to wealthier regions/higher UBIsundefined: here [this is] a specific development context, with cash amounts not nearly sufficient to ensure what one might arguably consider a materially comfortable, healthy, and save life.
Finally, with regards to a potential future with UBI and jobs AI-automated away, in the present study: lack of unavailability of meaningful remunerated jobs, along with, e.g., potential challenges to traditional sources of self-worth.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",85,65,95,,,,85,95,63,85,65,95,76,40,91,30,10,75,87,74,98,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4,4,2.5,5,2.5,5,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

","In the broader fields of economic policy, statistics, etc.: 10 years.",20,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),"I mainly think of development/RTC papers in general, rather than of 'UBI' studies.","Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,"Despite a few potential shortcomings, overall the papers is rather outstanding in its methodological rigour.",,"Mostly highly transparent & logically explained, however, in some occasions insufficient detail for reader to understand/judge well.","Nope, data, code, and some statistical details are not indicated. Information about distributions of raw observations mostly absent.","Despite some reservations with regards to validity of results for ""UBI"" in more general, high relevance of findings for development context cash-transfer programs.

Relevance to GP: 45, 70, 90 ‘Difficulty to judge on an absolute scale, as this requires comparing with a not well-defined universe of papers beyond the current field.’",,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Universal Basic Income: Short-Term Results from a Long-Term Experiment in Kenya,https://unjournal.pubpub.org/pub/evalsumuniversalincome/,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-08T11:12:19.521-04:00,
Universal Basic Income: Short-Term Results from a Long-Term Experiment in Kenya,,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Evaluator 1,Fix ratings on table,,"This is an extremely impressive (and costly) RCT that will likely be of use to policymakers when designing the structure of benefits.
This study uses a RCT to evaluate three different timings of the same amount of unconditional cash transfer. In one arm, the full amount was transferred as a lump sum; in another, the amount was transferred as a UBI over two years, and in the third, the same amount was transferred over a two-year period, but with the promise of ten years of additional (similarly valuable) transfers. Each participant received $3,000 in transfers over the study period (with the long-term arm expecting to receive an additional $15,000 over the duration of the study). The study seeks to evaluate how the timing of transfers affects the results of the transfers - for instance, if the promise of a UBI makes recipients less likely to work because they can rely on future transfers, or if a lump sum allows for costly upfront investments in productive assets.
The size of the RCT (and cost of the entire program) bears some emphasis; there are some 23,000 treated adults (pg. 11); the cost of this RCT was somewhere in the neighborhood of $70 million USD. As such, it likely seeks to inform governments about policymaking, since it is more similar to scale to a government program than a standard economics RCT. If a government seeks to do UCTs - as indeed, many governments did during the pandemic, and continue to explore beyond the pandemic - it is important to understand how the structure of transfers will impact their results. It will also inform GiveDirectly, a NGO that provides UCTs as it continues to scale up its giving. (GiveDirectly moved approx. $30 million a year before the pandemic; they now move closer to $200 million a year, and continue to refine their model to best support those in extreme poverty.)
One can also imagine it being influential even when considering targeted benefits. Cash benefits can be structured as a one-time transfer, or can pay out over time; since this study shows that people in extreme poverty had larger income gains from receiving a single large transfer, one could imagine governments shifting towards one-time payments rather paying out a benefit over time.
The RCT itself is well done. The analysis is clear; there is a pre-analysis plan that is linked to in the text. The design itself makes a great deal of sense; these are important questions and it seems useful to have a well-powered RCT seek to answer them.
Since every adult in a village receives a transfer, one can examine general equilibrium effects; for instance, the authors seek to answer if prices are higher in villages where everyone is somewhat better off than in control villages. (They do not find that they are, though they note that they are simply not rejecting the null; this is not a precise zero.)
There are some familiar results here, likely expected to any reader familiar with the cash transfer literature. There is no evidence that people work less if provided with a UBI (though it is not clear to me that this would be true in a case where the UBI was large enough to cover all basic needs), nor do people particularly use UCTs for temptation goods.
There are also less familiar results. They find that the economic effects of the lump sum transfer are similar in size (and for some outcome variables, larger) to that of the long-term UBI - even though the lump sum transfer recipients have received all that they will have, the long-term UBI recipients have the security of 10 more years of transfers. This, to [me], suggests that recipients are extremely credit-constrained - and that this is not alleviated by the UBI. Even when they receive a UBI, this does not allow them to make the lumpy investments needed for long-term returns.
Given the size of the study, and the stature of the authors, I do not feel I need (or indeed, am qualified to give) advice to the authors on how they might place in a top economics journal. I suspect the paper will publish well; fewer other RCTs on UCTs have this sample size and are able to truly answer questions about implementation.
If I were to quibble with this study, it would be on the presentation of results. There are many outcome variables here, and three different arms; I found it difficult to follow which results belonged to which arm. It was difficult to keep them all straight. I would have much appreciated a summary table comparing the size of results (and also noting if it was possible to reject the hypothesis that the effect sizes were the same). Ideally, such a table would group types of outcome (e.g. consumption, assets, “softer” measures) and perhaps even be color-coded with relative effect size.
Similarly, the introduction brings up a policy-relevant question that the paper does not address. On page one of the paper, the authors mention that “one could ask whether the transfer raises the incomes of the recipients by at least as much as the amount of the transfer”. While it is possible to calculate if this is true from the results in the paper (e.g. from income and asset changes), this summary is not presented in the paper. I think it would be particularly helpful for policymakers to directly compare the (income + asset) changes to the cost of the UCT. Policymakers - especially in developing countries - operate under significant constraints; this would likely help them better evaluate (expensive) UCT programs in comparison to other programs.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",93,83,100,,,,95,100,90,93,70,100,80,70,90,90,80,100,90,80,100,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4.7,4.7,4,5,4,5,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",five years,"This is my first referee report, but I regularly make project funding decisions for my day job.",False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),"This is a very large, extremely expensive RCT on how the timing of payments in unconditional cash transfers affects the outcomes. It is extremely impressive, and I suspect will be influential on both policymakers, as well as influencing future decisions by the implementing NGO. I expect this to publish well, as it should; it is clean, well-identified work that answers important questions. There are relatively few surprises here - UCTs work, and extremely poor people are credit constrained - but it is nonetheless a very good study. Novelty need not be everything; it is useful to see how things operate at (some) scale.","Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,This is a pre-registered RCT where the PAP is clearly linked in the text.,"This is a spectacularly large and expensive RCT. I think it contributes substantially to both GiveDirectly's likely future practice (perhaps focusing more on lump sums and less on payments over time), and is likely to contribute to policy-making decisions. The lower end of this credible interval is simply because this experiment is so costly it may be difficult for even policy makers to replicate - few developing country governments can make credible commitments for 12 years.","This is probably the paper's biggest weakness; despite the relatively straightforward design, it is sometimes difficult to follow what the effects in each arm were. I think a summary table of relative effect size for each outcome measure (e.g. smallest, largest) would be extremely helpful in following. There are also some minor issues with concept definitions - e.g. ROSCA is used before it is defined.","While I did not review the replication materials, RCTs are generally straightforward to replicate. The method of randomization and stratification are clear.","UBIs are an active topic of policy discussion, and have been implemented by governments (if only as pilots) in rich countries. Some developing country governments are also experimenting with unconditional cash transfers (e.g. Togo during COVID). These results, particularly those that compare a short-term UBI to a similar-sized lump sum, are likely to inform these decisions. I could imagine governments choosing a single lump sum transfer partially based on this paper.

Relevance to GP: 40, 60, 80 ‘For most moral weights, cash transfers are dominated by other interventions. GiveWell estimates that for the their moral weights, their health interventions are ~10x better than GiveDirectly's cash transfers. Even if you weigh health much less highly than GiveWell, there are likely better income interventions (e.g. migration subsidies). This paper provides more information about the impact of cash transfers. This could be useful in providing a better baseline to compare highly effective interventions against, but GiveDirectly is still unlikely to be competitive with the most cost-effective interventions.’","While this paper makes few theoretical innovations, this is policy-relevant work based on a spectacularly large (and expensive) RCT. If the AER and other top 5 journals wish to contribute to policy discussion, this paper should be published in such journals.

The stature of the authors (Banerjee in particular) and cost of this RCT seem likely to result in a top tier journal placement.","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Universal Basic Income: Short-Term Results from a Long-Term Experiment in Kenya,https://unjournal.pubpub.org/pub/evalsumuniversalincome/,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-08T11:03:02.941-04:00,
Selecting the Most Effective Nudge: Evidence from a Large-Scale Experiment on Immunization,,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Evaluator 3,,"The method is mostly a clever combination of existing methods; the main contribution is showing [the] consistency and normality of their estimator. These results need not always hold, as they depend on assumptions, and so applicability is not universal. The authors provide guidance by presenting simulations showing that for n > 3000, normality seems to hold. The field data reveal no surprise[s], and all the interventions have been tested before. Yet, the paper is strong, and the method will possibly remain relevant despite the recent advances in adaptive experimental design, if the authors provide additional guidance or software.","Overall Summary
The paper presents a technique to select a (best) policy from a large factorial design, including, for example, various dosages of a treatment that can be ordered with respect to their intensity. Essentially, the technique aims to aggregate variants of a treatment that yield (effectively) the same effect, thereby reducing the dimensionality of possible options, and consequently increasing [the] power and precision of estimates for the effect sizes of truly effective treatments. The technique is applied to a large-scale experiment, where 75 policies are tested that involve combinations of reminders, incentives, and local ambassadors, with the aim of increasing the number of child immunizations in the state of Haryana (India). A combination of ambassadors and reminders [leads to] to the largest impact when paired with incentives; without incentives, a very similar combination is identified as the most cost-effective one (any form of ambassador paired with reminders).
The method
The method is mostly a clever combination of already existing – sometimes more, sometimes less – established methods. The true merit of the paper with respect to the method lies in identifying those, putting them together, and finally showing that under ""strong assumptions"" (p. 34), the method ""rules out some of the cases where model-selection leads to invalid inferences"" (p. 34), and produces estimates that are consistent and asymptotically normal. Specifically, in a first step, all treatments and their interactions are included in a regression via marginal effects (known from a staggered design, for example). Then, a LASSO is performed. This, however, cannot be performed directly on the marginal effects due to the high correlations. The authors identify a ""Puffer transformation"" due to Jia and Rohe (2015)[1] as a suitable solution. The LASSO then identifies marginal effects that are effective, and others that are not (essentially based on their p-values). Only then the variant aggregation starts, such that dosages that do not lead to different effects (according to the LASSO) are grouped. Treatments that have no effect whatsoever are either ""pooled or pruned (pooled with control)"". Scientifically, I am a bit unsure whether this should be allowed; a discussion seems warranted. It is obviously principled and data-driven, but conceptually, it seems wrong to me.
Regarding the asymptotic results, I cannot fully assess them, as going through all the proofs seems to be out of scope considered that this is not the core of my work. Yet, it is clear that asymptotic arguments and asymptotic results imply that they need not always hold (as they depend on N). Hence, a limitation of the ""existence"" result (the correct support of effective marginal effects is indeed selected by the LASSO), as well as consistency and normality results is that applicability is not universal. The authors provide guidance by presenting simulations showing that normality is approximately fulfilled in the given setting. Whether or not the strong assumptions are likely to be fulfilled in different settings will remain a judgment call; in particular, finding a penalty sequence for the LASSO that fulfills Assumption 5 seems non-trivial. Unless I've missed it, there seems no guidance in that respect (even though there are robustness checks).
A suggestion would thus be to provide some more intuition about the settings in which the assumptions can be expected to hold; after all, the present paper may use thousands of data points, which is not necessarily representative. Unless there will be no software implementation, I am unsure how much practitioners will [adopt] the technique; the Jia and Rohe (2015)[1] correction for correlated regressors seems technical, too, and so does the Andrews et al. (2021)[2]-correction for the winner's curse. As a final comment, it is to be seen whether adaptive designs or the current design approach will become more prevalent in the future.
The field intervention
The field data result from a collaboration with the government of the state of Haryana in India, with the objective [of] improv[ing] immunization rates. The data consists of about 300,000 children, and about 60,000 households satisfied the eligibility criterion of having children between the ages of 12 to 18 months. While this allows for an impressive study, by now, the innovation of the tested tools is somewhat limited. That is: all tested interventions have been studied before, including by the authors, and including in this very same dataset (Banerjee et al., 2019, ""Using Gossips to Spread Information: Theory and Evidence from Two Randomized Controlled Trials"")[3]. This fact is communicated only on page 28, which is a bit late for my taste.
[However], previous research has ignored interactions between the treatments (at least of the full set considered here), which is to be seen as the main contribution of the field data. The tested interventions are: i) incentives (in the combinations low/high and flat/increasing with every immunization shot, where low corresponded to about 500 minutes of mobile phone talk time for all five immunization shots), ii) SMS reminders (no reminder, reminders to 33% of parents, reminders to 66% of parents), and iii) ambassadors (randomly selected, information hubs identified by fellow villagers, trusted advisors regarding agriculture and health identified by fellow villagers, trusted advisors who were identified as information hubs). In addition to informing about possible interaction effects, and despite not being particularly innovative with respect to the tested interventions from today's perspective ([the] baseline data was collected mid-2016), the study can contribute to our understanding of the relative merit of the interventions, which is also extremely valuable, even today (see Figure 4 for the results; only high incentives (increasing with every vaccine) are effective, and this to a similar degree [as] relying on ambassadors - even more than on trusted ambassadors).
Comments on the field intervention: undefined
On page 27, the authors refer to an online appendix, in which they report about their data quality checks. In the main text, they describe the data quality as ""excellent"". In the appendix (p. 92), we learn that children’s names and date of birth were accurate in 80% of the checks, and that there were ""almost no"" fake child records, which is, however, not quantified. Finally, for 71% of the children, the vaccines overlapped completely. I work with field data myself, and I know that these number[s] are realistic, but objectively, this cannot be described as excellent. Administrative data from Denmark or Norway is excellent, but 71% overlapping vaccines certainly are not. As the errors are not different across treatment groups, results should be unaffected, though.
Substitution effects are analyzed in an online appendix, but the tables are in landscape format, but the relevant pages are not, and hence about half of the tables/pages seem to be cut. As a result, [the] notes are not informative, as half of the information is missing; several columns seem to be missing, too. The content of the table remains cryptic, making it difficult to assess whether the incentive-effects are actually substitution effects, or are indeed new immunization shots. It remains unclear how the dependent variable and the regressors are coded (i.e., whether the regressions predict showing up for an immunization under a given incentive scheme and having been immunized somewhere else before (as the main text would suggest), or whether the regressions predict showing up for immunization despite not being in the register (as the Table headings suggest)). Yet, what can be seen looks innocent, in the sense that the share is not different across treatments, and hence there is hope that this is also true for the missing columns. However, the control means look incredibly high (up to .69) should the dependent variable really equal 1 if a child is not in the register. If this were the case, this issue should deserve more discussion.
Section 4 could benefit from proofreading and streamlining. At the end, as a reader one is confused which sub-set of the data is finally analyzed.
According to the pre-registration/pre-analysis plan (first mentioned on page 29, footnote 27 -- also a bit late), the authors were interested in (some) previously not analyzed interaction effects between incentives, reminders and ambassadors (called ""gossip"" in Banerjee et al., 2019)[3]. A systematic approach like the current one did not seem to be the plan, however, not even in the updated pre-registration from 2018. The authors could comment on this change (which is completely legitimate, in my view).","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",85,75,90,,,,80,100,50,85,80,90,80,50,100,70,25,85,95,70,100,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4.5,4.7,4,5,4,5,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",10+ years,20+,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),"Given the fact that each part of the paper in isolation (method; field data) is not in the 15% of best papers in my view (meaning with largest impact), the lower limit of the credible interval is rather low. As the method might have some impact (e.g., if software and more guidance is provided, and if referees are not bothered by the fact that normality is at least quite questionable for sample sizes below 1000), and as the combination (method + field) makes the paper rather unique and quite strong, I have a quite high upper limit for the credible interval.","Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,"As far as I can tell, questionable research practices have been avoided. The robustness to some assumptions is checked, but not for all - in particular - Assumption 5, for example, is quite cryptic. In particular for that, no intuition is given, either. Other assumptions are cleanly explained and justified, even with intuition.","Even though the method relies on asymptotic and other arguments, and hence, applicability will always be a question, I see the method's relevance as the paper's biggest selling point. I am a bit less sure, however, how relevant it will become in practice. The results of the field data are relevant, but of no particular surprise given my reading of recent literature in the field.","This varies a lot throughout the paper. At places, formulas are not explained; the simulation results are not really discussed. E.g., the ""r"" in Figure 3, Panel A - if a result of the simulations - should be discussed.","Data and code for replication is not (yet?) made available; this is not unusual until the paper can be accessed via the journal's homepage, once accepted. A pre-registration is available (but it is not prominently mentioned, as this project has not been planned from the beginning). Hence, for the time being (paper is a working paper, not yet accessible via a journal homepage, but apparently accepted), the lower limit of the credible interval must be very low, but I absolutely expect code and data to become available in the future.","Yes, totally. The paper is very relevant for researchers wishing to implement (large-scale as in N > 1500) RCTs as well as health practitioners. Assumptions will not be fulfilled in very many settings, though; it is unclear how clustered structures will have to be considered. The authors are to congratulate for making their exposition very accessible in most parts and providing intuition behind their setup for most of the assumptions, yet applicability of the method will remain questionable.

Relevance to GP: 90, 95, 100 ‘The paper deals with finding the most effective policy methodically, and applies this to a an applied question in the health-context of India.’","The paper certainly has the scope and potential for a publication in a journal ranked with 5; quality-wise, I feel that there is a huge overlap of publications in journals ranked with 4 and journals ranked with 5. Without checking the proofs thoroughly, I cannot for sure assess whether it is really made for the very best journals. As assumptions are somewhat strong and not always intuitive, I could imagine that a journal ranked with 5 passes. As the field data is impressive, and the method potentially useful (even though it is unclear exactly how applicable it is because of the asymptotic arguments and somewhat cryptic assumptions), I understand that a journal ranked with 5 wishes to publish the paper.
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Selecting the Most Effective Nudge: Evidence from a Large-Scale Experiment on Immunization,https://unjournal.pubpub.org/pub/evalsumeffectivenudge/,Econometrica,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-08T10:55:43.266-04:00,
Selecting the Most Effective Nudge: Evidence from a Large-Scale Experiment on Immunization,,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Evaluator 2,,"Strengths: - Introduces a new technique - treatment variant aggregation (TVA) - to answer policy-relevant questions - Strong theoretical and simulation results grounded in real-world data - Demonstrates advantages in both selection and estimation of policies by comparing to many alternative estimators - Application to an urgent, real-world problem Weaknesses: - Simulations rely on parameters from a single dataset, limiting generalizability - Unclear practical advantage of TVA in selecting better policies compared to alternatives - Bayesian estimators [that are more sophisticated than the ones they tested] could rival or surpass TVA","This paper introduces a new technique - treatment variant aggregation (TVA) - to select a policy from a factorial design and estimate its effect. The authors demonstrate that TVA has desirable theoretical properties and performs well in simulations, both in selecting effective policies and accurately estimating their effects. They apply TVA to study interventions aiming to increase vaccination rates. The authors conclude that the most effective intervention identified by TVA increases vaccination rates by 44%, while the most cost-effective intervention increases vaccination rates by 9%.
Summary
The paper’s primary contribution is a new technique - TVA - to select a policy from a factorial design and estimate its effect. The factorial design assumes M arms each with R dosages. For example, an arm might be monetary incentives with dosages none, low, medium, and high. TVA works as follows:
Feature engineering. Construct a feature matrix such that the features represent marginally increasing dosages. For example, a coefficient on one of these features would represent the marginal effect of switching from medium to high monetary incentives, controlling for the dosages of the other arms.
Feature selection. Use a LASSO regression on a Puffer-transformed version of the feature matrix engineered in step (1) to select certain features. The authors refer to this step as “pooling” (removing features corresponding to different dosages of a given arm) and “pruning” (removing arms altogether).
Effect size estimation and selection. Estimate the coefficients on the policies selected in step (2) using OLS.
Post-selection inference. Apply post-selection inference techniques to obtain quantile-unbiased point estimates and confidence intervals for the best-performing policy from step (3).
The authors show that TVA has desirable theoretical properties. They also use simulations to demonstrate that TVA performs well compared to alternatives, including OLS, certain types of Bayesian estimators, and post-selection inference without feature selection. Specifically, TVA:
Is likely to identify the best policy
Applies minimal shrinkage to the best-performing policy
Accurately estimates the effect of the best-performing policy
Strengths:
While none of the components of TVA are novel, the way the authors stitch them together in a pipeline designed to answer policy-relevant questions is insightful and impressive
Including theoretical results and simulations demonstrates that TVA is an effective tool. Using data from a real factorial experiment to ground the simulation results is especially compelling. This suggests that TVA performs well on the sorts of datasets researchers are likely to see in the real world.
The authors demonstrate TVA's robustness by comparing it to various alternative estimators in different simulation settings.
One of the major drawbacks of Andrews et al., 2021 is that their post-selection inference technique had a high MSE. By applying Andrews et al., 2021 after pooling and pruning, they significantly reduce the MSE of this estimator.
Finally, the authors apply TVA to an important real-world problem - improving vaccine uptake in Haryana, India. I consider this a genuinely important problem that is well addressed by the author’s research.
Weaknesses:[undefined]
The simulation results appear to draw certain parameters common across simulations from a single dataset (related to vaccination rates in India). These parameters may not be representative of other datasets researchers are likely to encounter. While the authors take steps to mitigate this issue by varying simulated sample sizes and other simulation parameters, the results would be even more compelling if they relied on more than one underlying dataset.
The authors show that TVA is more likely to select the most effective policy than OLS. However, the authors do not estimate the practical effects of this advantage. For example, suppose the best policy increases vaccination rates by 44%. TVA selects this policy 100% of the time, whereas OLS selects it only 50% of the time. However, the other 50% of the time, OLS selects the second-best policy, which increases vaccination rates by 43%. According to the metric the authors present in their simulation results, TVA is twice as likely to select the best policy! However, the practical effects of this improvement are negligible. In sum, it is not clear how much higher vaccination rates would be if we selected policies by TVA as opposed to applying other selection methods. The results would be more compelling if they demonstrated that the policy selected by TVA is, on average, much more practically effective than policies selected by the alternative estimators they consider. Absent these results, it is difficult to evaluate TVA’s practical advantage in selecting better policies.
The authors compare TVA to many alternative estimators, and it is not reasonable to expect them to exhaustively explore every available alternative. However, the paper considers only one alternative Bayesian estimator - a simple version of spike and slab - and a slightly more sophisticated Bayesian estimator could rival or surpass TVA. In particular, I would be interested in seeing how TVA performs compared to a Bayesian estimator that estimates the prior mean as a function of the dosages in each arm and then applies a Bayesian shrinkage estimator as usual, as described in Section 1.4 here. One could even estimate the prior mean using the first steps of TVA; a LASSO regression on the Puffer-transformed “Hasse lattice” features.
Overall, I consider this an extremely impressive and useful paper. I expect the technique it introduces is significantly more accurate in estimating the best-performing treatment than several alternatives, such as OLS, direct application of Andrews et al., 2021’s hybrid estimator, and simple spike and slab Bayesian estimators. I am also cautiously optimistic that it selects more effective policies than alternative estimators, although I am uncertain of how much better it is from a practical perspective (e.g., in terms of increasing vaccination rates compared to counterfactual policy selection methods). I look forward to seeing researchers apply this useful technique to select and estimate policy effects under factorial designs in future work.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",95,80,99,,,,90,95,70,95,80,99,90,70,95,90,70,95,95,75,99,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4.7,4.7,4,5,4,5,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",[Range coded for anonymity: 5-10 years],5-10,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,"Relevance to GP - 70, 85, 90",,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Selecting the Most Effective Nudge: Evidence from a Large-Scale Experiment on Immunization,https://unjournal.pubpub.org/pub/evalsumeffectivenudge/,Econometrica,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-08T10:43:08.706-04:00,
"Pharmaceutical Pricing, and R&D as a Global Public Good",https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4728113,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",,qwert,"Strengths and achievements:
Applies a classic public-good model to explain why U.S. drug prices exceed those in ROW and remain below the global optimum
Empirical markups dispel the notion that non-U.S. markets fully free-ride
Positive GDP–price correlation aligns with theoretical predictions
Critiques, limitations, and suggestions:
Empirical tests are correlational and could have alternate explanations
Theoretical results rest on the assumption of optimal centralized government action
Policy prescriptions likely warrant further empirical validation",,"Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",30,10,50,70,50,90,70,90,60,25,10,40,80,70,90,100,100,100,35,20,70,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",2.5,,1.5,3.1,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",,,True,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",RR.docx,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Pharmaceutical Pricing and R&D as a Global Public Good,https://unjournal.pubpub.org/pub/evalsumpharmpricing/release/3,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-06T21:54:18.260-04:00,
Selecting the Most Effective Nudge: Evidence from a Large-Scale Experiment on Immunization,,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Evaluator 1,,"This is an absolutely superb paper tackling a hugely important policy question. The authors develop a new econometric approach to aggregating high-dimensional factorial designs in RCTs in order to identify the most effective policies, and apply it to the question of increasing childhood immunization rates. I am thoroughly impressed by every aspect of this paper: the dataset used, the specific treatment arms used, the policy relevance, and the approach to identifying the best policy.","
This is an absolutely superb paper tackling a hugely important policy question. The authors develop a new econometric approach to aggregating high-dimensional factorial designs in RCTs in order to identify the most effective policies, and apply it to the question of increasing childhood immunization rates. I am thoroughly impressed by every aspect of this paper: the dataset used, the specific treatment arms used, the policy relevance, and the approach to identifying the best policy. The overall takeaways of the paper are not only of particular policy relevance but also provide key insight into the economics of reminders.
A key strength of this paper relative to other papers in immunization is the outcome dataset. Most papers use parents’ self-reports of vaccination status, whereas this paper critically creates a new administrative database to accurately record immunization activity in both treatment and control. This is particularly important when evaluating treatments such as reminders because when parents self-report vaccinations, this kind of treatment may impact reporting itself irrespective of latent treatment effects on actual immunization
Banerjee et al. (2010)[undefined] is one of the most important and influential papers in this literature, illustrating the efficacy of incentives in increasing immunization rates. Another component of this current paper which I found very compelling was that it examines the key policy questions that remained following the results of that initial paper regarding the level of incentives over the course of the immunization course.
Another major gap in the literature that this paper addresses is on the role of SMS campaigns in preventative health. There is wide variation in the treatment effects from these types of messages, and it has thus far been unclear as to the reason why: is it necessary to have a famous Nobel Laureate deliver the message via video, for instance, as was done by some of the authors during COVID? One natural explanation for the heterogeneity in treatment effects is that SMS messages exhibit the kind of complementarity that is at the core of this design.
This paper is also impressive in the scope of its applicability. Not only is Haryana a huge state with many unvaccinated children, but the interventions are easily applicable to other states in India, and in fact other low-income countries. With the near universality of mobile phones across the developing world, there is no particular aspect of this treatment that does not apply to other settings. What’s more, even if there is a concern, the overall econometric method here is certainly portable, so even if the specific method for selecting ambassadors, for instance, needs to be modified given constraints on surveying from a census, tweaks on any specific arm can be easily incorporated into a near-replication of this study.
My only real feedback is that I think the point that the authors make that immunization is so cost-effective, it is still probably the best use of money to fight childhood disease even if there are many payments to inframarginal households, should probably be emphasized more and earlier on (perhaps in the introduction), as that really shapes what the bottom line take-away is about the best policy from this paper. Relatedly, the question of whether there may be long-run persistence in these policies is in some ways not especially relevant: even if there is zero persistence year to year within a village (which for the record I doubt), it is still so cheap relative to the benefit of immunization that these policies are well worth it.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",98,96,100,,,,97,100,95,97,95,100,96,93,100,96,92,100,100,100,100,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",5,5,5,5,5,5,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",I have been in this field about [range coded 7-10 years.],I have reviewed about 30 articles for academic journals.,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Selecting the Most Effective Nudge: Evidence from a Large-Scale Experiment on Immunization,https://unjournal.pubpub.org/pub/evalsumeffectivenudge/,Econometrica,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-04T16:16:08.271-04:00,
Does Conservation Work in General Equilibrium,https://vsalazarr.github.io/jobmarketpaper/,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Evaluator 1,,"This paper investigates the impacts of conservation policies on deforestation, […] explicitly [taking] into account indirect general equilibrium effects. Intuitively, protecting land in certain locations affects local land prices, wages, and agricultural output prices that may lead to deforestation elsewhere, given that regional markets are interconnected – this is known in the literature as ‘leakage.’ Though widely acknowledged as conceptually important, the existing empirical works have not yet explicitly dealt with this issue, except via ‘reduced form’ spillovers, focusing on regions that are adjacent to protected areas (PA). This ‘traditional’ approach, though informative, is not fully satisfying as spillovers may affect non-adjacent regions as well. As I see it, this is a crucial gap in the literature that this paper aims to fill. That is an important contribution.
To do so, the authors develop a quantitative spatial equilibrium model of the Brazilian economy. They find that targeting protection in the regions with highest deforestation levels can be effective, despite the fact that leakage increases significantly in a longer time-horizon. Specifically, after one year, 2-3% of the deforestation reductions are outdone by leakage, which is pretty small. Although this number increases to 10% after 10 years, it is still sufficiently small in my estimation to justify protecting the land.","This paper investigates the impacts of conservation policies on deforestation, […] explicitly [taking] into account indirect general equilibrium effects. Intuitively, protecting land in certain locations affects local land prices, wages, and agricultural output prices that may lead to deforestation elsewhere, given that regional markets are interconnected – this is known in the literature as ‘leakage.’ Though widely acknowledged as conceptually important, the existing empirical works have not yet explicitly dealt with this issue, except via ‘reduced form’ spillovers, focusing on regions that are adjacent to protected areas (PA). This ‘traditional’ approach, though informative, is not fully satisfying as spillovers may affect non-adjacent regions as well. As I see it, this is a crucial gap in the literature that this paper aims to fill. That is an important contribution.
To do so, the authors develop a quantitative spatial equilibrium model of the Brazilian economy. They find that targeting protection in the regions with highest deforestation levels can be effective, despite the fact that leakage increases significantly in a longer time-horizon. Specifically, after one year, 2-3% of the deforestation reductions are outdone by leakage, which is pretty small. Although this number increases to 10% after 10 years, it is still sufficiently small in my estimation to justify protecting the land.
Main comments
My overall assessment of the paper is positive. It addresses an important problem, it uses appropriate data and modeling techniques, and is well written. I believe it should be published in a good journal. My goal here is to help the authors increase the chances of publication.
My main comment is that the paper could be much more ambitious. It could investigate further implications of preservation policies that would be important for policy makers.
The paper could convert the counterfactual deforestation figures into avoided carbon emissions, linking its main results to the opening paragraph of the article. There are alternative carbon maps available that could be used to that end.
Armed with the carbon emissions numbers, they could calculate the social benefits of PAs [protected areas using the social cost of carbon, which is also available in the climate economics literature.
In addition, given the complete general equilibrium model used, the authors can calculate the welfare impacts everywhere in Brazil. Quantifying Brazilian social welfare should be interesting in itself, and could be used to compare the local benefits/costs of conservation with the international gains from reduced carbon emissions. Is it the case that PAs benefit the world at the expense of Brazilian farmers, workers and consumers? And if so, how much the world could compensate Brazil for its environmental policies?
These are important questions that matter for conservation policies and international agreements; that can be addressed by the paper; and that can make the paper much stronger and influential. Further, the authors can mention that their methodology can also be used to investigate environmental policies in other important contexts, such as in Indonesia and Congo.
Specific comments
The introduction misses a discussion of the main quantitative findings of the paper.
Section 2 misses a description of the data sources and the main variables used in the paper. It should also present some summary statistics to give the reader a sense of magnitudes. The unit of analysis (microregions) and the sample period (2003–2019?) should also be clear early on.
I struggle to see the importance of Sections 2.2. (Econometric Specification) and 2.3 (Results). Though they provide some useful information (covering event-studies for the Priority List, and regression discontinuity for protected areas), they are not central to understanding the results of the spatial general equilibrium model. In fact, the equilibrium model points to the failure of these empirical strategies to identify the causal impacts of these environmental policies. As such, I do not see good reasons to spend so much time on these strategies.
A good rule of thumb is to maintain in the main paper only the information that is crucial to understand its main findings. Everything else either can go to the Appendix or can be cut entirely from the article. I recommend moving Sections 2.2 and 2.3 to the Appendix and refer to them briefly in the main text.
Section 2 should include, and expand, the discussion of the stylized facts (current on page 19) that the equilibrium model will try to incorporate.
In terms of the model presented in Section 3, I have two main substantive questions/comments:
Brazil is the main worldwide exporter of soy and beef. Does the model incorporate an export sector or consider a completely closed economy? If the economy is assumed completely closed, I would recommend including an export sector into the model.
The model comprises the entire Brazilian economy, but some assumptions give the impression of being motivated by the Amazon rainforest alone. For example, assuming open access to land seems reasonable for the Amazon, but much less reasonable for the rest of the country, which has a well-consolidated (and highly competitive) agricultural sector. One possibility – if feasible – is to model the Amazonian region differently than the rest of the country.
Minor comments
Page 2: “Since approximately three-quarters of global deforestation is driven by agriculture…” What is the data source for this statement?
Page 3: Where is the arc of deforestation in Figure 1? It might help to have a map of the biomes somewhere in the paper (maybe in the Appendix).
Page 3: What is the fraction of protected areas in the Amazon, and in Brazil in general?
Page 4: In the paragraph “Typically,…” the authors can mention that leakage leads to the violation of SUTVA, motivating the use of equilibrium models.
Page 4: “Our model considers a multi-region economy…” of Brazil? Or of the Brazilian Amazon? Please clarify this early.
Page 7: the paragraph “Before turning to…” is too defensive. You can convey the same information without being so defensive.
Page 7: The paragraph “Reduced-form evidence…” is repetitive, you can cut it out.
Page 8: Here is the first time I see reference to “additionality.” This is an important issue in the literature of the impacts of PA, and should appear much earlier in the paper. Some PAs in Brazil seem to be placed in regions with no immediate threat of deforestation, leading to small or zero additionality, while others were put in places in which forests were at risk, leading to substantial additionality.
Page 10: “The graph below shows…” Which graph?
Page 13: Where are the synthetic difference-in-differences results?
Page 19: The paragraph “Mechanisms for leakage” seems repetitive. The mechanisms are already clear at that point in the paper. I would move this discussion to the main results section.
Page 19: Footnote 9 misses the definition of L_o.
Page 21: In “Model features” is worth explaining which are the $K$ agricultural commodities considered in the empirical exercise.
Page 21-22: I would include Table 3 at the end of the model description. One cannot fully grasp it before understanding the model.
Page 22-23: It is worth explaining the mechanics of Figure 11. Even better is to bring Figure 16 currently in the Appendix to the main text and explain it.
Page 24: There must be a typo here. It claims that “there is free entry of deforesters.” The free-entry condition leads to zero profit in equilibrium, i.e., q.Z.(I)^\delta-p.I=0. But equation (4) reflects the FOC of the profit maximization problem.undefined
Page 24: I found the explanation of the estimation of \delta and \psi confusing. It would be helpful to have the data sources and variables explained earlier, as suggested above. Discussing this estimation after presenting the demand side could also help.
Page 25: It is worth mentioning that labor and land are complements, which explains why when the price of land goes up, the demand for labor goes down, in spite of the substitution effect.
Page 20: I did not understand how the “wage and occupational choice” part relates to the rest of the model. For instance, by reading just this section, it is unclear how the z_i^A and z_i^{NA} relate to the TFP Z_r^{Ak} presented in the previous page.
Page 26: The economic discussion after explaining how \alpha is estimated requires knowledge of the consumers’ demand. It is therefore worth postponing the discussion to [put after you present] the consumers’ part, maybe in the results section.
Page 27: It would be useful to have some a-priori justification of the utility function specification used. And to provide the formulae more explicitly too.
Page 28: Some justification for taking \omega=9 is warranted.
Page 28: Some variables missed a proper definition when introduced, as for example X_d^g.
Page 29: Presenting the migration decisions (Section 3.5) after closing the model (Section 3.4) seems weird.
Page 46: Implied \delta is 0.26-0.29, but \delta in Table 3 equals 0.5. This is confusing, please check.
I missed the citation of a few important papers
“The Environmental Impacts of Protected Area Policy,” (2023, RSUE) by Reynaert, Souza-Rodrigues, and van Benthem.[undefined]
This is a recent overview of the empirical literature on the impacts of protected areas. It explicitly states that incorporating spatial general equilibrium effects is an important gap. The authors here can mention this fact to reinforce the contribution of their paper. They can also find other papers in the review that may be worth citing.
“Optimal Environmental Targeting in the Amazon Rainforest,” (2023, RESTUD) by Assuncao, McMillan, Murphy, and Souza-Rodrigues[undefined]
This paper focus on the Priority List and finds important complementarities between the list and the protected areas.
“An Evaluation of Protected Area Policies in the European Union” (2023, Working Paper), by Grupp, Mishra, Reynaert, and van Benthem.[undefined]
This paper uses state-of-the-art techniques (including the use of machine learning) to study the impacts of PAs in Europe (one of the rare papers focused on developed countries), and found negligible impacts there. Developing a general equilibrium model for Europe to capture leakage might therefore not be as important as for Brazil.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",70,60,80,,,,70,80,60,80,75,85,65,60,70,65,60,70,70,65,75,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4,4,3.8,4.2,3.6,4.4,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",[about 10 years],"“I review about 10-15 papers/proposals per year, on average.”",False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,"Relevance to GP: 68, 80, 92",,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Does Conservation Work in General Equilibrium?,https://unjournal.pubpub.org/pub/evalsumconservationequilibrium/,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-04T16:07:25.702-04:00,
Existential Risk & Growth,https://philiptrammell.com/static/Existential_Risk_and_Growth.pdf,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Ioannis Bournakis,,"The paper examines how hazard rates change with varying speeds of technological advancement, questioning the wisdom of slowing progress. It finds a trade-off between technological progress and safety as countries prosper under optimal policies, illustrating a Kuznets curve where hazard rates decrease post-technological advancement. Despite its critical inquiry into whether technology jeopardizes society, it overlooks integrating a benefit function with hazard assessment. This omission limits the ability to assess technology's overall impact. With adjustments, it merits publication in a 3-star journal, likely achieving acceptance after revisions.","General Assessment
The paper analyses the evolution of hazard rates when technological development occurs at different paces. As technological progress induces risks, the fundamental question posed by the authors is whether it would be meaningful to decelerate technological progress. The paper answers that as countries become richer under optimal policy, there is a trade-off between consumption and safety. Therefore, a Kuznets curve is observed, with the hazard rate falling after technology advances. My general comment is that the paper, undoubtedly, deals with a crucial question: whether technology threatens the global community. Despite the general relevance of this question, I think there is a misleading point in not modeling any benefit function along with the hazard function. Without considering both benefits and potential risks of technology within the same framework, it is impossible to identify whether technology induces existential catastrophes for society. A simple argument can illustrate this point. Technological advancements have contributed to environmental degradation, yet within the same technological evolution, we can create technologies that are more environmentally friendly and mitigate the negative impact on the environment. Not modelling the potential gains from accelerating or decelerating technological development is the biggest omission in the current approach. My overall assessment of the paper, based on my experience in the field, suggests that with some modifications, it should be suitable for publication in a 3-star journal (ABS ranking). I predict that after revisions, it will be accepted in a journal ranked at the 3-star level (either field or general).
Specific Feedback
Before providing feedback on specific categories, I have some additional comments:
• I found the title somewhat inaccurate. I think the word “Growth” should be replaced with “Technological Progress” or simply “Technology.”
• The discussion would benefit from more examples regarding trade-offs following accelerated technological progress.
• Further on the above point, the statement that civilization encounters the probability of a risk of an existential catastrophe due to technological progress is thought-provoking but should be elaborated with some examples. One needs to be more specific (preferably earlier in the paper) regarding the precise sources of this threat. Is it going to be from environmental degradation, an artificial virus emerging from lab experiments, or the continuous progress of artificial intelligence?-How plausible is the assumption that technology grows exogenously?
• Have the authors considered the case where 𝐴 induces a constant hazard, regardless of how quickly the increase occurs? This approach seems more straightforward and might also allow for including a technology-related benefit function within the same framework. This setup seems more realistic, offering a greater scope for policy design.
• The statement that technological growth ( 𝐴̇ | 𝐴 ) has been roughly constant over the last century must be backed up by references. It is difficult to treat this as a universal point without supporting evidence.
• On page 34, does this statement describe the case where the risks posed by two events occurring simultaneously are associated with certainty, while the sequence of events might induce uncertainty, as we do not know the exact intensity of the risk? “One valid criticism of this functional form is that it overemphasises a channel through which the risks posed by a series of technological developments can be cheaper to mitigate if they occur at once than if they occur in sequence.”
• On page 8, the statement “Unlike temporary accelerations, however, permanent accelerations can render survival possible when otherwise impossible. Shrinking a heavytailed curve with an infinite integral can yield a thin-tailed curve with a finite integral” has mathematical meaning but needs to be placed within a more intuitive context, possibly with some examples. For instance, what can be an example of a temporary acceleration of technological progress? In the context of artificial intelligence, how can permanent acceleration of AI render survival possible, while temporary acceleration cannot? What are the key characteristics of temporary acceleration that affect survival? These abstract concepts should always be placed in a context to become more understandable.
Specific Assessment
1. Overall assessment: 65%
The paper explores a topical issue: the relationship between technological development (or progress) and existential risk. This objective is crucial, as we need approaches that help us understand the risks induced by technological change. Recent developments in the global community, such as vaccines and the evolution of AI, fall within this agenda and certainly deserve a more systematic analysis. One remark I can make here is that the definition of existential risk is too broad and, in that sense, not entirely accurate. It is difficult to argue that some types of technological progress can threaten the very existence of humanity. This does not seem meaningful. However, the value and contribution of the present analysis are not significantly affected if the authors could be more specific when referring to the risks induced by technological progress. For example, the evolution of AI can induce many risks in various aspects of economic life alongside its undeniable benefits. The analysis would become much more meaningful if placed within this context. Similarly, how further technological progress can affect environmental degradation, which can indeed be an existential issue for humanity, should be explored. Overall, I would say that the objectives of the project are definitely crucial. It would be more useful to elaborate more on their application in specific contexts.
2. Advancing our knowledge and practice: 55%
One key message of the present analysis is that technological progress (or development, according to the authors’ terminology) follows a pattern similar to the Kuznets curve, where the hazard initially increases as technology evolves at a slower rate and then declines after reaching a certain point beyond which technology progresses faster. This statement seems somewhat arbitrary, and I am not convinced that this thesis adequately describes the relationship between technological evolution and hazards after reading the entire paper. I do not want to argue that this thesis of authors is invalid, but I would find it more appropriate to assume a constant hazard independent of how quickly technology evolves. After all, to understand the role of technology one needs to consider that risks coexist with benefits.
I found it challenging for a theoretical framework like the one provided in the paper to robustly establish this pattern. Whether technological progress and hazard exhibit a monotonic relationship is more a matter of empirical scrutiny, which is not the scope of this paper. Additionally, the broad definition of existential risk makes it even more difficult to justify the Kuznets relationship. For example, according to the authors' approach, the evolution of mRNA vaccination technology initially induces proportionally more risks, which then decrease as the technology standardizes without too many new developments. However, the crucial question is whether the benefits outweigh the risks of this technological evolution at all stages and all paces of development. One could argue that despite the temporary acceleration of mRNA technology, the benefit rate has been greater than the hazard rate. What is the view of the authors on this? Therefore, even a temporary acceleration of technology can be beneficial on net, without implying that survival becomes impossible, as outlined in Section 2.
Overall, I think the analysis lacks a function that accounts for the potential benefits of technological progress. While there is a systematic analysis of the evolution of the hazard rate, the model should also incorporate the rate of benefits. The crucial point is to develop a framework that improves our understanding of which outcome prevails in the state of technological progress and under which conditions. I believe this is a significant omission.
3. Methods: Justification, reasonableness, validity, robustness: 85%
This paper provides a theoretical framework that is rigorous, coherent, and well-executed. While there may be objections regarding the empirical validity of some propositions, the analysis is thorough, and all propositions are mathematically proven, indicating the internal consistency of each argument. Certainly, there is further scope for robustness analysis, especially if the authors could have guided how to empirically test some of these hypotheses. I recognize that this is not the scope of the present work, but perhaps the authors could add a few sentences indicating future research directions in this area.
4. Logic and communication: 70%
Section 3 assumes that technology evolves exogenously. For the sake of simplicity, this assumption is convenient. However, it is difficult to accept that investment in new technological advances is driven solely by external factors, rather than by firms' needs to develop products that meet specific demands and capture new market opportunities. Assuming an endogenous approach to technology would make it easier to understand my earlier point that technological evolution is often driven by firms responding to market needs and opportunities. Furthermore, it's important to explore how the benefits of technological 5 advancements often outweigh potential risks and threats. This aspect deserves more detailed discussion, especially in the conclusions of the paper. Some parts of the discussion, particularly in the conclusions, do not always convey a clear message and could benefit from further elaboration.
5. Open, collaborative, replicable science: 75%
The analysis posits a theoretical explanation of whether technological development can lead to existential risks. This is a strong assertion that lends itself to theoretical testing, though empirical validation is often less straightforward. As I already suggested, the authors could examine specific instances of technological advancements in particular domains to ascertain whether the hazard rate outweighs the benefit rate and whether technological evolution has indeed elevated society to a higher steady-state condition. Such an approach would facilitate the replication of the theoretical propositions in practical scenarios. In the current context, it would have been beneficial if the authors had proposed how the present model could lead to a reduced-form equation that serves as a benchmark for empirical formulation. I remain skeptical about the feasibility of this suggestion but believe it warrants consideration.
6. Real World Relevance: 68%
The present analysis addresses a critical real-world issue: whether technological progress induces existential risks. Given the multitude of technological challenges we face today, this question is crucial. However, a significant limitation of the analysis is its high level of aggregation, which often hinders the identification of specific implications for different sectors.
7. Relevance to global priorities: 60%
The analysis touches upon global priorities concerning the importance of policy interventions to mitigate hazards arising from technological progress. Yet, the paper falls short in providing clear policy implications or defining specific domains for potential policy initiatives. One clear implication from the analysis is the need for varying levels of policy intervention tailored to the technological development stage of each country. For instance, wealthier economies experiencing accelerated technological progress may face lower risks, whereas less developed countries with slower technological advancement are at higher risk and thus require more intensive (or targeted) policy interventions. However, the paper lacks 6 specific recommendations on optimal policy measures. I believe the analysis would benefit from including a section outlining indicative points for a policy intervention roadmap tailored to different technological regimes (accelerated vs. decelerated)","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",60,68,70,,,,85,90,80,60,55,65,70,65,75,63,60,70,65,60,72,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",3.5,3.5,3.2,4,3.2,4.1,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",About twenty years in the field of economic growth.,More than 100 for journals of various standards.,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),"The paper addresses the critical topic of how technological development intersects with existential risks, highlighting the need for nuanced definitions and specific contextual applications.","Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,"This paper provides a theoretical framework that is rigorous, coherent, and well-executed.","To understand the role of technology one needs to consider that risks coexist with benefits. This approach does not exist in the present approach, I found this the biggest shortcoming.",The assumption of an exogenous technology is simple and convenient but not always realistic.,I remain skeptical about the feasibility of testing empirically the proposition of the present analysis.,"Relevance to GP: 65, 70, 75 - A significant limitation of the analysis is its high level of aggregation, which often hinders the identification of specific implications for different sectors.","Top B-journal/Strong field journal
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Existential risk and growth,https://unjournal.pubpub.org/pub/evalsumexistentialrisk,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-04T15:59:16.884-04:00,
Existential Risk & Growth,https://philiptrammell.com/static/Existential_Risk_and_Growth.pdf,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Seth G. Benzell,,"Authors find: (1) existential risk follows an inverted-u shape in both technology level and over time, (2) technological accelerations reduce cumulative existential danger.
These findings are novel and important, implying a “time of perils” after which safety is (asymptotically) assured.
Two critiques: (1) the representative agent of the model is neither a normative goal nor a positive prediction, making implications nebulous. (2) omission of savings from the model eliminates a potential mechanism for intertemporal consumption smoothing, which may impact demand for safety.","Executive Summary:
Authors find: (1) existential risk follows an inverted-u shape in both technology level and over time (2), technological accelerations reduce cumulative existential danger. These findings are novel and important, implying a “time of perils” after which safety is (asymptotically) assured. Two critiques: (1) the representative agent of the model is neither a normative goal nor a positive prediction, making implications nebulous (2) omission of savings from the model eliminates an alternate mechanism for intertemporal consumption smoothing
Full Review
Determining whether and when anthropocentric existential risks will actually bite is a difficult question for several reasons. One reason is the difficulty in estimating the ‘existential risk damage function’. Another is human agency - depending on our incentives we might consciously work to reduce risk and increase safety.
This paper is focused on the second question. It investigates the dynamics of economic growth existential risk, assuming simple but plausible analytic forms for output, existential danger, and representative agent utility. It highlights that, under the assumption that risk aversion is strong: (1) existential risk follows an inverted-u shape in both technology level and over time (2), technological accelerations reduce cumulative existential danger.
I found the paper well written and generally easy to follow. I especially appreciated the way that section 2 previewed the main results in a mathematically accessible way. The paper anticipated my desire to think about a less risk averse planner in section 3.3.4, and a humble discussion of parameter selection in appendix A1. I will say I found section 4 somewhat extraneous -- although I appreciated the intuitive result that if dot(A) directly and immediately increases danger, with consumption benefits lagging, that dot(A) increases can be dangerous.
I do have some critiques of some aspects of the model and its discussion, but overall I find it to make an important contribution to our understanding of the relationship between economic growth and existential risk.
Why do we care about this particular representative agent? Distinguishing the normative and positive content of the model.
From the perspective of any potential optimizing representative agent, an increase in A is always a good thing for welfare (by beta > alpha). The key words being “from the perspective of”. The paper’s implicit normative framing is: An existential catastrophe would be exceedingly bad and we should prefer decision rules that don’t produce it. In other words, the paper doesn’t take the goals of the representative agent as necessarily normative.
On the other hand, the paper only weakly argues that the representative agent is anything like a positive model of how AI risk decisions will be made in the real world. While the argument that the wealth induced by AI will decrease the marginal propensity of consumption and induce substitution into safety is a generally robust one, the details of “who’s marginal propensity to consume”undefined is left open -- is this the MPC of the median voter in the US? Of the President? Of the average person globally? The same question holds for the discount rate, which is also heterogenous. Similarly, “Race” dynamics are likely to be important between leading labs and countries (Aschenbrenner, 2024)[1], which would also be hard to model with a single representative agent. This disconnect between the representative agent and the real world is worse than the one in Jones (2016), which has a similar model, but is concerned with individual mortality.
I think the paper could do a better job explaining why these particular growth/risk paths are interesting if they are neither a positive prediction nor a normative imperative. I think the authors would say that the representative agent represents a sort of “idealized international institution” that a sane hegemon would plausibly sign up the world for (perhaps in the wake of a successful “The Project” (Aschenbrenner, 2024)[1]).
Whether or not that’s the case, in follow-up work it might be useful to evaluate or contextualize the social planner analyzed according to other explicit social welfare functions that might be normatively endorsed. For example:
A “hard” utilitarian might choose a gamma with perfect substitution between consumption across periods – so no decreasing marginal product of consumption -- and a discount rate equal to the unpreventable existential risk rate
This agent would view the representative agent as much too conservative and risk averse
An agent who is only trying to minimize existential risk, perhaps conditional on some minimum consumption level
This agent would view the representative agent as much too incautious
Is Leaving Capital Out of the Model Harmless?
A second question I have about the modeling is how harmless the assumption of ignoring capital accumulation is. I have one concern and one comment related to this.
The concern is that some of the results may be driven by the fact that “decreasing safety” is the only way to increase immediate consumption in the model. In real life, we also have the choice of reducing the saving rate. This additional mechanism might weaken the papers’ findings: a shock to dot(A) increases future wealth in a way that only incentivizes more safety in the current model. In a more general model with savings, an increase in dot(A) could instead lead to dissaving in the short term to smooth consumption between periods, and less of an increase in safety spending. (I could also see it pushing in the opposite direction, making future existence more valuable because of the ability to intertemporally substitute with savings).
A more minor point is that the capital-labor distinction is one of the major ways that heterogeneity in preferences complicates the representative agent framework. The advance of AI is likely to increase the share of income paid to capital and reduce the share paid to labor, as well as redistributing income between types of workers. Likely, those who own capital may have lower risk tolerances than those who provide labor (they are both richer, and may also be more patient in general -- although a lifetime savings model would suggest, alternatively, that the richest agents might be on the verge of retirement and relatively impatient). This is just one mechanism by which technology may itself shape the preferences of a representative agent, but I think an important one (Benzell et al 2023)[2].
Minor Comments:
The authors argue that historical anthropocentric existential safety expenditure has been near 0. I question this. If I view my enemy as completely alien to my values, wouldn’t military spending count? One historical theory I have in mind is that the Habsburg Empire intentionally slowed economic growth and increased military spending to avoid a socialist revolution. An even older example could be the prophecies of Jeremiah and similar prophets who connected contemporary decadence and evil (the high consumption of the ruling class?) to a coming disaster, and recommended shifting resources to ameliorating religious rituals.
While this paper draws on Jones (2016)[3] it does so much less than the previous draft, and does a much better job highlighting innovations vs. replicating that paper. Relative to Jones (2024)[4] this paper is less precise in distinguishing the effects of degrees of risk aversion, but has richer dynamics. My critiques mostly apply to Jones (2024)[4] as well.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",95,80,99,,,,90,95,75,80,50,100,90,85,95,95,90,99,80,50,100,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4.5,4.5,4.2,5,3,5,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",8 years,4,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,"I am mostly concerned about the high-risk aversion selected as focal, as well as robustness to allowing for saving. There is also the general background concern that AI safety research might be completely ineffective.",,,,"Relevance to GP: 60, 80, 100",,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Existential risk and growth,https://unjournal.pubpub.org/pub/evalsumexistentialrisk,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-06-04T15:51:39.010-04:00,
Does Online Fundraising Increase Charitable Giving? A Nationwide Field Experiment on Facebook,https://pubsonline.informs.org/doi/full/10.1287/mnsc.2020.00596?casa_token=h9qjBpiQEpkAAAAA%3AGe0xmoR1ZSbGgmJBNswWR9m686q9OS8014Jrs1k_QmJEuEyVQPzWgVmGv5Sj9uGy9L_eE17R9my2,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Tabaré Capitán,tabare,"This is a large-scale, well-executed field experiment on Facebook ads for charitable giving in Germany. Randomized at the postal code level, the study finds that the campaign increased donations both short- and medium-term, without reducing future giving to the same charity—but possibly crowding out donations to others. Content and delivery method had no differential effect. The design is strong and materials are fully replicable.","# Evaluation
## Title
Does Online Fundraising Increase Charitable Giving? A Nationwide Field Experiment on Facebook
## Evaluation summary
This is a high-powered field experiment with little room to improve. In partnership with a charity in Germany, the authors designed and deployed a campaign in Facebook to increase donations to the partner charity. They were able to reach, essentially, all of Germany, at the postal code level. They randomized postal codes into five groups. One group was a control. The next four groups come from a 2x2 factorial design, varying the content of the (video) ad and the way in which the ad was shown to Facebook users. Given randomization, simple comparison of means between the control and pooled treatment groups yield causal estimates of the effect of the campaign. The key result of this paper is that the ad increased revenue both during the campaign and in the subsequent five weeks. Importantly, the ""long-term"" increase, provided the well-timed deployment around the holiday season, suggests that the increased donations did not come at the expense of future planned donations to the same charity. However, the same does not hold true for other charities. External data suggests that the increased revenue crowded out donations to other charities. Furthermore, using the 2x2 factorial design, the authors show that neither the content of the video nor the impression assignment strategy make a difference in the overall treatment effect. 
The experimental design, methods, and robustness checks are appropriate; making their causal estimates credible. Furthermore, the authors provide a comprehensive supplement and replication materials. The replication material detail how to obtain the data and it is indeed accesible; the code is well organized and easy to use. In the remaining of this evaluation I list the main claims of the paper, as well as some lingering concerns or issues. 
## Main claims
1. Campaign increased donation revenue and frequency during the campaign and in the subsequent five weeks.
2. The campaign was profitable for the fundraiser. 
3. The effects were similar regardless of the content of the campaign or the impression assignment strategy.
4. The campaign crowds out donations from other charities.
## Concerns
### Is the campaign profitable?
### Comment on Profitability Estimates
The paper argues that the campaign was profitable, which is a key practical claim and a major justification for scaling similar interventions. Based on point estimates, this appears plausible: the immediate return per euro spent is estimated at €1.45, rising to €1.75 when incorporating assumptions about future donations from new recurring donors, and up to €2.53 when including revenue from all donors exposed to the ad.
However, the claim rests on assumptions on top of significant uncertainty, especially in the long-term estimate. The 95% confidence interval for the €2.53 ROI is wide—ranging from €0.15 to €2.74. While point estimates suggest the campaign may be profitable, the interval includes values that wipe out profitability. In hypothesis testing, we would treat the inclusion of zero in a confidence interval as evidence of no statistically significant effect. By that logic, the inclusion of low revenue values  in the CI should raise similar caution. For example, €5000 in revenue is within the CI, as opposed to the €47726 point estimate used to calculate their most conservative ROI of €1.45. At most, the paper suggests that an online fundraiser may be profitable, but it is a risky move.
Moreover, the analysis assumes a “lifetime” value for recurring donors based on short-term behavior: about 30% of new donors chose recurring donations. But without data on donation persistence, the assumption of lifetime giving is strong. If recurring donations last for only 1–2 years, the actual return could be substantially lower. Additionally, the extrapolation assumes that each donor’s recurring amount equals their initial donation adjusted for discounting, which may not reflect actual donor trajectories, particularly if influenced by platform defaults or one-off generosity.
Finally, the suggestion that profitability could be increased through targeted campaigns is reasonable but incomplete. Targeted ads are typically more expensive under Facebook’s auction model, especially when targeting higher-value individuals. If ad pricing internalizes the predicted return per user, the margin for increased profitability may be limited or negative. It would be constructive for the authors to acknowledge that the ROI under algorithmic targeting depends on both improved efficiency and potential increases in ad costs—something not currently addressed.
In sum, the campaign appears profitable by point estimate, but the confidence intervals and strong behavioral assumptions warrant a more cautious interpretation. Highlighting this uncertainty explicitly—especially in the abstract and introduction—would enhance transparency without undermining the value of the findings.
### How external is the external validity?
The external validity of the findings within Germany is strong by design, as the sample covers essentially the entire country and includes a large, heterogeneous population of potential donors. However, generalizing these results to other national contexts—especially outside Europe—requires more caution.
Donor behavior is shaped by cultural norms, institutional arrangements, and the structure of the nonprofit sector. For instance, in the United States, charitable giving is more prevalent as a share of GDP and strongly incentivized through tax deductions. In contrast, Germany (and much of continental Europe) tends to rely more on state-supported social services, with relatively less individual-level charitable giving and weaker tax incentives.
This implies that U.S. donors may respond differently to online fundraising efforts, both in terms of volume and substitution patterns. Additionally, competition among nonprofits is more intense in the U.S., and digital fundraising campaigns are more common and professionally managed. These factors could affect both the baseline responsiveness to ads and the degree of donor substitution between organizations.
To inform external validity, the paper could (have) usefully reflect on how Germany compares to other contexts along key institutional and behavioral dimensions: tax treatment, norms around giving, fundraising saturation, and digital engagement. Comparative evidence on donor behavior (e.g., Bekkers & Wiepking 2011; Andreoni 2006) suggests that motivations such as empathy, perceived impact, and social signaling are broadly shared, but their relative strength may vary.
### Randomization is a process, not a result
Table A1 is titled “Results of Randomization,” yet what is shown is not evidence of whether randomization occurred, but rather covariate balance between treatment arms. Randomization is a design property—it either happened or it did not—and cannot be inferred from the observed balance in covariates. As such, interpreting covariate balance tables as a test of randomization is conceptually incorrect.
Moreover, the use of t-tests to assess balance is misleading. $p$-values are a function of both effect size and sample size. As the number of observations increases, even trivial differences in means can become statistically significant. Conversely, with small samples, substantial imbalance may yield high $p$-values. This results in the paradox where increased sample size suggests “worse” balance due to lower p-values, despite the imbalance remaining constant.
See Freedman (2008) for a formal critique:
> Freedman, D.A. (2008). \""On regression adjustments to experimental data.\"" *Advances in Applied Mathematics* 40(2): 180–193.
Freedman shows mathematically that balance tests do not provide evidence for or against randomization, and that regression adjustments (including t-tests) post-randomization can introduce bias in finite samples. (Which is admittedly not the case for this paper). The key insight is that under random assignment, any observed imbalance is due to chance, and no correction or test is necessary to “validate” the assignment.
A more appropriate approach is to report standardized differences in covariates without relying on hypothesis tests. This allows readers to assess the magnitude of any chance imbalances without conflating statistical significance with practical importance. Imbens & Rubin's book on causal inference provide guidance in this regard.

## References
Andreoni, J. (2006). Philanthropy. _Handbook of the economics of giving, altruism and reciprocity_, _2_, 1201-1269.
Bekkers, R., & Wiepking, P. (2011). A literature review of empirical studies of philanthropy: Eight mechanisms that drive charitable giving. _Nonprofit and voluntary sector quarterly_, _40_ (5), 924-973.
Imbens, G. W., & Rubin, D. B. (2015). _Causal inference in statistics, social, and biomedical sciences_. Cambridge university press.
","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",92,85,95,85,80,90,97,99,95,90,80,95,92,90,95,97,93,100,,,,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4.8,,4.6,5,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",10+,10+,True,3-4 hours,,Yes,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.","Abstract: However, we also found some crowding out of donations to other similar charities or projects.
Table 3 shows a negative change in revenue in 23 other similar charities in response to the exogenous variation created by the campaign. 

It is particularly important because, according to the authors, there is a belief in this literature that charities are complements. In other words, a new donor is good news for everyone. In contrast, this result imply competition. If this is the case, then any funds spent on fundraising may be simply re-distributing a fixed cake (and making it smaller). 
",,"Like 90%. They don’t have the same level of data quality as their main dataset, but it is very comprehensive.",,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,"I am not familiar enough with this field for my answer to matter. Based on what the authors claim, big contribution.",,,,It is published in Management Science. I think it could have been top 5.,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,"Yes. But again, the paper is already published. I’m pretty sure the authors have moved on.",,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",Experimental and behavioral economics,Does online fundraising increase charitable giving? A nationwide field experiment on Facebook,https://unjournal.pubpub.org/pub/evalsumfundraisingcharitablegiving/,Management Science,"50_published evaluations (on PubPub, by Unjournal)",,,2025-05-30T13:11:30.158-04:00,3.5
Water Treatment and Child Mortality: A Meta-analysis and Cost-effectiveness Analysis,https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4071953,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Evaluator 2,,"1. I do not see any major technical issues with the paper.
2. The paper can be positioned more robustly i.e., the “need for it” needs to be better argued.
3. While the “effect” (mortality reduction) side of the paper has been done well, the “cost” part of the paper could be deepened. This would position it better and be supremely useful to policymakers.","Summary
A truly needed and useful paper. I commend the authors on what they have produced.
The authors conduct a meta-analysis of mortality reduction from water chlorination. They use a clear procedure to discover and include studies into their analysis, restricting to randomized controlled trials for ease of inference. They use both frequentist and Bayesian models to derive odds-ratios (OR) for reductions in child mortality from a variety of water chlorination programs. The find a ~24% reduction in mortality from chlorination. This is a high-quality result with careful inclusion of studies and an extensive sensitivity analysis. With this estimate in hand, they conduct a cost-effectiveness analysis of three water chlorination interventions: chlorine dispensers in Western Kenya, in-line chlorination in India and an MCH based delivery for a general global context. They find that MCH delivery is the most cost-effective while dispensers and in-line chlorination are relatively more costly, though all three interventions are cost-effective compared to WHO benchmarks.
The summary of my comments is:
I do not see any major technical issues with the paper.
The paper can be positioned more robustly i.e., the “need for it” needs to be better argued.
While the “effect” (mortality reduction) side of the paper has been done well, the “cost” part of the paper could be deepened. This would position it better and be supremely useful to policymakers.
Major Comments
I have two major comments.
1. Study framing
My first comment has to do with positioning this study. Right now, it is positioned as serving the need for evidence on the cost effectiveness of water chlorination on reducing mortality. While I am personally very partial to this and think it is worthwhile, I suggest that the authors argue the premise more convincingly.
There is no doubt the quality of evidence in this paper is excellent. It is a rigorous, high-quality update since the last such meta-analysis. But it is one among manyundefined and I find it a little hard to believe that policymakers—both globally, and at the national and local levels in the health sector—are unconvinced that clean water is a top choice when allocating their funds. Are policymakers still genuinely skeptical about the cost-effectiveness of chlorination to reduce mortality, especially child mortality, in a low-cost manner?
That many policymakers do not actually allocate as much as they “ought to” is more the subject of a political economy analysis and not down to irrational choices on their part. Just because they are not implementing chlorination programs in a concerted, robust and enduring way does not mean they do not accept the premise – they are constrained in many other ways which may prevent this. Clean water has been part of global health policy focus for decades, everyone recognizes it as a high priority investment area. So rationalizing it—yet again—is likely not the issue holding back deployment of water treatment interventions.
Let’s accept that there is little to no need to convince anyone that water chlorination is absolutely the right thing to spend their money on to prevent illness and death. Given this, I would urge the authors to think a bit more about positioning this study.
One thought in this regard is that this study could be made more about which water chlorination intervention is best suited for a given context. In other words, going a bit beyond what this study is currently doing. To explicate this, right now we have three interventions in three unique contexts – dispensers in Western Kenya, in-line chlorination in India and an MCH based delivery for a general global context. What if I was a Kenyan health policymaker and saw this study:
I see that the MCH based chlorination is the most cost-effective. I think to myself maybe that is the first step I should take. But then I note that the estimate is for a generic global program, so now I am unsure. My MCH program is pretty robust and I know my costs probably fall on the lower end of the global distribution but I can’t be sure. Maybe I should just take the Kenya specific result for dispensers and be done with it?
I have just played out a hypothetical here. The thing that happened “well” is that a policymaker compared two possible ways to achieve a child mortality outcome through water chlorination – the primary purpose of a CEA like this – but what was weak in the above was that MCH does not have a truly local, Kenya-specific cost-effectiveness estimate that the policymaker could use, or no information that helps them get close to it. And this is true for an Indian policymaker, and other policymakers in lower-middle income and low-income countries. This relates to my second major comment below i.e., a way to better position this work
2. Deepen the cost modeling
My second major comment has to do with the “cost” part of cost-effectiveness: specifically, to deepen the cost analysis. I think it is important to add some useful sophistication to the cost model, while keeping it comprehensible. Being more thoughtful about cost modeling and analysis has begun to take hold among policymakers and donors.undefined Deepening the cost part of this analysis offers one way for the authors to better position this paper.
The first layer of sophistication that can be added is to build out a cost model for each intervention which allows some variation. A full, high effort version is to:
Build a cost model that explicates the key cost elements e.g., materials, management, communications, transport and training, of a given intervention.undefined
Then vary those elements – much like a sensitivity analysis – to get a variety of possible costs for the intervention.
Then, calculate a variety of cost-effectiveness estimates.
This enables a policymaker to “locate” their specific context in the cost distribution for a given water chlorination intervention and look up the relevant cost-effectiveness for it.undefined A simpler way to generate a range of costs might be to develop a low, middle and high cost for dispensers, in-line and MCH then use those to derive a range of CEAs. Policymakers, readers and anyone else interested in this, will be able to locate their costs on the spectrum – maybe a policymaker has good information that—for dispensers let’s say—their context is close to the medium cost, therefore they will then use CEA estimates for the medium cost scenario. Sohn et al (2020)undefined show how you can think a bit more deeply about modeling costs.
The second layer of sophistication is bounding/confidence intervals which might allow a decision maker to see beyond just the “mean case” and understand the extremes. For instance, in-line treatment might have lower mean cost effectiveness but the “worst case” (upper bound on cost per unit effect) for dispensers might be lower than that for in-line which may drive a risk averse decision maker to pick dispensers for their context. I do not have any great examples for you but you could think along the lines of what Wakker and Klaasen (1995)undefined suggest.
Minor Comments
General
Focus on CEA: I appreciate that you want to acknowledge the broader benefits of these interventions i.e., your calculation of net benefits. I think this is kind of distracting to be honest. Focus on the CEA and make it about choosing the best water chlorination intervention variant that yields the lowest cost per reduction in child mortality.
Relatedly, in your main manuscript, section “Cost-effectiveness”, you do not state the cost per death averted (this was available in a previous version of the paper). I understand the switch to cost per DALY averted but the major thrust of your quantitative analysis is mortality. Are there strong reasons not to have that? Are comparisons not possible with cost per death averted? Is it possible to re-introduce this?
Timeframe for inclusion of studies: The publication dates of the studies used span a 23-year period, from 1998 to 2021; one can assume that actual implementation lagged publication. Page 17 “Search strategy and selection criteria” says you used search criteria from past meta-analyses and updated to also include studies “…from February 2016 to May 2020”. Looking at the search strings in Table S1, the search set titled “Limits”, in which the search window seems to be 2012 to 2016, except for Ovid which is 2012 to “current”. This is all a bit unclear.
Can the authors:
State in very clear terms what their full search window was?
State why they chose this window, i.e., why start at a specific date (oldest you allow) and why end at a specific date (latest you allow)?
The second of these questions also relates to the age of some of the studies included i.e., the oldest being over 23 years old. Is it alright to allow studies that are over two decades old in the analysis?undefined Supplementary Materials>4. Cost-effectiveness analysis>Drivers of cost-effectiveness states that baseline mortality is a major driver of cost effectiveness. One would assume that over time, health outcomes are improving in LMICs and LICs i.e., bumping the baseline levels of mortality downward.
Timeframes of studies: The studies have large variation in terms of follow up i.e., when outcomes of interest were measured (see unnumbered table at the end of Supplementary Materials>1. Details of included studies and comparison with other RCTs). This is partly addressed by the authors in the sensitivity analysis by showing that studies with shorter timeframes are not driving results.
However, this does raise the conceptual concern about what is meant by a water chlorination program in terms of timeframe, especially as it relates to the cost effectiveness. Currently, a five-year timeframe for implementation of the three representative interventions is used. That sounds reasonable but is there any reason (a) that we have the same horizon for all three (I can imagine something like in-line chlorination programming lasting longer than MCH or dispensers but I can imagine it the other way too) and (b) that we limit to thinking about a five-year horizon i.e., it may be that there is variation in cost-effectiveness based on differing time horizons (two, three, five, 10 and 15)?
I think some language to explain why a five-year horizon was chosen would be good to have.
Study weights: For the weights provided in Table S4, we are not given a description for the weighting scheme used for the frequentist model but are for the Bayesian model (2. Meta-analysis models > Study weights in Bayesian model). Possibly a non-issue, but just pointing this out. Any particular reason the weighting scheme for the frequentist model was not described?
Specific
Fig. 1: Very much appreciate the clear articulation of search strategy.
Figure 2 (A) the OR for Luby et al. 2006 is very large (a previous version of the paper showed a smaller OR for this). Why is this? Is this something to be concerned about? In table S3, I see that Luby et al 2006 report 2 deaths in treatment compared to zero deaths in control – this must be driving the large OR but surely there has to be some sensible way to constrain this rather than let it run away to a large number. Again, this is outside my technical expertise, so I defer to the authors. Just noting that anyone who reads this will see the strangely high OR for the Luby et al 2006 study in Fig. 2 (A).
Sensitivity analysis: I appreciate very much the sensitivity analysis which reassures us on the quality of the results.
Footnote 7 in Supplementary Materials>4. Cost-effectiveness analysis> Drivers of cost-effectiveness is missing its text.
Table S7, footnote F, last line has a citation missing – there’s a note that says “….IPUMS [add citation]);…”.
There’s a jump from table S7 to Table S13 in the Supplementary Materials.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",85,75,95,,,,75,85,65,65,45,85,95,90,100,90,80,100,50,25,75,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",3.7,3.7,3.2,4.2,3.2,4.2,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",[12-15 years; range coded to preserve anonymity],20+,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),A truly needed and useful paper. I commend the authors on what they have produced. I do not see any major technical issues with the paper.,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,The study executes a meta-analysis using both frequentist and Bayesian models to derive odds-ratios (OR) for reductions in child mortality from a variety of water chlorination programs. It derives a high-quality result with careful and clear inclusion of studies and an extensive sensitivity analysis.,"Definitely a high quality study with clear, replicable methods. As I write in my review, I think the authors could do a bit more to frame or position this study better - which speaks to how it contributes to the field.",,,"This is my major qualm with the work. I find it a little hard to believe that policymakers—both globally, and at the national and local levels in the health sector—are unconvinced that clean water is a top choice when allocating their funds. To this end, my suggestion to the authors is to think carefully about how they position their work and also I suggest that they improve their cost-effectiveness analysis by adding some useful sophistication to the cost model.

Relevance to GP: 85, 90, 95
Yes - with some improvements as I suggest in my review.",*Top tier public health journal,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Water Treatment and Child Mortality: A Meta-analysis and Cost-effectiveness Analysis,https://unjournal.pubpub.org/pub/evalsumwatertreatment/release/1?readingCollection=02bc1831,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-05-30T11:56:25.675-04:00,
Water Treatment and Child Mortality: A Meta-analysis and Cost-effectiveness Analysis,https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4071953,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Hugh Sharma Waddington and Edoardo Masset,,"We evaluated the conduct, analysis and interpretation of the meta-analysis findings. The paper concerns an important topic for the global burden of infectious disease, particularly in middle-income countries where most of the included studies were conducted. It presents useful information on childhood mortality following water treatment and protection interventions. However, we had major concerns about conduct and reporting, which did not follow systematic review standards of transparency, and we also had major concerns about the interpretation of the findings for policy and research.","Note: To aid the reader, we (the managers) offer paragraph summaries in italicized block quotes
Introduction
The evaluators support systematic reviews and meta-analyses like these
As authors of a systematic review published last year (Sharma Waddington, Masset, Bick and Cairncross, 2023)[1], which included many of the same studies as the review by Michael Kremer and colleagues (hereafter the “review authors” or “reviewers”), we have been able to undertake a thorough evaluation of the review. Policy decision making should be based on the results of systematic, critically appraised evidence rather than single studies (Waddington et al., 2012)[2], hence we support the approach the reviewers have taken to collect and synthesize the evidence systematically.
Highly relevant
Furthermore, the topic of the review is highly relevant for global health policy, perhaps even more so than the review indicates. For example, interventions that aim to provide populations access to improved water supplies in quantity and/or quality have been shown in systematic reviews to be strongly associated with reductions in diarrheal illness (Wolf et al., 2022[3]; Ross et al., 2023)[4], yet a systematic link to reported mortality had not been made until recently (Sharma Waddington et al., 2023)[1]. This is important since an estimated 90 percent of disability adjusted life years (DALYs) for diarrhea are due to mortality, mainly in childhood, the remaining 10 percent coming from episodes of illness across the whole population. The Global Burden of Disease (GBD) estimates for drinking water supplies (Wolf et al., 2023)[5] are currently based on systematic reviews of illness reported by children’s carers, under the bold assumption that morbidity due to causes like diarrhea is closely correlated with mortality. By collecting and reporting data on losses to follow-up due to mortality contained in participant flow diagrams in reports of randomised controlled trials (RCTs) published in health journals, and by obtaining unpublished data on mortality from authors working in development economics and health, the review provides direct estimates of mortality in childhood from water treatment and protection interventions, which can potentially be used in future GBD calculations.
Other strengths: CEA, sensitivity analysis, etc.
We also believe there is much to praise about the review’s methodological ambition to provide information for decision makers. For example, it includes cost-effectiveness analysis of various interventions, which is rarely done in meta-analyses. It employs prediction intervals to estimate the impact of a new intervention, and in this way it aims to account for heterogeneity between studies. Finally, it undertakes sensitivity analyses including estimating frequentist and Bayesian meta-analyses, together with an assessment of small study effects that finds no evidence for publication bias for mortality outcomes, which is a very rare finding in the literature on intervention effects.
Our major concerns about the paper relate to how the systematic review and meta-analysis have been conducted and reported, and how the analysis has been interpreted, as we discuss below.
Systematic review conduct
An important aspect of systematic reviewing is transparency in conduct and reporting, which helps to ensure the analysis can be replicated by others. The reviewers conducted systematic searches for published RCTs on water treatment and protection interventions, harvesting data on all-cause mortality that were reported in participant flow diagrams in some studies, and contacting authors of RCTs to obtain unpublished data on all-cause mortality in childhood that were not already in the public domain. Systematic searches might miss studies, particularly if the searches cut across academic disciplines; in this case, RCTs of diarrhea morbidity have been published by economists and epidemiologists. A previous version of the review omitted several trials that were included in the systematic review and meta-analysis of water, sanitation and hygiene (WASH) and mortality by us (Sharma Waddington et al., 2023)[1], several of which have since been included in the meta-analysis as indicated by the reviewers.
Why were some studies excluded?
However, several studies of apparently eligible interventions, which reported all-cause mortality in participant flow diagrams, remain excluded from the analysis. These include Ercumen et al. (2015)[6] which reports all-cause mortality from two trial arms (chlorine plus safe storage and safe storage alone), and Bowen et al. (2012)[7], a long-term follow-up of another household water treatment (HWT) study that was included (Luby et al., 2006)[8]; both studies, while underpowered, reported higher mortality rates in the household water treatment group than in the control.
Lack of PRISMA approach; deviations from pre-registration
In some respects, the review is transparent about what was done. Although a systematic review protocol was not, to our knowledge, registered with any of the usual repositories for such studies (e.g., Campbell, Cochrane, Prospero), a pre-analysis plan was submitted to the AEA registry in June 2020. Fig 1 provides information about the search process and Table S2 provides information about which studies were excluded from the analysis, together with the reason why, although not in the usual form that a systematic review would provide. Reputable journals require systematic reviews to present a Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA) study search flow diagram, discussion of excluded studies that users might reasonably expect to be included, and a PRISMA checklist that indicates, for example, deviations from protocol.undefined There appeared to be deviations from the AEA registry record, such as the original exclusion of “cases where the study population is considered to be non-representative (e.g. interventions targeting HIV+ populations)” (Tan and Kremer, 2020)[9]. The review included a study of water filters and safe storage by Rachel Peletz and colleagues that was conducted among immunocompromised households (Peletz et al., 2012)[10]. We also wondered why a RCT on household water chlorination in Kenya (Kremer et al., 2008)[11] was not included in the analysis or in Table S2; this study aimed to evaluate the final pathway in water-borne diarrhea disease transmission by addressing contamination between source and point of use, and is therefore potentially highly policy relevant.
Differences between drafts; inter-rater assessments
We understand the working paper we have been sent to review is the second draft of a paper that has been online since 2023. We observed that the odds ratio estimates and 95 percent confidence intervals (95%CIs) differed, in some cases considerably, between the two working paper drafts. For example, we observed absolute differences in odds ratios of 0.04 or more for half (9) of the estimates (Boisson et al., 2013 [12]; Chiller et al., 2006 [13]; Dupas et al., 2021 [14]; Haushofer et al., 2020 [15]; Luby et al., 2006 [8]; Reller et al., 2003 [16]; Semenza et al., 1998 [17]; Peletz et al., 2012 [10]; Kremer et al., 2011 [18]), of which six (in bold) had differences of 0.08 or more. As a benchmark, the pooled effect in frequentist random-effects analysis was 0.75, hence these differences represent around one-third or more of the pooled effect magnitude. It is not clear to what extent the differences mattered for the findings, since in some cases the odds ratios were smaller, while others were bigger. However, we note that in the first draft of the review, the frequentist meta-analysis pooled effect for the chlorination sub-group was not statistically significant, whereas in the version evaluated by us, the review was able to find a significant effect, albeit over a slightly larger sample size. Hence, we believe it would be useful for the review to report inter-rater assessments on effect size data extraction and/or to indicate how discrepancies in the calculations were resolved, particularly regarding differences in estimated odds ratios and 95% confidence intervals.
Risk-of-bias assessment for RCTs; reporting bias for mortality vs morbidity
A key component of systematic reviewing is to undertake a transparent critical appraisal of the included evidence using risk-of-bias assessments, to help the reader understand how trustworthy are the findings from the included studies. The review does report a risk-of-bias assessment, using a tool that was developed primarily to assess observational studies, but the assessments are not discussed in the text or supplement. The review indicates that all studies are RCTs but, as is well known, RCTs can be at ‘high risk of bias’ due to problems in design or conduct, an example being selection biases due to high (or highly differential) losses to follow-up (attrition) in treatment and control arms, or joiners in cluster-RCTs. Another key aspect of the risk of bias concerns the quality of outcomes data collected. The review mentions that mortality data are more accurate and less biased than reported illness, even in unblinded trials. This is surely correct. The review could discuss in more detail what are the potential biases in reporting mortality, and why these are minor in comparison to biases in reported diarrheal illness. If there is any evidence or research supporting this claim it would be useful to report it. For example, one reason why the evidence suggests that reported mortality is very likely to be an unbiased measure (e.g., Wood et al., 2008)[19] is that it is very unlikely that a child’s carers might misremember or misreport it, whereas they may well do for a common illness like diarrhea (Sharma Waddington et al., 2023)[1].
The risk-of-bias ratings reported in the supplementary materials range between 4 and 7 out of a total possible score of 11. We note that evidence suggests it is not appropriate to determine overall bias using quality scales (Jüni et al., 1999)[20]. Authors of critical appraisal tools have instead shown that it is possible to assess overall bias based on transparent decision criteria (e.g., Eldridge et al., 2016[21]; Sterne et al., 2016)[22]. The review should comment on the implications of the risk-of-bias assessment for the confidence in the findings of the evidence base.
Meta-analysis conduct
Meta-analysis and pooling subgroups
Regarding the meta-analysis that was conducted, the review reports an overall pooled effect together with a sub-group effect for chlorination. However, the review could also have reported pooled effects for filtration, where there were three estimates. Perhaps the reviewers felt that the Peletz et al. (2012)[10] study, conducted among immunocompromised groups, was not representative of general contexts; but we note that, even if that study was excluded, meta-analytical pooling can be undertaken provided there is more than one independent effect size.
Heterogeneity; Measuring adherence
However, a key purpose of meta-analysis is not just to estimate a pooled effect, but also to explain heterogeneity in estimates across studies. The review conducted analysis of adherence and length of follow-up, among other factors, finding no strong association between mortality and adherence, and a negative association for follow-up length – that is, there was no significant effect of interventions on mortality in childhood for follow-ups beyond 52 weeks, as has also been found for diarrhea morbidity (Waddington et al., 2009)[23]. It is very difficult to measure adherence accurately since it is impossible to prevent populations from drinking other (unimproved) water sources, and because other disease transmission mechanisms may be more or less important in highly contaminated environments. For example, when sanitation is classified as unimproved, so most people are openly defecating or sharing toilet facilities with people from other households and/or using facilities that do not adequately remove excreta from the environment, the primary sources of pathogens are fecally contaminated fingers, fields, floors, flies, food and fomites, as well as fluids if drinking water becomes contaminated too (Wagner and Lanoix, 1958)[24]. Perhaps drinking water is particularly susceptible to contamination in such circumstances, so HWT might be effective if you can get people that openly defecate to practise consistent water treatment and protection through intensive promotion or inline drinking water provision. On a similar note, the review did not discuss the interaction of the interventions with baseline environmental characteristics. The sensitivity analysis considered the baseline prevalence of diarrhea, and the review observed that the meta-analysis was not sufficiently powered to conduct a disaggregated analysis. However, the review could have examined or discussed how the results might differ in different contexts in greater detail, since this has been a major concern in the literature.
Bayesian meta-analysis issues; strong differences from frequentist results; generalizability
It is increasingly common to use Bayesian meta-analysis, an approach first proposed by Paul Hunter (2009)[25] for HWT using empirical bias correction factors. The review states that the mean effect estimated by the meta-analysis is specific to the sample considered (p.14). This is only partly true. The review considered between-study heterogeneity at the review level, and aimed to predict study effectiveness beyond the sample considered, as measured by the Bayesian uncertainty analysis. However, in the Bayesian meta-analysis, the posterior estimates for individual studies differed from the frequentist model, sometimes considerably; for example, the estimate for Luby et al. (2006)[8] shifts from a whopping OR=23.88 (95%CI=0.08, 7240) to OR=0.74 (95%CI=0.37, 1.49). It would be useful for readers, who may be less familiar with Bayesian meta-analysis, if the review can explain why these differences are so large.
Systematic review and meta-analysis reporting
The review presents the numbers of deaths in treatment and control groups for all of the studies included in the meta-analysis in Table S3. This has required great effort on the part of the reviewers, and stands to be useful to researchers working on the effectiveness of interventions to reduce mortality in childhood for years to come.
Metadata on intervention contexts
However, it is standard practice in systematic reviews and meta-analyses on WASH topics to report transparently on the populations, interventions and the counterfactual water supply and sanitation conditions too (e.g., Fewtrell and Colford, 2004[26]; Arnold and Colford, 2007[27]; Waddington et al., 2009[22]; Wolf et al., 2022[3]; Sharma Waddington et al., 2023)[1]. For example, the reader wants to know the interventions evaluated, the circumstances in which the evaluations were conducted, the types of populations covered, such as whether any were from immunocompromised groups, and the degree of movement up the drinking water and hygiene ladders afforded by the intervention. This information should be readily accessible, very preferably in the main text.
Report standalone vs joint interventions
The front section of the review does not clarify whether the studies included were standalone water treatment and protection interventions or whether they were implemented alongside other interventions. It is common for WASH interventions to be implemented in conjunction with other treatments, which also may have independent or interactive effects on morbidity and mortality. For example, as noted in the supplementary materials, several included study arms were of multicomponent interventions where HWT was provided alongside cookstoves (Kirby et al., 2019)[28] or sanitation and hygiene (Humphrey et al., 2019)[29]. One chlorine trial incorporated food hygiene education (Semenza et al., 1998)[17]. Other studies had hygiene and sanitation co-intervention arms that were excluded (e.g. Luby et al., 2018; Null et al., 2018), although the review states they were incorporated in sensitivity analysis. This point changes the interpretation of the results, in some cases in important ways. For example, since the focus of the review is all-cause mortality, it should be clearer which of the studies combined HWT with software and hardware that can affect children’s exposure to enteric or respiratory infection such as washing with soap and water, food hygiene or indoor cook-stoves. The conclusions may need to be qualified by observing that the results should be interpreted as approximations of the effects of ‘water treatment and protection’, if ‘water treatment and protection’ interventions were implemented as multicomponent packages including other activities affecting morbidity and mortality in childhood.
Reporting random effects weights, etc.
We have an additional point about the meta-analysis as it was reported. It is standard practice to report the random effects weight of each study in the meta-analysis, as well as relative and absolute between-study heterogeneity (I-squared and Tau-squared) for all analyses conducted including sub-groups. Having a low value of heterogeneity helps the reader understand if the pooled effect is likely to be valid across the sample of studies included in the meta-analysis. Values of Tau-squared are reported at the overall review level, but weights and sub-group heterogeneity statistics can be reported transparently in forest plots.
Publication bias; Power calculation approaches
Regarding the publication bias analysis, which provides a rare example where small study effects were not measured, we believe this is because most of the studies were not designed to measure mortality as a primary or secondary outcome. As noted by the reviewers, there may still be publication bias present (for example, mortality data from 29 studies were not obtainable, as discussed below). However, we are less convinced by the approach used to assess the statistical power of the meta-analysis. The review added null results (the post-hoc simulation on page 4) to the observed results and checked whether the meta-analysis still found a statistically significant effect. We wondered if post-hoc power calculations would be a simpler approach to address the same question. Perhaps the review could calculate the minimum detectable effect size or power of the meta-analysis as a function of the number of studies and see whether it is sufficiently powered to detect an effect size in the presence of publication bias.
Cost-effectiveness analysis
Choice of overall sample estimates; uncertainty in cost-effectiveness
Although it was not a primary aspect of our review (as requested by Unjournal editors), we also had concerns about the cost-effectiveness analysis. Firstly, the analysis used the Bayesian meta-analysis estimate across the whole sample of studies, including filtration, spring protection and solar disinfection. However, since two of the cost-effectiveness estimates directly concerned chlorination, it would seem more appropriate to use the pooled meta-analytic effects for chlorination alone in those cost-effectiveness analyses. Secondly, the review does not provide uncertainty estimates for the cost-effectiveness estimates with respect to either the confidence intervals on intervention effectiveness or sensitivity analyses to different cost scenarios or other assumptions (e.g., adherence rates).
Limitations of the WHO GDP threshold for decisionmaking
The review also made frequent use of the WHO GDP threshold. We note that many commentators within and outside the World Health Organization (WHO) have expressed their scepticism about this threshold and its use in decision-making (see for example this document for a review of debates on the GDP threshold within and without WHO). The GDP threshold is still widely used today, and the review is not exceptional in this. However, since the threshold has been criticized in many ways, we suggest that the review reports the limitations of using the threshold for decision-making, and explain how the threshold should be interpreted for decision-making purposes in this particular context.
Implications for policy
Lack of (evidence of) representativeness
The review states that “the studies included in the meta-analysis are broadly representative of the settings in which policymakers might implement water treatment programs” (p.15). It is hard to believe that 18 studies could represent the contextual variability one would find within and across countries and contexts within countries, especially when one considers that 14 of the 18 included studies were conducted in middle-income countries. It would be useful to understand who are the policymakers that would find this sample representative. Site-selection biases operate, whereby research sites selected for trials are those where there is the greatest contamination of drinking water and diarrhoea disease burden (Sharma Waddington et al., 2023)[1]. Perhaps it should be accepted that the sample is not representative of contextual variability. But if it is representative, we suggest adding some supporting evidence.
Routes of transmission; pathogens
It is important to understand the different routes of infection transmission, and which particular diarrheagenic pathogens drinking water treatment and protection can address in typical disease circumstances, in order to understand the relevance and generalizability of the findings for policy. Endemic diarrhea in L&MIC contexts is understood to be caused by exposure to viruses (especially rotavirus), protozoa (especially cryptosporidium) and bacteria (especially E.Coli and Shigella) (Liu et al., 2017)[30]. However, water treatment may not adequately address faecal contamination if the treatment technology itself is not efficacious in combating disease (Arnold and Colford, 2007)[26]. An example would be filtration, which is efficacious against bacteria and larger protozoans, but less so against common viruses like rotavirus. It also requires safe storage for sustained efficacy as there is no residual protection after water has been filtered. Chlorination kills bacteria and most viruses, and has the advantage of providing residual protection. But, in usual doses, chlorine is much less effective against protozoans like cryptosporidium and Giardia, common causes of severe diarrhea in low-income contexts, especially, but not only, among immunocompromised groups such as those living with HIV (Abubakar et al., 2007)[31].
Sustainability (persistence); Hawthorne effects
In order to understand generalizability of the findings from a review of behaviour change interventions, one also needs to understand if desired behaviours are practised and sustained, such as whether sufficient protective agents are applied to treat drinking water or adequate personal hygiene practised at the point of use so that contaminated hands or utensils are not placed in drinking water storage containers. One aspect of this is to assess rates of adherence and sustainability, as done in the review. The review did not find a significant association between adherence and mortality, which is likely due to the different measures of adherence used in the literature and the problems in measuring adherence to drinking water technology more generally, as discussed above. The only consistent relationship that was observed appeared to be the limited effectiveness of HWT after 6-12 months of follow-up. Factors associated with dis-adoption include users disliking the odour and taste of chlorinated water.
Much of the evidence on water treatment has come from RCTs conducted at zero or negligible financial costs to participants, with frequent follow-up by outsiders and disruption of normal domestic routines (the ‘mzungu effect’) (Waddington et al., 2009)[22]. There is therefore a high potentiality in these studies for Hawthorne effects, where being observed leads to greater efforts to adhere to treatment protocols, favouring the treatment group in unblinded trials. This bias is especially likely to occur when follow-up and measurement occurs frequently, as it does in many evaluations of HWT interventions. For example, in analysis that includes many of the studies used in the review, Pickering et al. (2019: e1143)[32] reported that “virtually all the evidence that promotion of… point-of-use water treatment with chlorine or flocculant disinfectant reduce diarrhea come from studies that had daily to fortnightly contact between the behaviour change promoter and study participant”. Hence, one useful analysis that the review could perform would be to examine the association between odds ratios and frequencies of follow-up visits by investigators. When there are lots of visits, the findings of the studies are unreliable guides to the effectiveness of real-world programmes that do not have frequent follow-ups, yet require participants to undertake behavioural modifications where children’s carers must always treat household drinking water while also ensuring that children never consume water from unsafe sources.
Implications for reporting of RCTs and meta-analyses
Need better RCT reporting standards in development economics, esp. CONSORT participant flow diagrams
In our opinion, what the review clearly highlights is that current standards for reporting of RCTs, especially in development economics, are not fit for purpose. Reputable journals publishing field trials in health require that CONSORT participant flow diagrams are reported, which show numbers of individual participants by study arm from recruitment of clusters and individuals within clusters, through follow-ups, together with important reasons for attrition like death (Moher et al., 1998)[33]. Without this information it is difficult to assess important threats to validity in these studies, which might occur due to problems in design and conduct. It is not sufficient to publish data openly, as many economics journals require, in order to assess them. For example, a key aspect of the internal validity in cluster-RCTs is knowledge about when and how individual participants were recruited, so that total and differential selection bias into the study from joiners can be assessed. The same follows for selection bias out of the study (attrition), although this is more commonly evaluated. It can also be useful to know who dropped out of the study between enrolment and randomization stages to evaluate external validity.
A recent survey by Chirgwin et al. (2021)[34] of WASH impact evaluations in L&MICs found that only half of trials in health had reported a study participant level CONSORT diagram, whereas no RCTs of WASH in economics had done so. 3ie has published CONSORT standards for RCTs in economics (Bose, 2010)[35]. What the review demonstrates is that this lack of participant flow reporting is extremely costly. Had the participant flows been reported transparently, there would have been less need for the reviewers to contact RCT authors to obtain the attrition data on all-cause mortality in childhood.undefined The reviewers themselves noted this process was “time-consuming… and led to the loss of some data that was once available but is no longer available” (p.16), since there were 29 studies whose authors responded that the mortality data had not been collected, or had been collected but were no longer available, or who did not respond at all.
Reasons for previous lack of analysis of mortality, solutions
The review suggests that the reason why there has been hitherto limited analysis of mortality is because multiple testing of hypotheses prevents researchers from analysing the impact of the interventions on mortality. We are not convinced about this since the lack of reporting of mortality data is more likely due to the use of small samples, the difficulty of collecting mortality data, and apparently the lack of familiarity with reporting mortality data. Hence requiring these data be analysed as part of pre-analysis plans is unlikely to address the problem sufficiently. We believe a more effective solution would provide incentives for authors of RCTs to report participant flow diagrams, as are done in other fields, including RCTs measuring the impacts of HWT on diarrhea published in health journals. RCTs are costly to undertake financially and often require substantial time engagement by participants, so there are strong ethical and, as shown in the review, practical reasons for authors to report participant flows, and for reputable journals and commissioners to require them to do so.
There are similar standards for reporting systematic reviews and meta-analyses, which we discussed above, relating to the publication of protocols, reporting of deviations from protocol and adherence to PRISMA conduct and reporting standards. A key purpose of a systematic review protocol is to help reviewers avoid making results-based choices (consciously or otherwise). This does not mean that deviations from protocol are not allowed, just that they are explained.
COI issues/statements
Finally, we believe the positionality of the reviewers is not reported satisfactorily. It would be useful to know, for example, if the included RCTs conducted by the reviewers were appraised by different authors. Furthermore, one of the reviewers is a Board member of Evidence Action, the campaigning NGO that provided data on which two of the cost-effectiveness scenario estimates are based, and another is a principal investigator of two studies that led to the organisation’s earliest campaigns (Drinking Water Chlorination and Deworm the World). We might expect these associations to be mentioned in reviewer declarations due to the potential for conflicts of interest. For example, UKRI states: “the existence of an actual, perceived or potential conflict of interest does not necessarily imply wrongdoing on anyone’s part. However, any private, personal or commercial interests which give rise to such a conflict of interest must be recognised, disclosed appropriately and either eliminated or properly managed. Reporting, recording and managing potential conflicts effectively… can help to generate public trust and confidence.”undefined","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",50,40,60,,,,50,60,40,80,70,90,50,30,70,50,20,70,20,0,40,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",2.5,3.5,2,3,2,5,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

","Sharma Waddington has worked in the field of international development for 23 years, and in the field of policymaking and evidence based policy for 20 years. Masset has worked in the field of evidence based policy for 25 years. The combined total is around 48 years.","Since we have both worked as academics and grant funders for impact evaluations like RCTs, systematic reviews and meta-analyses for a combined total of approximately 50 years, we think we have evaluated over 250 each, so perhaps in excess of 500 in total.",False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),"We have calculated a simple average across the scores, which assumes each scoring category is accorded equal weighting.","Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,"An important aspect of systematic reviewing is transparency in conduct and reporting, which helps to ensure the analysis can be replicated by others. The authors conducted systematic searches for published RCTs on water treatment, harvesting data on mortality that were reported in participant flow diagrams in some studies, and contacting authors of RCTs to obtain unpublished data on mortality. A previous version of the working paper omitted several trials that were included in a systematic review and meta-analysis of water, sanitation and hygiene (WASH) by us, several of which have since been included in the review. In several respects, the reviewers are transparent about what they’ve done. For example, Fig 1 provides information about the search process and Table S2 provides information about which studies were excluded from the analysis, together with the reason why, although not in the usual form that a systematic review would provide. However, several studies of apparently eligible interventions, which reported all-cause mortality in participant flow diagrams, remain excluded from the analysis. These include Ercumen et al. (2015) which reports all-cause mortality from two trial arms (chlorine plus safe storage and safe storage alone), and Bowen et al. (2012), a long-term follow-up of another HWT study already included (Luby et al., 2006); both report higher mortality rates in the house-hold water treatment group than in the control. There are other points of inconsistency in decision-making that we noticed. For example, as noted by the authors in the supplementary materials, several study arms are of multicomponent interventions where HWT is provided alongside cookstoves and sanitation and hygiene. While it is clear how some multicomponent HWT intervention arms were assessed (e.g. Luby et al., 2018; Null et al., 2018), it is not clear how multicomponent HWT intervention arms from other studies were evaluated (e.g., Semenza et al. 1998). We were also surprised that a RCT on household water chlorination in Kenya by one of the authors (Kremer et al., 2008) was not included in the analysis or in Table S2. We understand that the working paper we have been sent to review is the second draft, the first draft being publicly circulated in 2023. We noted that the odds ratio estimates differ, in some cases considerably, between the two working paper drafts, hence we believe the authors should report inter-rater assessments on effect size data extraction and/or to indicate how discrepancies were resolved. Another key component of systematic reviewing is transparent critical appraisal of the included evidence using risk-of-bias assessment, to help the reader understand how trustworthy are the findings. The authors did conduct a risk-of-bias assessment, using a tool that was developed to assess observational studies, but the assessments are not discussed anywhere in the text or supplement, other than to indicate that all studies are RCTs and that reported mortality is an unbiased measure. As is well known, RCTs can be at ‘high risk of bias’ due to problems in de-sign or conduct, an example being selection biases due to high (or highly differential) losses to follow-up (attrition) in treatment and control arms, or late joiners in cluster-RCTs. The risk-of-bias ratings reported in the supplementary materials range between 4 and 7 out of a total possible score of 11. However, evidence suggests it is not appropriate to determine overall bias using quality scales (Jüni et al., 1999). Authors of critical appraisal tools have instead shown that it is possible to assess overall bias based on transparent decision criteria (e.g., Eldridge et al., 2019; Sterne et al., 2016). The reviewers should comment on the implications of the risk-of-bias assessment for the confidence in the findings at the review level. The authors considered the between-study heterogeneity at the review level, and predicted study effectiveness beyond the sample considered as measured by the Bayesian uncertainty analysis. However, in the Bayesian meta-analysis, the posterior estimates for individual studies differ from the frequentist model, sometimes considerably; for example, the estimate for Luby et al. (2006) shifts from a whopping OR=23.88 (95% confidence interval (CI)=0.08, 7240) to OR=0.74 (95%CI=0.37, 1.49). It would be useful for readers, who may be less familiar with Bayesian meta-analysis, if the reviewers can explain why these differences are so large.","Policy decision making should be based on the results of systematic, critically appraised evidence rather than single studies, hence we agree broadly with the approach the reviewers have taken to collect and synthesize the evidence systematically. The topic of the review is highly relevant for global health policy, perhaps even more so than the reviewers indicate themselves. For example, interventions that aim to provide populations access to improved water supplies in quantity and/or quality have been shown in systematic reviews as strongly associated with reductions in diarrhoeal illness (Wolf et al., 2023), yet a systematic link to reported mortality had not been made until recently (Sharma Waddington et al., 2023). This is important since an estimated 90 percent of disability adjusted life years (DALYs) for diarrhoea are due to mortality, mainly in childhood, the remaining 10 percent being accounted for by episodes of illness across the whole population. The Global Burden of Disease (GBD) estimates for water supplies are currently based on systematic reviews of morbidity, under the strong assumption that morbidity due to causes like diarrhoea is closely correlated with mortality. By collecting data from participant flow diagrams in reports of randomised controlled trials (RCTs) published in health journals, and by obtaining unpublished data on mortality from authors working in development economics and health, the reviewers provide direct estimates of mortality from water treatment and protection interventions, which can potentially be used in future GBD calculations. We also believe there is much to praise about this paper’s methodological ambition to provide information for decision makers. For example, it includes cost-effectiveness analysis of various interventions, which is rarely done in meta-analyses, and employs prediction intervals to estimate the impact of a new intervention, and in this way it aims to account for heterogeneity between studies. Finally, it undertakes sensitivity analyses including estimating frequentist and Bayesian meta-analyses, together with an assessment of small study effects that finds no evidence for publication bias for mortality outcomes, which is a very rare finding in the literature on intervention effects.","It is standard practice in systematic reviews and meta-analyses on WASH topics to report transparently on the populations, interventions and the counterfactual water supply and sanitation conditions. It is common for WASH interventions to be implemented in conjunction with other treatments, which also may have independent or interactive effects on morbidity and mortality. This has no direct implication on the meta-analysis or on the estimation, but it might change the interpretation of the results. However, the front section of the paper does not clarify whether the studies included are standalone water treatment and protection interventions or whether they are implemented alongside other interventions. The reviewers could qualify their conclusions by observing that the results should be interpreted as approximations of the effects of “water treatment and protection”, if “water treatment and protection” interventions (in the included studies) are implemented as packages including other activities affecting morbidity and mortality. The reviewers reported an overall pooled effect together with a sub-group effect for chlorination. However, there was a potentiality for pooling effects for filtration, where there were three estimates. Perhaps the reviewers felt that the Peletz et al. (2012) study, conducted among immuno-compromised groups, was not representative of general contexts; but we note that, even if that study was excluded, meta-analytical pooling can be undertaken provided there is more than one independent effect size. It would also be useful for the reviewers to report the random effects weight of each study in the meta-analysis, as well as relative and absolute between-study heterogeneity (I-squared and Tau-squared) for all analyses conducted including sub-groups. Having a low value of heterogeneity helps the reader understand if the pooled effect is likely to be valid within the sample of studies included in the meta-analysis. A purpose of meta-analysis is to explain the heterogeneity in estimates across studies. The reviewers conducted analysis of adherence and length of follow-up, among other factors, finding no strong association between mortality and adherence but a negative effect for follow-up length (that is, there was no significant effect for follow-ups beyond 52 weeks, as has also been found for diarrhoea morbidity). It is very difficult to measure adherence accurately since it is impossible to prevent populations from drinking other (unimproved) water sources, and because other disease transmission mechanisms may be more or less important in high-risk environments (e.g., when sanitation is classified as unimproved so most people are openly defaecating or using shared facilities that do not adequately remove excreta from the environment). On a similar note, the study does not discuss the interaction of the interventions with the baseline characteristics of the environment. The sensitivity analysis considers the baseline prevalence of diarrhoea, and the reviewers observe that the meta-analysis was not sufficiently powered to conduct a disaggregated analysis. However, the reviewers could discuss a more about how the results could differ in different contexts, since this has been a major concern in the literature.","The review presents the numbers of deaths in treatment and control groups for all of the studies included in the meta-analysis in Table S3. This has required great effort on the part of the reviewers, and stands to be useful to researchers working on the effectiveness of interventions to reduce mortality in childhood for years to come. However, we are also concerned that the review was not reported using standards of systematic review transparency. For example, a purpose of a systematic review protocol is to enable help ensure reviewers are not making results-based choices (consciously or otherwise). This does not mean that deviations from protocol are not allowed, simply that they should be noted as such. The review was registered with the AEA registry in June 2020, but standard information reported in systematic reviews like a PRISMA study search flow diagram, or a PRISMA checklist that indicates, for example, deviations from protocol, are not given. There do appear to be deviations from the AEA registry record, such as the original exclusion of “cases where the study population is considered to be non-representative (e.g. interventions targeting HIV+ populations)” (Tan and Kremer, 2020). Yet the reviewers include a study of water filters and safe storage by Rachel Peletz and colleagues (Peletz et al., 2012) conducted among immunocompromised households. We believe that the review clearly highlights that current standards for reporting of RCTs, especially in social science, are not fit for purpose. Reputable journals publishing field trials in health require the reporting of CONSORT participant flow diagrams, which show numbers of individual participants by study arm from recruitment of clusters and individuals within clusters, through follow-ups, together with important reasons for attrition like death in childhood (Moher et al., 1998). Without this information it is difficult to assess important threats to validity in these studies, which might occur due to problems in design and conduct. It is not sufficient to publish data openly, as many economics journals require, in order to assess them. For example, a key aspect of the internal validity in cluster-RCTs is knowledge about the point at which individual participants were recruited, so that total and differential selection bias into the study from joiners can be assessed; the same follows for selection bias out of the study (attrition), although this is more commonly evaluated. It can also be useful to know who dropped out of the study between enrolment and randomization stages to evaluate external validity. What the review demonstrates is that this lack of participant flow reporting is extremely costly. Had the participant flows been reported transparently, there would have been less need for the reviewers to contact RCT authors to obtain the attrition data on all-cause mortality in childhood - which the reviewers themselves noted was “time-consuming… and led to the loss of some data that was once available but is no longer available” (p.16) - and the ability to evaluate the credibility of the findings from the studies would be dramatically improved. RCTs are costly to undertake financially and often require substantial time engagement by participants, so there is a strong ethical and, as shown in the review, policy argument for authors to report participant flows, and for journals and commissioners to require them to do so. We also had concerns about the cost-effectiveness analysis. Firstly, the reviewers used the Bayesian meta-analysis estimate across the whole sample of studies, including filtration, spring protection and solar disinfection. However, since two of the cost-effectiveness estimates directly concern chlorination, it would seem more appropriate to use the pooled meta-analytic effects for chlorination in cost-effectiveness analysis. Secondly, sensitivity analyses to different assumptions are not reported. Finally, we believe that the positionality of the reviewers is not addressed adequately in the paper. It would be useful to know, for example, if the included RCTs conducted by the reviewers were appraised by different authors. Furthermore, one of the reviewers is a Board member of Evidence Action, the campaigning NGO that provided data on which one of the cost-effectiveness scenario estimates is based, and another is a PI of two studies that led to the organisation’s earliest campaigns (Chlorination of Drinking Water and Deworm the World). We might expect these associations to be mentioned in author declarations due to the potential for conflict of interest.","The reviewers say that “the studies included in the meta-analysis are broadly representative of the settings in which policymakers might implement water treatment programs” (p.15). It is hard to believe that 18 studies could represent the contextual variability one would find within and across countries. It would be useful to understand who are the policymakers that would find this sample representative. Perhaps it should be accepted that the sample is not representative of contextual variability. But if it is representative, we suggest adding some supporting evidence of this because the argument as it stands is not very convincing. It is important to understand the different routes of infection transmission, and which particular diarrhoea-causing pathogens drinking water treatment and protection can address, in order to understand the relevance and generalisability of the findings for policy. Diarrhoea in L&MIC contexts is caused by exposure to viruses (especially rotavirus), protozoa (especially cryptosporidium) and bacteria (especially E.Coli and Shigella) (Kotloff et al., 2013). However, water treatments may not reduce faecal contamination if the treatment technology itself is not efficacious in combating parasitic infections (Arnold and Colford, 2007). An example would be chlorination which kills bacteria and most viruses – and has the advantage of providing residual protection – but, in usual doses, is ineffective against cryptosporidium, a common cause of diarrhoea in low-income contexts, especially among immunocompromised groups such as those living with HIV (Abubakar et al., 2007). Filtration, while efficacious against bacteria and larger protozoans, is less efficacious against common viruses like rotavirus, and also requires safe storage for sustained efficacy as there is no residual protection. But in order to understand generalisability of the findings from a review of behaviour change interventions, one also needs to understand if desired behaviours are practised and sustained, such as if sufficient protective agents are applied to treat drinking water or adequate personal hygiene practised at the point of use so that contaminated hands or utensils are not placed in water storage containers. One aspect of this is to assess rates of adherence and sustainability, as done by the reviewers, and discussed above. Factors causing dis-adoption include, in the case of chlorination, users disliking the odour and taste of chlorinated water. Much of the evidence on water treatment has come from RCTs conducted at zero or negligible financial costs to participants, with frequent follow-up by outsiders and disruption of normal domestic routines (the “gringo effect”), and over relatively short periods of time (Waddington et al., 2009). There is therefore a high potentiality in these studies for Hawthorne effects, where being observed leads to greater efforts to adhere to treatment protocols, favoring the treatment group in unblinded trials. This bias is especially likely to occur when follow-up and measurement occurs frequently, as it does in many household water treatment interventions. For example, in analysis that includes many of the studies used in the Kremer meta-analysis, Pickering et al. (2019: e1143) report that “virtually all the evidence that promotion of… point-of-use water treatment with chlorine or flocculent disinfectant reduce diarrhoea come from studies that had daily to fortnightly contact between the behaviour change promoter and study participant”. The findings, therefore, may be unreliable guides to the effectiveness of real-world programmes where participants are required to undertake behavioural modifications that require children’s carers always treat household drinking water while also ensuring that they never drink water from unsafe sources.

Relevance to GP: 30, 45, 60
In theory the topic (interventions to block diarrhoeal disease transmission) and approach (systematic review and meta-analysis) are potentially highly useful for global prioritization efforts. Our major concerns were with the conduct, reporting and interpretation of the findings of the review. In addition, frequent use of the WHO GDP threshold was made, but we note that many commentators within and outside WHO have expressed their scepticism about this threshold and its use in decision-making. The GDP threshold is still widely used today, and the current study is not exceptional in this. However, since the threshold has been criticised in many ways, we suggest that the reviewers report the limitations of using the threshold for decision-making, and explain how the threshold should be interpreted for decision-making purposes in this particular context.",,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Water Treatment and Child Mortality: A Meta-analysis and Cost-effectiveness Analysis,https://unjournal.pubpub.org/pub/evalsumwatertreatment/release/1?readingCollection=02bc1831,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-05-30T11:45:49.018-04:00,
Biodiversity Risk,https://www.nber.org/papers/w31137,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Evaluator 2,,"This paper seeks to measure biodiversity risk. It does so in several ways:- Via a news based measure - textual analysis of corporate disclosures - a survey of finance professionals, regulators and academics The authors then apply their measures in an asset pricing and portfolio context and find that markets are pricing biodiversity risk to some degree but their survey respondents believe that market pricing of biodiversity risk is incomplete. ","As might be expected from this prominent team, this is extremely well executed research within the confines of what counts as research within the finance discipline. I believe, however, there is a better but harder way to go which involves much more interdisciplinary research and using actual measures of biodiversity loss as opposed to measures that are largely extracted from either news or expert opinion, with experts largely being members of the finance industry or academic finance researchers. 
What surprises me the most, is that in this attempt to measure biodiversity risk there has been no deep reflection on potential measurement validity issues that could arise from this exercise and perhaps learning from the many lessons that are emerging from trying to measure ESG risks generally and climate risk in particular. From Berg at al. (2022, Review of Finance)[1] we know that such top-down aggregate measures are very problematic. Nguyen et al. (2023, PLOS Climate)[2] highlights that even in the case of climate transition riskundefined scope 3 emissions there is very high divergence between measures from different data providers. 
All this should make us very cautious about aggregate measures in the biodiversity context given how multifaceted it is and how complex measurement is. There are what I would call “internal” attempts to validate the measures that the authors have developed relative to other measures they developed. However, none of these really provide a test of external validity and all the measures could be subject to some degree of impression management or fad, as opposed to measuring actual biodiversity risk.
Other comments:
It feels like the paper is perhaps doing too much and is perhaps more than one paper. I think purely focusing on measurement is a task big enough for one paper. Doing measurement and really nailing some form of external validity based on actual biodiversity measures would be a paper in its own right that could perhaps more strongly validate the aggregate measures that the authors have developed.
As we know from the climate context, transition riskundefined and physical riskundefined are very different and have very different measures and impacts. I would encourage the authors to think more deeply about this distinction between physical and transition risks in the biodiversity risk and develop the related discussion in the paper (which was light). In this context, the pricing and portfolio analyses seem premature.
Page 4 and page 13 - the sectors that are identified as having high biodiversity risk are not the ones commonly characterised as having biodiversity riskundefined. Despite the author's best effort to convince us that this is not a not another proxy for climate risk, the sectors look suspiciously like climate sectors. The statement on page 13 that the energy sector uses a lot of land is patently untrue.
In a related point to the above, the authors undertake a correlation of climate and biodiversity measures from the data on Figure 3. However, it seems clear to me that there is a long-term trend between the measures that is not captured by the higher frequency correlations undertaken by the authors. Perhaps a cointegration test might be revealing in this context.
I did not see mention of the authors obtaining ethical approval to do the survey, perhaps I missed it.

Overall, this is an interesting paper and I suspect the measures that the authors develop will ultimately be widely used. They will lead to a  greater focus in the area of biodiversity risk and future research will further refine these measures. However, I suspect that true measurement and true progress will come from bottom-up approaches that distinguish transition, physical and liability risk in the context and where actual physical measures of biodiversity risk are used.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",61,51,72,,,,45,50,40,45,39,50,68,60,75,90,80,100,63,55,70,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",3.5,4.4,3.1,4,3.3,5,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",Over 20 years in sustainable/climate finance.,,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),See my review,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,See my review,See my review,See my review,excellent - replicability should be high,"Very good

Relevance to GP: 75, 86, 100
Highly topical issues","Should make it in a marginal A or B journal.
Authors are prominent in prominent institutions.","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Biodiversity risk,https://unjournal.pubpub.org/pub/evalsumbiodiversityrisk,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-05-30T11:31:35.954-04:00,
Biodiversity Risk,https://www.nber.org/papers/w31137,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Evaluator 1,,The paper evaluates whether biodiversity risk is priced in asset markets and breaks new ground in doing so. It concludes that biodiversity risk is priced and provides open-source metrics for measuring biodiversity risk based on media reporting and firm-level biodiversity risk exposure based on 10K reports. The paper's strength is that it uses primary data that is well-documented and offers it for further research. One important weakness is that both the statistical and economic significance of the result are not sufficiently discussed.,"Summary
The paper analyses how biodiversity risk is reflected in the stock market. It contributes to the early literature on the relationship between finance and biodiversity (Garel et al. 2023[1]; Flammer, Giroux, and Heal 2023)[2]. It finds that biodiversity risk is priced and provides open-source metrics for measuring biodiversity risk based on media reporting and firm-level biodiversity risk exposure based on 10K reports. In doing so, the paper addresses a new and important topic and facilitates further research on biodiversity risk in financial markets.
Following a methodology developed by some of the same authors in the context of climate finance (Engle et al. 2020)[3], the paper constructs a ""biodiversity risk news index"" that provides a time series of news about the level of biodiversity risk. The authors also develop several metrics of firm-level exposure to biodiversity risk by parsing 10-K reports. In addition, they collect survey responses from 664 academics, practitioners, and regulators to understand perceptions of biodiversity risk. Using these inputs, the authors construct a mimicking portfolio and document a positive correlation between changes in the tone of biodiversity news and the portfolio's returns. The authors benchmark their biodiversity risk hedging portfolio against other factors. The paper concludes that biodiversity risk is priced into equity markets, but market participants do not consider the pricing appropriate. An additional analysis of municipal bond portfolios concludes that the same risk is not priced in bond returns.
Evaluation
The paper’s main claim is that biodiversity risk is priced in equity markets. Specifically, the paper claims that news about biodiversity risk are correlated with returns of a portfolio sorted on biodiversity risk exposure.
The authors provide reasonable evidence for this claim. This is shown by constructing mimicking portfolios that are long (short) in industries with low (high) past exposure to biodiversity risk, according to an industry aggregation of the firm-level exposure measures created by the authors. Using this approach and a mimicking portfolio approach (which determines weights by regressing the series of innovations in the biodiversity news index on the series of firm-level excess returns), the authors find apositive correlation of 10% to 20% between the portfolio returns and innovations in the news about biodiversity risk, using nine different variations of constructing the portfolios.
The authors do a good job of dismissing the concern that the measures of biodiversity risk capture other known firm characteristics by repeating the mimicking portfolio exercise with 207 characteristics (Chen and Zimmermann 2022)[4].
The authors are aware that they are testing a joint hypothesis, stating: “If biodiversity risk is priced in asset markets—and if our measures of exposure to this risk are correct—we would expect the price of these portfolios to move with the arrival of (aggregate) news about biodiversity risks.” Upon finding evidence of such comovement, the authors conclude that their measures are correct and that the risk is priced.
Two points limit the paper’s central claim. First, it is not clear whether the observed correlation is sufficient to accept the hypothesis. Second, the paper assumes that news about biodiversity risk reflects the true biodiversity risk.
Regarding correlation, the paper is not clear on whether a correlation of 10% to 20% is enough to conclude with confidence that biodiversity risks are priced. The confidence interval of these point estimates is not provided (see Figures 9 and 10, for example). There is also no discussion of the estimate's economic significance. Appendix A.4.3 reveals a p-value of one of the nine estimates (0.0462), but this information should be prominently disclosed in the main body of the paper.
The paper employs several measures, but the leading measure is based on news in the New York Times. It is not unreasonable to assume that the media reflects biodiversity risks. However, it is a critical assumption that is made, and the paper does not provide a sufficient discussion of this concern. The underlying biodiversity risk is potentially very large and slow-moving, as the paper points out in the introduction. It is conceivable that media-based indices capture changing levels of attention while the underlying risk remains unchanged. It is also the case that biodiversity risks can play out at very local scales, which means that a month-to-month change in national news coverage can be completely unrelated to dramatic local events, or that the news arrives when the risks on the ground have already materialized. It is important to be clear that the paper technically studies whether news about biodiversity risk is priced.
Further limitations that apply to the potential replication of the results is that the authors do not provide instructions on how the sample of firms is collected (Appendix A.4.5 describes how the sample of municipal bonds was collected, however). Furthermore, the paper does not report any of the results from the regression used to determine the mimicking portfolio weights (on page 30), which is an important step of the methodology.
Advancing Knowledge
The main academic contributions of the paper are to (1) introduce new firm-level measures of biodiversity risk exposure and an index of biodiversity-related news, and (2) show that biodiversity risk is priced in equity markets. The authors provide evidence that their new measures of biodiversity risk are not simply capturing previous measures of climate risk in equity markets. While the results suggest that some risk is being priced in equity markets (but not in municipal bond markets), the authors explicitly leave the question of whether the risk is adequately priced to future research.
The authors acknowledge that they do not differentiate between cash flow effects and investor preferences. When a New York Times article highlights deforestation, investor interest in deforestation is likely to rise, and investor favor for companies that engage in deforestation may decrease. This could lead to a decline in stock prices for these companies, resulting in lower returns in the short term, even if there are no changes in cash flow forecasts. In the current set-up, there is no way to differentiate between the shift of investor preferences or a change in future cash flows, which results in lower returns.
Real World Relevance
There are two reasons why this paper has real-world relevance. First, the authors provide the exposure measures and news index on their website (https://www.biodiversityrisk.org/). Investors are free to use this data to identify firms that use biodiversity-related words in their 10-K disclosures, for example. Second, the authors spend several pages (in section 2.3) explaining which specific risks the industries with the highest biodiversity risk are exposed to. Arguments are supplemented by snippets from firms’ 10-K disclosures. This analysis could be useful for practitioners who are trying to understand how biodiversity affects the business operations of certain industries.
The question of whether biodiversity risk is adequately priced is most relevant to the real world. The paper provides an indicative answer based on surveys, suggesting that the risk is not adequately priced. This is an initial step, but it is not yet a reliable answer because it is survey-based. Respondents may wish that biodiversity risks would be priced more because then market forces would help to limit biodiversity risks. Without an answer to this question, this paper offers a welcome first step but is by itself of little use to policymakers who are contemplating potential market interventions.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",65,50,80,0,0,0,70,85,60,70,60,80,81,70,91,90,70,100,71,59,80,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4,4.5,3.5,4.5,4,5,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",More than 10 years.,About 50.,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,"The authors did a good job of providing several analyses. Also, it is appreciated that the authors adjust for multiple testing. However, the lack of reporting of confidence intervals and standard errors weakens the robustness of the results.",,"The paper presents a good analysis of the perception of biodiversity risk in equity markets. However, the methodology introduced in the paper is not always easy to follow for readers unfamiliar with the topic, as it omits a short, simple explanation at the beginning and covers several aspects simultaneously.","The authors should be commended for providing the exposure measures and news index on their website. However, the authors fail to mention how the sample of firms was created, and do not provide sample statistics. Such an omission makes replication nearly impossible.","On one hand, practitioners can benefit from using the firm-level exposure measures. The authors also explain how certain industries are exposed to biodiversity risk. On the other hand, policy makers have little to gain from the paper, as the question of whether the risk pricing is adequate is not addressed.

Relevance to GP: 85, 92 100
The paper addresses a topic that is of global relevance.",,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Biodiversity risk,https://unjournal.pubpub.org/pub/evalsumbiodiversityrisk,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-05-30T11:31:19.923-04:00,
"Zero-Sum Thinking, the Evolution of Effort-Suppressing Beliefs, and Economic Development",https://www.nber.org/papers/w31663,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Evaluator 2,,"The paper introduces zero-sum environments to explain demotivating beliefs proposing a model and using data from Congo and the World Values Survey to support the model. It assumes demotivating beliefs to be universally incorrect, which is unsupported. In contexts with corruption or instability, hard work may not yield proportional outcomes, therefore demotivating beliefs may reflect true states of the world, thus negating the need for their model.","The paper explores the evolution of demotivating beliefs, which are deemed suboptimal from an evolutionary standpoint, by introducing the concept of zero-sum environments. These environments, where the success of one individual directly corresponds to the loss of another, can lead to higher economic payoffs in the short term by reducing competition among those holding similar beliefs. However, in the long run, such environments can result in lower innovation and reduced personal and social welfare. The authors have developed a neat evolutionary model that is well-written and utilizes data from the Democratic Republic of Congo and the World Values Survey to validate the model's predictions. However, the model is based on the very strong assumption that demotivating beliefs are wrong, and alternative interpretations are possible, which makes the argument and evidence problematic
First and foremost, while the introduction of zero-sum environments to explain the evolution of certain beliefs is an interesting angle, the assumption that demotivating beliefs are inherently incorrect seems too strong and unsupported by any evidence provided. In many contexts, these beliefs are actually accurate reflections of the environment. For instance, in environments characterized by high corruption or political instability, hard work may not yield proportional outcomes due to external factors. you may work hard for a promotion, only to see it go to a less skilled colleague who is a friend of the boss. Or you may work hard to grow your business, only for a war to destroy everything. The world can be unjust and demotivating. The authors do not seem to even consider this possibility, which I find puzzling. It’s important to recognize that in many parts of the world, and for many people, this is the rule rather than the exception. The authors cite Benabou and Tirole extensively, but these scholars focused on the opposite puzzle—how people can believe in a just world despite evidence to the contrary. If the authors want to assert that demotivating beliefs are incorrect, they need to substantiate this claim more thoroughly. Entertaining the possibility that these beliefs are sometimes accurate diminishes the need for an evolutionary explanation, as there would then be no ""evolutionary puzzle"" to solve—beliefs would simply evolve as correct representations of reality. I suggest the authors consider this alternative perspective more thoroughly.
Second, the measurement of witchcraft beliefs through a question asking whether individuals believe in gods or spirits other than the Christian God is problematic. This approach conflates non-Christian beliefs with witchcraft, which is both inaccurate and misleading. In addition, beliefs in the evil eyes are mentioned but not measured at all. The authors should provide more robust evidence to support the correlation between belief in non-Christian deities and witchcraft. Without this, the argument risks oversimplifying belief systems, while presenting the Christian God as the true one.
Third, the authors include under the same umbrella of demotivating beliefs, concepts as diverse as witchcraft, envy, and the notion that poverty results from factors other than laziness. Their connection is not clear and leads to controversial conclusions. Particularly, operationalizing demotivating beliefs through agreement with statements like ""The poor are poor because they are lazy"" is highly problematic. Such statements are matters of opinion, rather than objective truth, to say the least. But by equating disbelief in such statements with belief in witches (arguably not correct) the authors essentially suggest that people are poor because they are lazy as an objective truth. The paper’s structure, introducing witchcraft first and then the “poor are lazy” argument, seems to reinforce this problematic perspective.
Fourth, the authors' choice to dismiss as incorrect the belief that effort may not always translate into output while simultaneously accepting the idea that outcomes can be appropriated by others in a zero-sum environment is puzzling. What makes this distinction particularly perplexing is that both variables are measured by asking respondents to state their view of the(ir) world. In one instance, the authors accept the respondents' views as accurate, in the sense that the world is their subjective perception of it and therefore true (e.g., ""Gaining happiness requires taking it away from others"" or ""If my ancestors' spirits are looking out for my brother, they are less likely to look out for me""). However, in the other case, they treat the belief that ""Hard work doesn’t generally bring success—it’s more a matter of luck and connections"" as inherently wrong. This selective acceptance raises concerns about the consistency of the model’s underlying assumptions. Clarifying this distinction would enhance the robustness of the model.
Fifth, while the authors suggest that the correlation between zero-sum environments and demotivating beliefs is coincidental, it might make more sense to explore a causal relationship between the two. For example, in a zero-sum environment, individuals might put in a lot of effort only to see their gains diminished by someone else's success. This environment could naturally lead to the formation of demotivating beliefs, which may, in this case, accurately reflect the effort-outcome relationship. The evidence presented by the authors, which shows a correlation between zero-sum environments and demotivating beliefs, is equally compatible with this causal interpretation. Exploring this possibility could provide a more nuanced understanding of the relationship between these variables.
Finally, regarding the empirical evidence, I have a few comments:
First and more importantly: Even if the data presents a correlation between zero sum and demotivating beliefs, this does not necessarily validate the model, as the correlation could align with alternative explanations that do not require demotivating beliefs to be incorrect.

The analysis from the Democratic Republic of Congo presents individual-level correlations between demotivating beliefs and zero-sum beliefs. However, Proposition 4 needs a country-level analysis. The authors should test whether the mean demotivating belief is higher in populations with greater zero-sum perceptions, which could then correlate with lower average effort and material welfare. In contrast with the Congo dataset (which, by the way, I do not see any particular advantage of using Congo versus any other country), the WVS dataset offers an opportunity to explore this at a cross-country level but authors choose to correlate zero sum beliefs and demotivating beliefs only at the individual level (with country fixed effects). Note that going a step further and employing a hierarchical analysis could allow testing a potential cross-level moderation effect, where zero-sumness moderates the relationship between demotivating beliefs and happiness.
The variables used in the Congo study are somewhat “sketchy”:

The first way of validation of the perceptions of zero-sum-ness reflecting true zero-sum-ness is slightly unconvincing. The survey uses related questions (like the ""banana question"") rather than relying on truly revealed preferences, which would be more reliable.

The measurement of witchcraft beliefs is problematic. The question equates belief in non-Christian gods and spirits with belief in witchcraft, and further equates non-Christian beliefs with the notion that effort does not lead to success. This is both inaccurate and misleading.

Regarding the envy question, it measures how much people believe in envy, not their beliefs about society (the authors acknowledge this).
Regarding the WVS survey variables, the data are robust, and the variables are clear. However, I encountered some difficulty replicating the exact coefficients, as the manuscript does not specify which waves were used. If this information is not already provided, it would be helpful if the authors could include it. That said, the results are consistent across different waves and control variables.
The other part of the evidence, namely the correlation between zero-sum environments, demotivating beliefs, and both subjective and objective welfare, can be better explained by assuming that these beliefs reflect the true state of the environment. The hump-shaped relationship between zero-sum environments and demotivating beliefs can also hold if we assume that beliefs are more or less normally distributed around the true state of the demotivating environment. Finally, the association between demotivating beliefs and growth via innovation is better explained by assuming once again that these beliefs mirror a demotivating environment.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",40,30,50,,,,25,35,15,25,15,35,70,60,79,80,70,90,50,40,60,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",3,3.5,2.5,3.5,3,4,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",10 years.,15,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),"The model is good, and overall the study can be very useful and relevant. However, the assumption of demotivating beliefs = incorrect beliefs is too problematic.","Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,"While the methods are valid, the underlying assumption is wrong or to the very least unsupported. By changing this assumption, the model does not need the sorting and competitiveness pathway, and the inclusion of zero-sum environments is also in doubt. Other indices could be used instead. Therefore I think the model is wrong and that's why the low evaluation.",I hope no interventions will be based upon the notion that believing the world can be unjust is just a pessimistic view of the world.,"Apart from the lack of reasoning of why demotivating beliefs are incorrect beliefs, in the rest of the paper, the authors do a great job.","I would only comment, that the authors should clearly state which waves of the WVS they used.",,"Its a difficult question to answer. With the current form, I think it should not be published. If the paper relaxes the assumption of inherently incorrect demotivating beliefs and see what happens, it has much better potential.","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,"Zero-Sum Thinking, the Evolution of Effort-Suppressing Beliefs, and Economic Development",https://unjournal.pubpub.org/pub/evalsumzerosumthinking/release/3?readingCollection=02bc1831,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-05-30T11:26:22.097-04:00,
"Zero-Sum Thinking, the Evolution of Effort-Suppressing Beliefs, and Economic Development",https://www.nber.org/papers/w31663,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Evaluator 1,,"At its core, this paper formalizes and tests a theory posing that the poor are poor because they do not work hard enough. I describe conceptual, theoretical, and empirical issues.","Summary
In a series of papers in the 1960s, anthropologist George Foster introduced a “theory” of development based on what he called “the image of limited good”, which, for the purpose of this evaluation, can be summarized as follows. Zero-sum environments are more conducive to effort-suppressing beliefs, which in turn lead to less effort and subsequent economic development. The paper under evaluation, henceforth CBHNW, provides a theoretical representation of Foster’s hypothesis along with empirical tests in support of their theory.
CBHNW states that “by providing a formal theory that builds on Foster’s insights and testing its predictions, we establish a strong link between zero-sum thinking, demotivating beliefs, and economic activity in small-scale non-industrial societies as well as industrialized societies.” This would indeed contribute to our understanding of economic development. However, in this evaluation I argue that this paper fails to establish such a link—conceptually, theoretically, and empirically. Hence, CBHNW provides no contribution to our understanding of economic development. Instead, the significant amount of time and effort devoted to following Foster’s old ideas is a gloomy reminder that (some) research in economic development still builds on the idea that the poor are poor because they do not try hard enough.
Conceptual issues
CBHNW is based on George Foster’s ideas, which were received with much criticism even soon after publication in 1965 (Kennedy, 1966[1]; Acheson, 1972[2]; Acheson, 1974[3]). That is to say, my criticism is far from novel. CBHNW, however, does not acknowledge (nor engage with) this academic debate. If the aim of CBHNW is simply to “provide a theoretical representation of Foster’s hypothesis”, the remaining of this subsection is irrelevant; convincingly or not, they did it. I presume, however, that the goal of this rather comprehensive paper is more ambitious. Thus, I must start by discussing Foster’s ideas.
Let us start with some context. Foster was a relatively wealthy anthropologist from the United States, who developed his ideas, mostly, by interpreting what he saw, read, and heard about life in very small and poor villages in Mexico. From his necessarily skewed viewpoint in small villages in Mexico, he then made inferences towards “peasant societies” in general.
The first part of his argument is that “peasants” share a mindset in which “the cake” is fixed, which means that to get a larger piece, one must take it away from someone else. The second part of the argument is that this zero-sum environment leads to a “cognitive orientation” in which people avoid standing out since their success would be perceived as someone else’s downfall. The evidence in support of these arguments is, under the most generous interpretation, very weak. Instead, a critical reading of his work indicates that Foster had a preconceived idea in his mind, shaping a mold into which he would then squeeze his observations through far-fetched interpretations. For example, to support his idea of the “fixed cake” mindset of the “peasants”, Foster extends the idea to noneconomic aspects of life, leading to arguments such as that “a mother’s ability to love her children is viewed as limited by the amount of love she possesses” (Foster, 1965; p.298). He also states that “it is a truism that health is a ‘good’ that exists in limited quantities” (Foster, 1965; p.299), based on observations such as that “the loss of blood and semen is said to be considered permanently debilitating in some areas” (Kennedy, 1968); one wonders who is benefiting from the loss of blood and semen. In summary, there is much to be said about Foster’s work. My aim is not to critique his work in detail—that has been done since the 1960s—but it is useful to expose the weakness of the arguments leading to the foundation of the theory of economic development in CBHNW.
CBHNW stated goal forces the authors to be more precise than Foster, which leads to additional conceptual concerns. I present a selection. According to CBHNW, envy is an effort-suppressing belief. That is, people may choose to work less hard and accumulate less wealth to avoid the envy of their peers. Surprisingly, this is presented as a fact. To me, the opposite seems more likely; people may work harder in response to their own feelings of envy. Indeed, after a quick search, I was able to find research in favor of the role of envy as a motivator but not on its role as demotivator (e.g., Salerno et al., 2019[5]). Furthermore, I also found research on the evolutionary advantage provided by envy (Hill et al., 2011). The closest work in support of envy as demotivator is on people hiding their success for the sake of social harmony, which is not the same as suppressing effort (Arnett and Sidanius, 20183[6]). Thus, envy, which is the main belief explore is not even unequivocally effort-suppressing.undefined The second effort-suppressing belief in CBHNW is witchcraft, which is the reductionist way chosen to describe “traditional religious beliefs”. More specifically, the authors refer to the “evil eye”, which is a fairly universal belief that people can harm other people by giving them a “bad look”—typically driven by envy. Critically, a belief in the evil eye does not necessarily lead to less effort, it suffices to hide the success. Finally, CBHNW deals with the degree of zero-sumness of the environment. It remains unclear, however, whether they refer to the actual environment or the perception of the environment. The theory implies the former, yet the empirics focus on the latter. Furthermore, there is an implicit assumption that all members of a given society are exposed to or perceive the same zero-sumness. In practice, the poor may rightfully feel like their environment is zero-sum, while the rich may feel the opposite. This heterogeneity may explain much of the empirical evidence presented in the paper.
Finally, CBHNW proposes a novel account to partly explain why the Global North is richer than the Global South. They argue, emphasis is mine, that
“Europe’s global expansion and colonialism after 1500 CE—including the Columbian Exchange—may have reduced zero-sum thinking through the emergence of new trading opportunities (..). an inflow of new technologies and resources, like new crops (…), fertilizers/guano (…), and stimulants like sugar, coffee, and tea (…), and a perception of limitless, though not unoccupied, land. Unleashed by colonialism and aided by the devastating impact of Eurasian diseases, this shock may have opened an exit ram from the trap to zero-sum thinking. According to our theory, such effects could have triggered a cultural shift to a less demotivating belief system accompanied by higher effort exertion and more learning-by-doing. This cultural shift could have ushered in technological breakthroughs and a transition to modern, intensive economic growth.” (p.45)
Let us take a minute to consider what this quote says.undefined Europe’s colonialism, in which they grew richer by directly taking away from others, taught them that the world is not zero-sum—even if they were literally taking resources away from the local populations. This newfound realization led to a cultural shift in which they were less prone to hold demotivating beliefs, such as envy. With less envy, thus morally superior according to the christian tradition, Europe got a new powerful work ethic. Finally, through hard work, Europe got richer. Yes. Somehow, CBHNW argues that the “Great Divergence” stems from Europe’s industrious nature instead of their loot. The implication being that the Global South is poor because they did not work hard enough, not because Europeans brought diseases that wiped out the many advanced societies that existed in, for example, America.undefined
Let me be clear, the poor are not poor because they do not try hard enough. To the skeptical reader, I propose the following thought experiment. Suppose there is a community with 100 residents all with similar wealth. One day, one half of the residents take the other half’s property, resources, and labor with no legal consequences. Five generations later everyone is relatively free and living under a common rule of law in the same community. Naturally, the descendants of the sacked half are significantly poorer. Would the origin of the great divergence be a mystery? No, the origin is the “exchange” five generations ago.
Theoretical issues
Given the conceptual issues listed in the previous subsection, I do not see the value of formalizing Foster’s ideas. For those who may see some value, I provide a brief critique of the theory taking the premise as given.
CBHNW claims that “Evolutionary models are underutilized in economics”. This is not a positive but a normative statement, which may explain the authors’ modeling choice as a preference. Beyond this sign and the stated importance “to have theory (and predictions) in a setting where values and beliefs are in motion and not assumed to be fixed or in some stable equilibrium” (p.2), there is no additional discussion on the appropriateness of their modeling choice.
My concern is the way in which players interact with each other, that is, through production in matched pairs. This type of interaction, while convenient for an evolutionary model, implies a rather strange institutional framework. Presumably, in the small-scale economies in Foster’s study-sites, there was a substantial degree of subsistence farming and some local markets. Today, most people interact in a market. Without a convincing discussion on the ways in which the chosen evolutionary model is a good approximation to more realistic scenarios, it is hard to interpret the scope of the theoretical predictions. In its current form, every prediction should be qualified by “assuming paired production”.
Empirical issues
The empirical section of the paper aims to test the predictions from the theory, which I have argued is set on an unrealistic framework of paired production. What we may learn from these tests about the actual world remains an open question. Here, I limit myself to point out a few issues in the empirical section.
There is significant subjectivity in the way in which abstract concepts are represented. For example, the validation exercise (p.24) reveals that seemingly minor differences in the zero-sum index can lead to radically different views of the world, which begs the question on whether the index is truly capturing the abstract concept. Here, it may be useful to present the distribution of the index, which is only present for the cross-country data.
Effort is the key variable linking development with beliefs in the theory, yet there is no true measure of effort. Instead, CBHNW argues that education is a good proxy. That is simply not true, the best predictor of a person’s education level is simply the parents’ education level. Hence, the empirics are missing the key link to development.
The theory implies a causal story, yet all empirics rely on association.
In page 20, the statements are presented in pairs, but it is not clear whether one of each pair is randomly presented. If so, it would be useful to see results by type of framing. I would, for example, expect a very different answer from “Gaining happiness requires taking it away from others” and “It is possible for everyone to be happy”.
Also related to the statements starting in page 20 used to build the zero-sum index, the first statement in each pair seems driven by envy, which means that the regression of the envy and the zero-sum index is simply showing that more envious people are more likely to have envious views. This type of open interpretation is common to most regression.
In page 32, “The second measure captures the respondent’s view of whether the poor can escape poverty through effort: ‘In your opinion, do most poor people in this country have a change of escaping from poverty, or is there very little change of escaping?’”. I note that nowhere in the question is there any reference to effort.
Table 4, every variable is a proxy for income, which again is better predicted by parents’ income. As I mentioned elsewhere, given the high levels of inequality throughout the world, it is feasible for the poor to perceive the world as zero-sum, while the rich perceive the world as non-zero-sum. Perhaps the authors can run robustness tests ruling out the effect of inequality.
Figure 5 and 6. The hump shape can also be explained by ex-post rationalization. In Figure 6, for example, the poorest decile worked hard, and it did not pay off, thus, work does not bring success. For the richest decile, it is convenient to belief that their standing comes from work instead of the most likely scenario, they were born into the richest decile in the first place.
Conclusion
Unfortunately, I do not see a way to make this paper better. It is a shame that so much time and effort was spent for the sake of Foster’s patronizing ideas.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",,,,,,,21,33,10,,,,80,70,90,78,70,85,,,,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",,4.8,,,4.5,5,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",About 10 years.,>10,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),"At first glance this paper is looks very strong, as there is certainly much skill in their technique and story-telling. Closer inspection reveals the shaky foundations. In essence, it is a beautiful house of cards.","Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,The story-telling is so good that the reader easily forgets that a discussion of the appropriateness of the model or possible alternative explanations are not provided.,,,There is no mention about how to access their survey data. The cross-country dataset is well described.,,"I think it will be published in a top 5 journal. It is after all a ""great paper"" built on a shaky foundation by an all-star team. 
The evaluator wrote “I believe it should not be published.”","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,"Zero-Sum Thinking, the Evolution of Effort-Suppressing Beliefs, and Economic Development",https://unjournal.pubpub.org/pub/evalsumzerosumthinking/release/3?readingCollection=02bc1831,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-05-30T11:18:18.461-04:00,
Intergenerational Child Mortality Impacts of Deworming: Experimental Evidence from Two Decades of the Kenya Life Panel Survey,https://www.nber.org/papers/w31162,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Evaluator 1,,"A well-written and coherent paper. One of the first showing intergenerational effects of childhood health intervention[s] in [a] LMIC, including cost-benefit analysis. Statistical power calculation[s were] not provided. Literature review is slightly one-sided. Some results in tables could be discussed further in [the] text. Some typical outcomes [were] not present (nor mentioned), [… and ] if [they] exist in the data [it] would be interesting to see them.","This paper is using a widely-studied randomized school-based deworming program in Kenya to explore the presence of intergenerational benefits. In particular, it looks at infant mortality and under-5 mortality, and examines five possible channels. The effects are [a] 20% reduction in mortality rate (though only under-5 is significant), with all examined channels contributing to the reduction. The study also explored heterogeneous effects by parents’ gender and age, with only age being significant.
This is a very interesting and important paper studying [the] intergenerational effects of early childhood health interventions. Overall, the paper is well written, focused on a very specific well-defined hypothesis, which is answered adequately and using appropriate and well-executed methods. The assumptions in the cost-benefit analysis seem plausible and the estimated return can indeed be considered conservative.
Although I found the paper well-written and of high quality, here are some comments/suggestions the authors might find useful:
There are several studies analysing impacts of deworming mentioned, although all on the more ‘positive’ side. Perhaps some of the more ‘critical’ papers could also be cited as contrast to provide a more balanced view of the literature. [Note: numbering added.]
It would be helpful if the authors comment on (or include) something on statistical power, currently there is no mention of it.
It would be helpful to the reader if the authors include a table in the additional exhibits (for online publication) with summary statistics for the covariates discussed in page 6.
For the channels analysis in Table 2: The number of observations changes for each outcome. Are results robust to keeping the same observations across all specifications?
Dose-response interpretation: Although I wouldn’t rule it out theoretically, [are] the results in Figure 2 providing enough evidence of this? Looking at the two-year bins, it looks more like 0 and 1-2 being together (same) VS 3-4 and 5-6 being together (same), i.e. no dose-response. Perhaps this can be supported via hypothesis testing?
It would be interesting to see if there are any effects on birth outcomes, such as birth weight [or] gestational age (if these have been measured in the survey). Additionally, it would be interesting to see if there is an effect on sex ratio for births (proxy/indicator for fetal loss).
Are there any other health outcomes measured at (or before) age 5? Height, for example, to examine stunting?
If either of the two previous suggestions are true, and provided the same measurement for the parents exists, it would be interesting to compare same outcomes for parents and their children and the impact of the intervention on their correlation.
I found the heterogeneous results by parents’ age too interesting and valuable to be pushed to the appendix. Perhaps some could be moved to the main body (e.g. Tables A2 and A5) along with further discussion. Also, Table A.6 is not mentioned anywhere in the text (I can see how column (2) is replication of column (9) Table 2, but would be worth a brief discussion of overall findings).
The authors mention in the discussion the study of Duhon et al. (2023)[1] on other aspects of the intergenerational transmission of human capital. Although the discussion of the non-/cognitive gains is beyond the focus of this paper, a brief discussion of the health gains examined in that study would be beneficial for the reader.
I think the authors would assist the reader by providing discussion (or direct comparison when possible) of the study of Barham et al. (2023)[2] in Bangladesh, as it’s the one most closely related to theirs (or any other among the ones cited).","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",87,77,97,,,,85,95,75,85,75,95,85,75,95,85,75,95,95,90,100,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",3.5,3.5,3,4,3,4,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",10 years,40,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,"Intergenerational Child Mortality Impacts of Deworming: Experimental Evidence from Two Decades of the Kenya Life Panel Survey
",https://unjournal.pubpub.org/pub/evalsumintergendeworming,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-05-30T11:12:49.087-04:00,
Building Resilient Education Systems: Evidence from Large-Scale Randomized Trials in Five Countries,,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",,,"Provides important evidence on effectiveness of a low-tech intervention to stem learning losses from school closures (likely to increase in coming years) by scaling it up vertically and across diverse geographies.

Note that results only apply to numeracy skills and for temporary and abrupt school closures. Future work should see if it is effective for other skills and circumstances (displaced people, girls), and if contextualizing content/methods improves outcomes.","Written report
This is a review of a five-country study that has attempted to assess the impact of two phone-facilitated interventions to continue the education of students which was disrupted due to the Covid-19 pandemic. The interventions were 1) regular SMS messages containing educational content, as well as nudges to engage in educational activities, 2) the previous SMS intervention plus regular, short-duration tutoring over a phone call; with [the] content of the tutoring adjusted by frequent assessments so that teaching was targeted to the level of learning of the student. The control group had access to whatever material their respective governments provided, typically through television or radio broadcasts, or print-outs.
The primary purpose of the study was to scale-up a low-tech and multi-method intervention addressing the educational needs of children whose schooling gets disrupted due to crises. The study also evaluated the impact of the interventions on secondary outcomes such as student’s non-cognitive skills, student & caregiver perception of learning progress, caregiver willingness to pay for such an intervention, teachers’ (from the phone-call intervention) perceptions towards their profession, and potentially adverse economic outcomes on caregivers. The results were generally positive for the phone intervention – seemingly robust to several checks, though not so much for the SMS intervention. Secondary outcomes also appeared to show positive results.
Importance and strengths of this paper
The study contributes positively to research on education in emergencies (EiE), particularly for children whose access to schooling has been suddenly disrupted. The paper is important for the following reasons:
Disruptions to education can have negative outcomes due to the loss of learning caused by such a phenomenon. Loss of learning can lead to an incomplete education which can potentially lead to poor economic outcomes (Hanushek & Woessmann, 2020)[1] or worse, especially for marginalised sections of the society (World Bank Group, 2018).
At the time of writing this review in 2024 disruptors to education such as extreme weather events, armed conflict, and disease outbreaks appear to be escalating and/or probabilities of them appear to be increasing (Gowan & Davis, 2024[2]; Mia, 2023[3]; Mora et al., 2022[4]; Seneviratne et al., 2023[5]; Torkington, 2024)[6]. In light of these efforts to develop & test EiE interventions and the results of such efforts are valuable.
Besides education in emergencies, there is potential for this intervention to be employed elsewhere such as distance learning for individuals who are hard to reach or from low-income backgrounds. There is some indication for this (Nejezchleb, 2020)[7] though more empirical research is necessary.
What are the specific strengths or commendable aspects of this paper?
The strength of this paper is not in the novelty of the topic or the interventions (since it has been mentioned by the authors themselves that this was developed and tested in an earlier pilot study in Angrist et al., 2022)[8] but in what it does – refining, scaling up, and providing evidence on the effectiveness of relatively-more accessible EiE interventions.
The heterogeneous contexts across which this study takes place (of note an extreme weather event in one study location) and the individual results for each country is another positive aspect of this paper. Through these we can infer if an intervention is generally effective or effective only in a specific context.
The third strength, and this is a catch-all point for a lot of things, is making the maximum possible use of a large-scale study by investigating any bias via robustness checks as well as looking at the effects this intervention might have on a range of secondary outcomes from perception of progress to adverse economic outcomes on caregivers.
Identifying and assessing the paper’s primary claims
Claim #1 [SMS + Nudges + Tutoring —> Numeracy]:undefined SMS messages containing educational content along with nudges to engage in educational activities combined with short-duration tutoring sessions over a phone call, the content of which was adjusted through regular assessment of the student’s learning levels, raised numeracy skills (measured as ‘levels’ and later standardised using values from the control group) by 0.30 to 0.35 standard deviations.
i) The source for this claim is section I of the paper. The primary results from table 1 in the appendices report a range of figures between 0.32 and 0.40 standard deviations.
ii) My subjective 90% interval for this is (0.10, 0.50) and is primarily based on the results from table 4 of this paper which shows the country-wise effects of the interventions. An additional factor for this confidence interval is that this study addresses some of the concerns pointed out by Crawfurd et al., 2021[9] in its pilot, that of households self-selecting into the treatment.
The confidence for this claimundefined would be improved by robustness checks that check all or a minimum of more than half the countries included in the study. At least three robustness checks are conducted in only one country each.
Claim #2 [SMS + Nudges —> Numeracy]: SMS messages containing educational content along with nudges to engage in educational activities alone raised numeracy skills by 0.08 standard deviations.
i) The source for this claim is Section I and appendix Table 1 of the paper.
ii) My subjective 90% interval for this is (-0.02, 0.18) and is entirely based on the results from Table 4 from this paper which shows the country-wise effects of the interventions.
Claim #3 [Minimal difference between government and NGO-hired teachers]: There is minimal difference in learning outcomes for the phone-call based intervention when tutoring (that’s part of the intervention) is carried out by government teachers versus when it is implemented by teachers hired by NGOs.
ii) The probability of this claim being true would be placed at 70% on the basis of it being conducted across a large (N = 4941) and diverse sample, but still short of the sample drawn to estimate the results of the intervention’s direct effects. And also due to a lack of information on the exact analysis
performed to determine equivalence of effects.
Identifying some of the paper’s secondary claims
Note [from evaluator]: This section does not include all of the secondary claims made in this paper. The claims identified (and not assessed) are selected based on the reviewer’s subjective opinions on what would be relevant for policy makers.
Claim #1 [Perseverance]undefined: Across countries a sample of students (N = 8962) was selected to examine the effects of this intervention on their non-cognitive skills, one of which was perseverance (measured as willingness to attempt a second riddle after solving one before). Students that were part of the phone-call intervention had a 2.9 percentage point higher probability of [being] willing to try a second riddle (compared to a baseline willingness of 83.3%).
Claim #2 [Unemployment]: In the Philippines and Uganda a sample of female caregivers (N = 2446) was selected to examine if the interventions contributed to unemployment. The SMS-only intervention appeared to
reduce female caregiver unemployment by 2.9 percentage points while the phone-call intervention increased it by 1.9 percentage points (from a baseline unemployment of 42.9%).
Claim #3 [Caregivers’ perceptions]: Across countries a sample of caregivers (N = 9188) was selected to examine the effects of this intervention on their perception of their children’s progress. Caregivers that were part of the SMS-only intervention perceived on average a 0.07 standard deviation increase in their children’s numeracy levels while those from the phone-call intervention perceived on average a 0.11 standard deviation increase.
Claim #4 [Teacher morale]: In Nepal some of the tutors for the phone-call intervention were selected from a pool of government teachers. From a small sample of this pool (N = 290), those that were assigned to the phone-call intervention had a 15.8 percentage point higher probability of expressing a willingness to be a teacher again (compared to a baseline willingness of 73.5%).
Concerns
[Possible misinterpretation of takeup rates]undefined In section III.D.5 it is mentioned that in Uganda there was low take-up of alternate learning mediums like radio (29%) or printed materials (22%) during Covid-19, however there is a possible misinterpretation as Uwezo Uganda, 2021 shows that use of any type of material is around 69%. Low take-up of any one type of medium may reflect individual preferences and not necessarily problems with access. Regardless, the fact that cellular phone access is more prevalent (as mentioned in the paper in section II.A) might still help reach out to those who continue to be left out.
[SMS intervention description] There appears to be little to no description or discussion of the SMS-only intervention given how it was still effective in certain contexts.
Limitations of the paper and potential ways it could be improved
[Robustness checks inconsistency]undefined The battery of robustness checks in this paper although helpful can appear to be a bit misleading when taking into consideration that some are conducted on samples from individual countries (e.g. testing validity using back-checks, differences with assessment methods and randomising questions) while others are conducted across all countries included in the study (e.g. inclusion of country weights, fixed effects). Fewer but broad[er] robustness checks would be better over many and a mix of broad & narrow ones.
[Caregivers mediate access to mobiles] This intervention can also be impacted by how caregivers mediate access to mobiles, especially to girls (Damani et al., 2022)[10]. While the study appears to be balanced (based on tables A1 & A3), it is possible that there was lower participation in the study of female students from specific countries. But any inquiry on this would have perhaps overextended the scope of this query and as such it would be better to consider this as a point for future research.
[Disruption \neq displacement] Based on the evidence presented this intervention would be best suited to situations where education has been suddenly disrupted and learning losses need to be prevented. It would be less suited to education of displaced people as their concerns can revolve around bureaucratic barriers to access education or education tailored to their background (Shuayb et al., 2023[11]; Shuyab, 2019)[12].
[Phone access may limit scaling] Although the intervention is a low-tech intervention, it is still a technological intervention and can face issues with accessibility of cellular phones/landline phones, charging, connectivity or cellular network rates (Houngbonon et al., 2021[13]; Khurana et al., 2022[14]; Tandon et al., 2024[15]; Valenza et al., 2022)[16]. Expanding this intervention to different contexts should ideally be preceded by scoping studies for its viability.
[Network shutdown issues] Cellular networks are also vulnerable to shutdowns by governments or extreme weather events (Richtel, 2011[17]; Saaliq, 2019[18]; Yancey-Bragg, 2024)[19], and though the study had a situation where a typhoon occurred and did not appear to significantly disrupt learning outcomes (table 7 of the paper), it is still noticeably lower than those not affected by it; with sustained outages potentially having an even a bigger impact. A possible work-around is to complement this intervention with conventional mediums like printed self-study material.
Future workundefined
Future work should focus on concepts beyond basic numeracy skills. Would this intervention work for more foundational concepts like number recognition or more advanced ones like calculus? Would this intervention work for other areas like social sciences, language, or social & emotional learning (SEL)?
Future work that wants to explore ‘novel’ avenues on this specific intervention can explore how adapting the intervention to local contexts in terms of educational content, tutoring methods and medium of instruction can possibly improve the effect of this intervention. Researchers can look at Raisch et al., 2024[20] an example of contextualisation of educational interventions. 
Another important area for future research – but one that would perhaps require substantially more investment in resources is if this intervention can go beyond just preventing learning losses and be used as a viable, alternative medium of learning i.e. for sustained learning, teaching entire semesters or courses over it.
Nit-picking
Error bars in figure 2 are faint.
Elaborating on the outcome/dependent variables in the Data or Empirical Strategy section instead of just in the footnotes of tables would have been preferred.
Elaborating why the learning variable is a ranked outcome would have been preferred.
Questions to the authors
Why not imitate Gneezy et al., 2019 in capturing & isolating the impact of ‘real-effort’ on the tests?
Why not use a real-effort question as a control variable instead of as an outcome variable in a robustness check?
Why use student grade as a control variable when baseline learning was already a control variable?
Why is India left out of Figure A4: Learning Curve – Improved Implementation and Targeted Instruction Across Trials?
Can a copy of the SMS instructions be provided?","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",83,73,93,80,72,88,78,88,68,88,78,98,79,71,87,62,52,72,90,85,95,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",3.9,4.5,3.5,4.4,4,5,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",1 year,"None, have reviewed proposals for ethical review though - around 10 or 20 of them",False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.","SMS messages containing educational content along with nudges to engage in educational activities combined with short-duration tutoring sessions over a phone call, the content of which was adjusted through regular assessment of the student’s learning levels, raised numeracy skills (measured as ‘levels’ and later standardised using values from the control group) by 0.30 to 0.35 standard deviations.",,"My subjective 90% interval for this is (0.10, 0.50) standard deviations and is primarily based on the results from table 4 of this paper which shows the country-wise effects of the interventions. An additional factor for this confidence interval is that this study addresses some of the concerns pointed out by Crawfurd et al., 2021 in its pilot, that of households self-selecting into the treatment.",,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Building Resilient Health Systems: Experimental Evidence from Sierra Leone and The 2014 Ebola Outbreak,,The Quarterly Journal of Economics,"deprioritized bc. of journal-publication status, authors' permission, age, etc.",,,2025-05-29T13:44:12.807-04:00,
Building Resilient Education Systems: Evidence from Large-Scale Randomized Trials in Five Countries,,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Anon  1,,The paper studies an important question of [the] effectiveness of low cost education programs which could reduce learning losses faced by primary students due to shocks or emergencies in five developing countries.,"Comments from journal-tier ratings and “Overall assessment”
The paper studies an important question of effectiveness of low cost education programs which could reduce learning losses faced by primary students due to shocks or emergencies in 5 developing countries. …
… the importance of the question makes it one of the recent interesting topics in economics research. Additionally, since the low cost intervention has effectively been scaled across 5 countries, I think the study has made a unique contribution by contributing to the external validity of education in emergencies research.
The quality of the paper is very high. The contribution of the project, the credibility of the methods used to study causality, its impact on real world issues. Furthermore, the cost effectiveness nature of the intervention makes it very useful.
[Major] commentsundefined
[…] The paper makes a unique contribution by scaling the RCT and providing medium effect sizes of the intervention. I present my concerns in the hope that it may improve the paper:
Missing health indicatorsundefined
As the intervention was implemented in the aftermath of the COVID-19 pandemic, I am surprised to see that no information on child and caregiver health is provided. The econometric model does not include any health indicators in the baseline control variables.
More details on phone ownership
I think the paper could benefit from [including] more details about how the phone tutorial worked, especially if the phone owner and the caregiver were two different individuals. Were there regular intervals during the phone calls due to unavailability of other phones or network problems? Did it have any impact on the learning gains? Did the teacher ensure that the caregiver did not change over time, especially since it was right after the pandemic? How did the teacher ensure that there was not more than 1 student benefiting per call? The phone was on speaker, right? So could it be possible that a student who already had the lesson joined the same lesson in another friend's house, essentially having a similar lesson (say addition) twice? This also feeds into the problem of inadequate robustness checks. A more detailed description of a typical phone tutorial will help.
Additional mechanisms
As teachers and caregivers also seem to benefit from the intervention, it could be that improved teaching effectiveness and improved caregiver attention could be potential additional channels to explain the results.
Minor comments:
1) Section IIIB and IIIC can be shortened.
2) It could be worthwhile to scale up the randomisation of instruction delivery via NGOs or teachers to other countries than Nepal. Some countries may have social norms where government teachers are given more legitimacy than NGO workers (or vice versa) in regard to instruction delivery. In that case, you may find one-directional results in terms of effectiveness.
Note: undefined
Other comments (from the ratings sections)undefined
Overall, I liked the simplicity of the RCT design and it was cleanly executed. The main results are presented in an interesting manner. My concerns are with the robustness checks and mechanisms.
The logical flow in the data analysis is consistent with the arguments set forth by the authors. There is some repetition of content in Sections III B and C (already covered in the Introduction) which can be avoided.
…With reference to discussion on methods and data analysis, the authors have done a good job. The tables and figures are labelled properly and explained well.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",90,,95,,,,85,90,80,95,90,100,84,75,90,75,60,90,95,90,100,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4.3,4.5,3.7,4.8,4,4.8,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",I have finished my PhD between 2016-2021 and been an Assistant Professor of Economics since then.,Between 5-10,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),"The quality of the paper is very high. The contribution of the project, the credibility of the methods used to study causality, its impact on real world issues. Furthermore, the cost effectiveness nature of the intervention makes it very useful.","Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,"Overall, I liked the simplicity of the RCT design and it was cleanly executed. The main results are presented in an interesting manner. My concerns are with the robustness checks and mechanisms. I have pointed out the same above.",Please see my comments above.,The logical flow in the data analysis is consistent with the arguments set forth by the authors. There is some repetition of content in Sections III B and C (already covered in the Introduction) which can be avoided.,"Replicability in a RCT is always a tricky point. The RCT seems to be a costly matter however the intervention is very cost-effective. Plus the authors were able to reduce the costs further from the Botswana study due to economies of scale. From a perspective of cost, I would say the RCT is not very replicable. However, with reference to discussion on methods and data analysis, the authors have done a good job. The tables and figures are labelled properly and explained well. They are easily replicable.
Managers’ note: This does not fully capture our intentions when we asked about replicability (and openness, and data sharing). We did not want this to consider the costs of running a new RCT, which is largely out of the researchers’ control. We will work on explaining this criteria better in future.","Yes, the research question being of utmost importance and relevant to deal with the education crisis, I would say the paper and the suggested intervention could be relevant to practitioners.
Relevance to GP - 90, 95, 100
The paper deals with a global priority: learning loss due to emergenies. The intervention of using phone tutorials with SMS reminders is a cost-effective method which could have high impact on primary education learning in developing countries.","In continuation with my previous point, the importance of the question makes it one of the recent interesting topics in economics research. Additionally, since the low cost intervention has effectively been scaled across 5 countries, I think the study has made a unique contribution by contributing to the external validity of education in emergencies research.","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Building Resilient Health Systems: Experimental Evidence from Sierra Leone and The 2014 Ebola Outbreak,,The Quarterly Journal of Economics,"deprioritized bc. of journal-publication status, authors' permission, age, etc.",,,2025-05-29T13:31:47.778-04:00,
The Returns to Science In the Presence of Technological Risks,https://arxiv.org/abs/2312.14289,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Gregory J. Lewis,,"I was principally tasked with kicking the tires on the ‘biocatastophe’ parameters as this is my subject of expertise. They check out, and I can find little to criticise.

Either I’m missing something, or the paper seems to be making a large mistake by comparing ‘all of the benefits’ of science on the one hand versus ‘only the technical risks of biocatastrophe’ on the other. Is AI risk the elephant in the room?","Thanks for the opportunity to review this intriguing paper. There are two main parts to my review:
I was principally tasked with kicking the tires on the ‘biocatastrophe’ parameters as this is my subject of expertise. They check out, and I can find little to criticise.

Although not my expertise, I read the wider paper, and I struggle to make sense of it. Comparing the returns of all of science vs. only some of the technological risks (i.e. biocatastrophies) seems to stack the deck against technological pessimism. The ‘AI risk’ numbers for both domain experts and superforecasters alike are much higher for ‘AI’ than ‘bio’ risks. I think the ‘bottom lines’ of the paper depend on AI risks being excluded from consideration, but this exclusion looks very dubious to me.
I am much more sure of myself on the first issue than the second (where I feel I must be missing something obvious), but the second issue is much more important in terms of ‘what to make of this paper’. I will discuss both.
1) What’s the ‘right’ parameter value for biocatastrophe risk?
The principal inputs to the paper’s modelling of technological risks are forecast estimates of engineered pandemics of particular severity. (I’ll share the paper’s convention in labelling these ‘biocatastrophes’).
Sourcing from the XPT
The paper sources its estimates for biocatastrophe risk from the Existential risk Persuasion Tournament (XPT), deriving various rate parameters from its raw values to plug into the modelling results. The paper’s defence of relying upon this source is straightforward:
As noted in section 2.3, while forecasting the future is very challenging and we do not even know if it can be reliably done at all over the long run, I think we have strong reasons to treat the results of this forecast as our best guess. I am not a domain expert in the relevant fields, but I think we have strong reasons to see these estimates as defaults, and to impose the burden of proof on critics who would seek to suggest alternatives. [p 25, my emphasis]
It is also, in my view, straightforwardly correct: I would also use this as my default source, and all the alternatives I can suggest are inferior to the XPT. In essence:
There is very little ‘first principles’ modelling of bio risk. The main paper is Millet and Snyder-Beattie 2017[1], and their quantitative offers were mainly ‘illustrative’ rather than ‘best guess’. In any case, their (calculated)undefined figures for x-risk (albeit being offered in a spirit of conservatism) are in a similar ballpark to those in the paper (and if anything tending towards the lower superforcaster estimates):
Table 4
I am not aware of any attempts to provide more rigorous modelling work since. Even if such work is out there, I am sure it would be sufficiently speculative it would at most inform a best guess, rather than thinking we should substitute in its results as our best guess.
I am not aware of any strongly informative ‘proxy indicators’ for biocatastrophe: stuff like how many ongoing clandestine BW programs, lead indicators like public health emergencies of international concern declaration etc. do tell you something about the risks you care about, but they’re analogous to the weak indicators geopolitical forecasters have to resort to. The XPT forecasts (from either supers or domain experts) on these proxies seem broadly concordant with their forecasts of the key risks, and the set of proxies used in the XPT looks pretty good to me.
Insofar as we are indeed reliant on judgemental forecasting, the XPT seems to be the best game in town, for reasons the paper articulates well. Yes, there have been previous opinion polls and individuals offering their guesses (e.g. the GCR 2008 conference poll, Toby Ord’s estimates in The Precipice), which tend to offer bigger numbers. Yet the XPT has more people, many of whom selected for relevant expertise or ability, working in concert for a longer time. We would expect this to arrive at superior forecasts.undefined
Another key advantage is most of the relevant ‘literature’ predates the XPT, and much of it was provided in its questions to the forecasters. This poses a greater burden on the critic as it is credible their pet information or pet considerations have already been ‘priced in’, and second guessing the judgement does not look promising given point 3 above. I am also not aware of any dramatic ‘post-XPT’ revelations which could be expected to dramatically shift the aggregate forecasts.
Favouring the superforecasters
Despite discussion, the superforecaster and biodomain experts remained far apart: domain experts gave larger numbers at least a factor of ~3, and at most a factor of ~100, depending on the exact question. The paper runs the numbers for both families of estimates. Although the initial model has the same upshot ‘either way’, the models from section 8 onwards which incorporates extinction do look qualitatively different depending on which estimates are plugged in. The paper’s discussion (Section 11.1) argues the superforecaster numbers should be favoured.
Running the numbers for both and highlighting sensitivity is great. I also agree that the superforecaster numbers should be favoured. My quibbles are on how strong this lean should be, and how this gap between estimates could be better presented to the reader.
I think the reasons given in the paper to favour superforecasters count for something, but do not add up to an impressive value of pro tanto. Briefly:
Higher intersubjective accuracy has not (to my knowledge) been validated as a good proxy for accuracy, and in principle could be confounded by (e.g.) effort and time spent in discussions.undefined The spread in forecasts (more later) demonstrates that being good at modelling the views of other participants does not prevent you from offering higher forecasts yourself.
‘Correlated pessimism’ for domain experts could be alternatively explained by low information: if I focus on AI, I may not know much about (e.g.) asteroids, but know enough about forecasting [to know that I should not be giving extremely low probabilities without more information. Superforecasters with a general interest may apply their efforts more evenly across questions. Although I’d expect an optimistic geopolitical disposition to get beaten out of a superforecaster with bitter experience, there’s very little corrective feedback if you’re biased to optimistically rate 1% long-run risks as 0.1%.
Finding a different set of domain experts predicted ~ 3 bio events over a period where one happened (and their lower bound estimates roughly matched)undefined is not the most damning indictment of domain expert accuracy: superforecasters, if well calibrated, will rack up occasional ‘big misses’ too.
Nonetheless, the best case I can muster for picking the domain experts over the superforecasts here is even weaker:
The standard story for domain experts being better is they have a deeper knowledge of what they are talking about, and maybe they have developed an intuitive tacit grasp of their field which enhances their accuracy but which cannot be communicated to a non-expert despite the best intentions of both. Yet the findings from Expert Political Judgement suggest domain expertise offers very little edge.
Your reviewer is both a domain expert and a superforecaster, and I would take the ‘over’ on the superforecaster medians - I also speculate others I know who ‘are both’ would likely do the same. Presumably this “domain expert superforecaster” estimate should ‘trump’ both domain experts and superforecasters in terms of checking more of the epistemic virtues. Ignoring the obvious problem this supposed aggregate is of “me and my imagination that similar folks would agree with me”, larger groups of worse forecasters can outperform much smaller groups of better ones. Besides, my numbers also lean towards the supers, at least if we’re taking the mid point as the arithmetic average.
What does the interval between ‘superforecaster’ and ‘domain expert’ risk estimates look like in the xrisk modelling?
For the initial modelling of ‘just’ mortality in the time of perils, you get the same upshot for the (much lower) superforecaster biocatastrophe risk estimates or the (much higher) domain expert ones. The picture looks different when you start considering extinction risks where bio can stop the future rather than causing greater or lesser bumps along the way.
If you plug the superforecaster numbers into the model in section 8, you’re indifferent between accelerating science being good or bad if you start valuing the future at ~~10^5 of years of current annual world value (Lambda). If instead you plug in the domain expert one, the breakeven value of the future at risk is ~~10^2. I’ll duck discussion of whether benchmarking by discount-rates is the most informative (I found it helpful), but I agree with the author the upshots now look sensitive to the risk estimate chosen: if the first number, accelerating science is credibly good, if the second number, it looks dubious.
But I would like to see the values of this modelling for the interval between ‘superforecaster’ and ‘domain expert’ risk estimates. I don’t have a clear intuitionundefined whether the graph of Lambda break-even vs. risk is basically linear, hyperbolic, log-linear, or something else. This region seems important as most people’s ‘best guesses’ for biocatastrophe risk (including the author’s, depending on how ‘75%’ the superforecasters are more correct is interpreted) will be ‘somewhere in between’ the superforecaster and domain expert numbers. If the arithmetic or geometric midpoint would give resulting Lambdas much closer to one side or the other, I expect this would be handy for most readers to know.
Spread, uncertainty, and value of information (the bioxrisk numbers are not that great)
There’s also a lot of spread within the groups of superforecasters and bio experts, which is discussed and handled well by the relevant appendix. However, although the numbers re. bio xrisks are the best we have, they are not that great.
Only come from 14 bio experts.
Participants did one-shot so did not have the same depth of discussion amongst and between them.
I don’t think we have measures of spread for either (although with small count the ‘90% percentile bioexpert estimate’ would be noisy), and it would be nice to know how much the groups overlap for the x-risk questions in particular.
As it turns out these numbers play a starring role in the main uncertainty the analysis highlights (i.e. maybe science isn’t worth it if there really is a ~1% chance of bio killing everyone by 2100?). These numbers not being that resilient might be worth highlighting - and, unlike the XPT generally, it may not be hard to gather more robust figures on these topics in particular.
Naturally, one cannot fault the paper for playing the imperfect empirical cards it is dealt. But I wonder whether it would be worth highlighting this source of uncertainty: if nothing else, the ‘murky’ picture for science if we take the domain expert figures for bio extinction risk could become clearer if larger numbers or further elicitation lead them to resolve to a higher or lower value.
2) Use and interpretation of the parameter
My major worry with the paper is not of getting its biocatastrophe numbers wrong, but the external validity of its subsequent use of the parameter in the model, and how those results are being interpreted.
Comparing all the gains from science vs. a subset of the risks seems illegitimate in principle
The paper seems to compare the benefits of science as a whole on the one hand versus a subset of the technical risks of science (i.e. biocatastrophies) on the other. I struggle to make sense of this.
If the aim is to assess whether accelerating science is worthwhile overall, one must weigh all the benefits versus all of the risks (AI being the elephant in the room - much more later). If accelerating science is indeed overall something that yields a negative expected value, finding that it would be worth doing if we only had to worry about engineered pandemics is at best irrelevant, and at worst misleading.
If the aim is to assess whether some subset of science which generates technical risk is nonetheless worthwhile to accelerate, it should only be getting credit for its ‘attributable fraction’ of the aggregate benefits of science. Commercial fusion power sooner (e.g.) may be an expected dividend of accelerated scientific progress, but we can be basically sure the breakthroughs required will not be found in the life sciences.
In contrast, comparing allundefined of the benefits of science vs. only the biocatastrophe can only adjudicate a much more extreme tech-pessimism: that the enhanced biocatastrophe risk would be sufficient alone to make accelerating science as a whole undesirable. That being false is consistent with either (or both): “Science in general is socially undesirable to accelerate”, or “Accelerating ~biology is socially undesirable as its benefits do not outweigh the increased biocatastophe risk it generates” being true.
It is treacherous for a reviewer to complain about the research question being ‘irrelevant’ or ‘trivial’,undefined but this seems the wrong question to be asking - especially as, to the best of my knowledge, the claim that increasing biorisk alone makes scientific progress generally net-negative is seldom made (although - back to the elephant - I have heard it mooted re. AI). If this is the question the paper wants to answer, it should make clear its results only rule against an extreme (bio)tech pessimism, and they provide no answers to steelman (or realman?) tech pessimism.undefined
Unfortunately, the paper seems to stray beyond its own modelling stipulation (primarily - elephant again – by sweeping AI risk out of view). Perhaps the best exampleundefined (besides the title of the work) can be found in early in the executive summary:
On the other hand, once we consider the more remote but much more serious possibility that faster science could derail advanced civilization, the case for science becomes considerably murkier. In this case, the desirability of accelerating science likely depends on the expected value of the long-run future, as well as whether we think the forecasts of superforecasters or domain experts in the existential risk persuasion tournament are preferred. These forecasts differ substantially: I estimate domain expert forecasts for annual mortality risk are 20x superforecaster estimates, and domain expert forecasts for annual extinction risk are 140x superforecaster estimates. The domain expert forecasts are high enough, for example, that if we think the future is ""worth"" more than 100 years of current social welfare, in one version of my model we would not want to accelerate science, because the health and income benefits would be outweighed by the increases in the remote but extremely bad possibility that new technology leads to the end of human civilization. However, if we accept the much lower forecasts of extinction risks from the superforecasters, then we would need to put very very high value on the long-run future of humanity to be averse to risking it. [p2-3]
The first sentence is not limiting itself to ‘that faster science could derail advanced civilization via biocatastrophies alone’. Yet the forecast estimates and modeling discussed as (e.g.) ‘annual extinction risk’ are only those for biocatastrophe. If we used the domain expert (or superforecaster) estimates for AI, bio+AI, or ‘anthropogenic x-risk’ generally, the future would need to be worth much less than 100 years of welfare for us to be averse to risking it for faster science.
Non-biotech risk cannot be reasonably discounted in practice
The paper stipulates in Section 2.2 it is restricting itself to biological risks: “I call [biotech enabled pandemics] the time of perils. Note my usage here refers specifically to biological perils, not perils from unaligned AI or other forms of new technology.” [p8] I complain this stipulation (and subsequent broader interpretation of findings) would ‘in principle’ stack the deck against pessimism.
If it were the case that biocatastrophe appeared to constitute the lion’s share of technical risk, although in principle it will lower-bound overall peril, in practice it is probably close enough unless the results proved very finely poised. Not so:


For both ‘catastrophe’ (>~10% dying) and extinction, bio comprises small fractions of the aggregate risk. If asked ‘are these risks technological?’ I aver most would answer ‘clearly not’ to natural pathogens and non-anthropogenic risks, ‘probably not’ to nukes, and ‘definitely’ to both AI and engineered pathogens.
AI, of course, is the elephant here, ‘beating’ bio across the board. It is extremely hefty given the paper’s preferred weighing to superforecasters and its finding that the xrisk modelling is the crux of the matter: these estimates are 38x higher for AI than they are for bio. So, in practice, ‘stipulating out’ AI risk from the technical risks being assessed seems to indeed stack the deck: the returns to science outweigh its technical risks, but only when the lion’s share of technical risk is ignored. Can this stipulation be justified?
Science acceleration would not contribute to AI progress/risk?
One rationale for exclusion would be if we would not expect generally ‘boosting science’ to contribute to AI risk like it would for engineered pandemics. The paper moots this mainly in fn1, although it is alluded to elsewhere:
One reason this report focuses specifically on biocatastrophes rather than risks from advanced AI is that the rate of progress in AI is currently driven by major labs that operate outside the traditional academic ecosystem where science today is largely performed. Fundamental advances in the life sciences, in contrast, continue to be predominantly made in the academic ecosystem. [p8]
Yet:
Even if the picture currently is as-described, a lot can change between now and 2100. Maybe we have a couple of cycles of tech disruption, and the key players in (say) 2060 spawn (Deepmind style?) from ongoing academic activity.
Pace ‘scale is all you need’, maybe you need more fundamental breakthroughs (e.g. that require ‘only’ someone very smart with pen and paper, not 10^lots FLOPs). It is at most ‘murky’ whether we would expect mega-labs to dominate if the path to AI risk runs through more theory, related fields like neuro and linguistics, or has a long wilderness without commercial application (etc.)
The scoping of meta-science in section 1.0 does not limit itself to ‘fundamental research’: major ‘applied’ lab progress could be complemented by (e.g.) ‘DARPA-like agencies’, public-private partnerships, acquiring FROs, continued flows of academia-trained CS PhDs, etc.
I grant this reason counts for something, but less than any impressive value of pro tanto (maybe a factor of 1.3x?). On its own, it falls a long way short of justifying AI risks to be wholly discounted. If there are other reasons which can successfully carry the rest of the burden, I did not find them.
AI only matters insofar that (not how) it can terminate the time of perils
In the initial model (e.g section 4.2), AI comes up in terms of its likelihood of closing the time of peril, where it takes the bulk of the probability (1.6%/year, vs. 0.08% for non-AI extinction and 0.3% for a major economic break). In this model, the only relevance to AI is its contribution to the aggregate chance of exiting the time of peril: whether the exit is ‘good’ or ‘bad’ is out of scope, and so the propensity of AI to lead to one or the other is irrelevant.
There is a smaller issue here on whether ‘non-bio’ things can cause catastrophe without closing the time of perils in ways that accelerated science could meaningfully contribute to. XPT AI-catastrophe estimates are greater than their x-risk/general transformation proxies (i.e. 2.1% vs. 0.4% for the supers), and some of the AI-disaster mechanisms mooted (e.g. nukes, supply chain disruption, ‘AI rebellion’) don’t guarantee transformation. A similar story applies to nukes, where most of the risk is ‘merely’ catastrophic.
Ultimately, I don’t think this adds up to much. Unlike AI, I think we can roughly discount any contributions of accelerated science to nuke risk (e.g. my understanding is, short of sci-fi-magic, you still need to source a lot of fissile material no matter what). For AI, although I can’t give a firm partition, my impression reading through the XPT report is that the sort of catastrophes folks had in mind were at least peri-AI-transformation.
The bigger issue is well-described at the start of section 8:
So far, these models have implicitly adopted the view that biocatastrophes reduce utility during the years in which they occur, but do not otherwise impact humanity's long-term trajectory. Whether biocatastrophes happen or not, we have maintained the assumption that we exit the current epistemic regime with probability 1-p in every period.
This assumption is inappropriate for the worst kinds of biological peril. To take a simple, if extreme, example, if an engineered pandemic kills every human on the planet, then progress towards transformative AI will stop. This is also an outcome of pandemics that do not kill literally everyone, but kill enough to lead to the complete collapse of society (as is commonly envisioned in fiction like The Stand, The Last of Us, or The Last Man on Earth). [p50, my emphasis]
Yet (AI-)extinction definitely ‘impacts humanity’s long-term trajectory’, and would stop progress towards (positive)undefined AI transformation too. Now which way AI results in the time of perils closing (i.e. good transformation or extinction/civ collapse) is back in scope. The possibility scientific acceleration could boost the risk window closing ‘the wrong way’ seems important to factor in (although doing so is fraught - more later).
Sensitivity?
Only looking at bio (/ignoring AI) seems a big assumption not only in terms of ‘dubious on the merits’ but also ‘decisive if made’.
Per the summary, the crux of the matter is in science’s potential contribution to ‘civilization ending catastrophes’. From section 8 we get some break-evens, roughly, the NPV of the future of current-world-value units to compensate for the extinction risk being run:

The (annual rate) dx values are derived from their extinction risk estimates for bio. And yet from the XPT we see AI risks are judged to be substantial multiples higher than bio ones:
Table 5
I beg forgiveness for not properly running the numbers myself to derive figures in terms of Lambda breakevens etc., but qualitatively I’d expect the superforecaster aixrisks to net out as somewhat more ‘optimistic’ than the biorisk estimates from domain experts, and the ai biorisk ones to be more ‘pessimistic’.
Qualitatively, moving from ‘bio’ numbers to ‘ai’ numbers should make the results look a lot worse for accelerating science:undefined the ‘worst case’ bio numbers in the paper are now in a similar ballpark to the ‘best case’ AI numbers (maybe Lambda of ??~1000), and the new worst case should be substantially worse (??~50). Or, another way of looking at it: the rosier picture of accelerating science now relies on both a heavy lean to the superforecasters and a heavy discount for ai risks, if only one or the other, the value of accelerating science looks tenuous.
Coda - maybe try and restrict to life science benefits?
One motivation for trying to keep AI out of the analysis that I am very sympathetic to (and that I suspect the author shares) is trying to ‘factor it in’ is a complete nightmare. In the same way I don’t want all my >10 year forecasts on ~every subject to have huge and uninformative ‘but what about AI?’ error bars, the paper is targeting an interesting topic which we should prefer not degenerate into yet another rehearsal of well-worn highly-uncertain AI-related considerations. Can’t we just bracket this out somehow?
My naive treatment of the AI risk estimates just before certainly wouldn’t do the trick: both the ‘good’ and ‘bad’ AI exits from the time of perils should require further scientific activity, so generalised scientific progress increases the rate of both. As ~everything depends on which hazard happens first, we likely return to standard AI considerations around which of the ‘good’ or ‘bad’ elements of AI development have greater marginal response to generalized ‘boosts’ applicable to both. The main policy upshot would be a pretty sterile version of differential technology development (“Try to prod AI development in the better-than direction.”)
Yet the paper’s set-up does make something like this the (~intractable) crux of the matter. Increments of existential risk are going to weigh heavily no matter how one prices the future. If we take the XPT seriously, the great bulk of ‘addressable’ existential risk relates to AI. It is not that surprising small nudges to this parameter could dominate the analysis if they were accounted for. “If we ignore AI risk, the returns of accelerating science likely outweigh the risks which could emerge from its advancement” ignores most of the issue.
I wonder whether partitioning the upside has more promise as a way of “bracketing out” AI. Near the start of this section I mooted an alternative ‘like for like’ comparison besides ‘all of science’s returns vs. all of its risks’: just take ‘biocatastrophies’ as the downside, but just take ‘bioscience returns’ as the upside to compare it against.
By the standards of what the paper is already trying to tackle, getting a handle on the ‘attributable fraction’ of bioscience benefits looks feasible: R&D is 10-50% life sciences (depending on how and what you count), and I expect funding share undercounts life sciences’ expected future contribution to aggregate benefits which heavily weigh life expectancy. On the back of the envelope, reducing the returns of science by a factor of (say) 3 doesn’t change the qualitative upshots: speeding up bio generally looks ‘worth it’ unless you hew heavily towards the domain experts. Although an imperfect dissection, it does largely ‘bracket out’ AI to indirect/second-order-y corrections which are more ‘reasonably neglectable’.undefined This doesn’t let you ignore AI when assessing and deciding “all things considered”, but it might allow a fair look at the ‘non-AI’ parts of the problem in their own right.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",71,27,87,79,16,90,75,88,33,81,56,96,88,64,95,89,76,99,65,18,89,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",,,,,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",9 years,~50,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,The Returns to Science In the Presence of Technological Risks,https://unjournal.pubpub.org/pub/evalsumtechnologicalrisks,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-05-28T18:04:08.275-04:00,
The Returns to Science In the Presence of Technological Risks,https://arxiv.org/abs/2312.14289,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Yassin Alaya,Move to applied stream - check typo in the release,"Paper summary
‘The Returns To Science In The Presence Of Technological Risks’ by Matt Clancy considers whether the benefits of science outweigh its risks by modelling the welfare effects of globally pausing science for one year. The benefits considered are increases in per capita income and a decreasing mortality rate. The risks are advances in biotechnology which might enable malicious actors to create dangerous pathogens. These risks are modelled as an increased rate of mortality due to more frequent, more severe pandemics and as a risk of extinction due to a pandemic killing the entire human population.
In a model without extinction risk, the benefits of science strongly outweigh the risks. It thus seems that the qualitative result[s] when ignoring the possibility of science increasing extinction risk are not sensitive to parameter choices. When extinction risk is accounted for, the conclusions depend on how valuable one reckons the possible future of humanity (which would be lost due to extinction) to be.
In this evaluation I focus on the choice of model parameters. To introduce the parameters, I briefly summarise the report’s model. Welfare effects are computed based on the following infinite period model:
V= \sum_{t=0}^\infty p^t n_t u(y_t)
Here, y_t is per capita income in year t, u(y_t) is the utility experienced by an individual alive in a year in which per capita income is y_t and n_t is the population size in year t. p is a discount factor (see discussion below). y_t grows at rate G each year. If science is paused now, the income growth temporarily drops to g in T years before growing again at G afterwards. Population size n_t grows at a constant rate s until in t_1 years, the ""time of perils"" commences and population growth is reduced to s-d, where d represents excess mortality due to the perils of biotechnology. When science is paused for one year, this time of perils commences one year later than it otherwise would have. […] the constancy of the population growth rate at s (or s-d during the time of perils) is based on the assumption that the birth rate and the mortality [rate] move in lockstep. However, pausing science for one year temporarily slows the decline of the mortality rate attributable to scientific progress after T years without affecting the birth rate. The result is a permanent decline in the net population growth rate to \bar s.
The returns to science in this baseline model are large and positive and remain large when science’s effect on health are modeled more realistically in section 6. In section 8, a version of the model with extinction risk is considered, where there is an annual extinction risk of d_x applying during the time of perils. Section 9 considers an extension where science reduces both d and d_x.","General comments
While it is unsurprising that the version of the model without extinction risk stipulates large returns to science that are almost unaffected by the time of perils (see the results of the modified model in section 6), an important contribution of the report is to establish a threshold [that] the value of the future of humanity must exceed to warrant pausing science when science contributes to extinction risk (section 8). An important result is also the threshold [that] the annual reduction in extinction risk due to science has to exceed in order for the net impact of science (after accounting for the fact that science also [may] bring about the time of perils) to be positive.
Overall, the methodology of the paper is transparent and well explained. The author appropriately balanced tractability with realistic modelling. The author considers a number of extensions to his baseline model that address particular considerations. However some considerations are left out of the model, e.g. animal welfare (the current value of humanity might be negative because of factory farming) and how utility depends on the total population size (in the current model, the flow value is proportional to the population size). There may be minor flaws in arguing for certain parameter choices (see my comments on parameter choices below).
Comments on parameter choices
The author carefully justifies his choice of parameters, making reference to the academic literature as well as to forecasts of superforecasters and experts who participated in the Existential Risk Persuasion Tournament (XPT). For some parameters (in particular p) it might be worthwhile to conduct robustness checks with alternative values, at least in the model which includes extinction risk. In the following, I summarise how the author chooses each parameter and offer some commentary.
Table 3","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",,,,,,,60,75,45,65,30,100,50,30,70,60,35,85,70,50,90,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4.5,2,3,5,0,4,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",,,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,The Returns to Science In the Presence of Technological Risks,https://unjournal.pubpub.org/pub/evalsumtechnologicalrisks,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-05-28T17:53:08.136-04:00,
The Returns to Science In the Presence of Technological Risks,https://arxiv.org/abs/2312.14289,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Mike Hinge,,"The paper as a whole is a high quality assessment of the likely trade-offs of scientific progress versus biorisks.
It makes good use of an empirical base, and different methods. It shows model sensitivities and allows assumptions to be changed.
It does not definitely answer if accelerating science is good, primarily due to uncertainties around existential risks.
Its conclusions are sensitive to the discount factor and more work would be needed before considering actions at a larger scale.","I think this paper is high quality and I recommend it, although I feel there are some important comments worth highlighting - especially before significant real world actions are taken on its conclusions.
The paper seeks to evaluate the likely net benefits of scientific efforts via modelling its interactions with biorisks. Insofar as this paper is a tool, it is primarily intended to advise donors and agents where their actions have the potential to accelerate or decelerate research as a whole (rather than specific fields, where other tools would be relevant). I think the author has done well with this task in principle, the paper does a solid job of dealing with uncertainty by allowing the reader to see the sensitivity of the results to differing assumptions and their conclusions are well reasoned. I have some comments below on my assumptions of relevant risk metrics versus the author, but this is to be expected, and the paper is well designed to deal with such factors.
Furthermore, I think the paper as an academic exercise is high quality - referencing a number of reasonable concerns, doing a good job in creating a solid empirical base as far as possible and comparing many different methods to address the same question. This raises its usefulness, and will likely allow either further scenarios to be run or allow the model to be expanded further. However, I would guess it is also likely this framework will need adaptation before it can be used on actions that impact AI risks and progress (see below).
I was asked to review the Python code and the attached Google sheet in detail, which I do below. I then include more analysis of the main report, and my commentary on its methods.
Evaluation of the attached Python code
I read through the Python code included with the paper, and have the following comments (applicable based upon the version I reviewed):
The code generates results in line with those in the report, and the code appears to be in line with the formula presented in the main text and annex - other than a single discrepancy that doesn’t significantly change the results.

The discrepancy in question is that the “Realistic Health Model - Calculating ROI Subcomponents” file appears to differ in its calculation of the final survival rate from the “Realistic Health Model - Calculating Total ROI” file - taking >t2 rather than >=t2. See below for the comparison. Based upon the main document I assume that >t2 is the correct formula, and the impact starts in year 75 (note, t2=74). This also seems to be how the lag is calculated in the Google sheet.
Table 1
This results in slight differences in calculation between the two files, and means that some of the results at the bottom of the python files may be slightly off. The resulting differences appear to be small however (less than 4 significant figures), based upon my experimentation, and easily fixed.
There are a few other small points on the code: for example possibly different variable names could have been chosen for “a,b,c” in the calculation of:
def logistic_func(x, a, b, c):
    return1 / (1 + np.exp(-(a + b*x[0] + c*np.log(x[1] - 1800))))

as “a” is also used later in the code to refer to age, potentially leading to confusion in the future if the code is expanded and the earlier “a” variable from the first logistic function needs to be referenced. However, based upon the order of how the code currently runs this doesn’t seem to create an issue.
At the end, the breakeven risk rate is labeled as “# Domain Expert peril rate (found by trial and error)”. This seems to be in error, as it doesn’t seem to relate to the domain experts (which are listed separately). The value may also be slightly off due to the code discrepancy above, but it seems to be still accurate to 4 significant figures, which is the detail given in the paper.
The parameter h is defined as # Survival without science, and set at 56.25%. This seems to be an incorrect description, 56.25% as per the report is the proportion of the rate due to science, and so 43.75% would be the survival without. However, h seems to be used correctly in the code (as the % of mortality reductions due to science), despite this description.
Evaluation of the attached Gsheet
From my checks of the Google Sheet I could not find any calculation errors as per the formula laid out in the paper and the results of the model itself, and its results tie with those in the paper at the time of my review.
The sheet is a potentially useful tool alongside the paper itself, and while I was able to pull it apart and copy sheets in order to run the scenarios I discuss in my review, I would suggest that a small amount of work to clean it up and add some functionality could make it a useful tool if released alongside the paper (I assume that this is the ultimate plan?). For example, a simple table of contents, key assumptions on one sheet that all others link to, as well as the ability to run and compare scenarios to see sensitivities (such as the impact of varying the discount factor or peril dynamics) would go a long way in raising its usability.
Comments on the discount rate
Throughout the paper a 2% discount rate is assumed, which covers the potential to exit the current regime via transformational AI, a break in current economic logic or an existential event, after which the break in science it assumes no longer would have any impact. This is sensible to include, the logic seems clear to me, and I do not have a strong intuition on any different numbers to choose.
However, the model is sensitive to changes in this discount factor, with a 25% increase or decrease in this discount factor altering the return of the status quo over a pause (as laid out in Chapter 5 Baseline) by a factor of ~400% based upon my simple calculations in the sheet:
Table 2
For example, this implies that any decrease in the chance of transformational AI creates a stronger incentive to push ahead with default scientific activity at the margin by lowering the discount factor, while a future rise in the probability of transformative AI could imply that the returns of science are much lower and could even go negative in certain scenarios (such as those that consider existential risks from biotech).
This may merit some more discussion in the paper, or further work as part of a follow on paper considering broader AI issues.
Other comments on methodology
Pauses in science, lagged implications for productivity
The paper makes the assumption that it takes around 74 years for a pause in science to manifest itself as a productivity shock, to account for the lag between fundamental advances, their translation into technology and the diffusion of that technology worldwide. This is a conservative assumption, and lowering this value raises the utility of continuing science. In reality, the benefits are likely to be smoothed to a degree, potentially starting with a lag of a few years to allow for frontier science to translate into technologies and then a period of diffusion from a low base up to a period when it is fully diffused and 100% of the impact is felt, for example the 74 years chosen.
This smoothing is mathematically more complex and may not change too much, however the simplified version seems to be creating some strange results. For example, as it stands the impact of pausing science causes the time of peril to occur in year 16 rather than 15, meaning that the benefits of pausing arrive significantly before their costs. This creates an interesting and possibly perverse conclusion: while the author is correct to state that based on the core assumptions the model predicts that the status quo has a higher expected utility than pausing science for one year, pausing science forever leads to an even higher utility still - if the eternal freeze prevents the time of biotech peril altogether and then the productivity shock starts in period 75 and continues forever. This result appears to be robust to changes in the assumed discount rate, but is not robust to bringing the productivity shock forwards or other changes, which would mean that once again the status quo wins out.
While this full pause scenario is artificial and a real pause of this length would need to model far more elements, it does point to an issue with the model when scaled up to larger shocks and when drawing more sweeping conclusions on the costs and benefits of influencing the rate of scientific progress via its methodology.
XPT, underrating domain experts (vs superforecasters)
The paper relies significantly on the Forecasting Existential Risks Tournament (or XPT), in order to establish plausible bounds for biological catastrophic and existential risks and how they may develop. I find the justifications in the paper broadly persuasive, it was a large scale study conducted with significant numbers of both domain experts and superforecasters (those with a past record of above average predictions over a broad range of topics), and gave both the channels and incentive for those involved to share ideas and update their opinions. I also found the points on leaning towards superforecaster opinions partially persuasive, particularly that they had the most accurate predictions of their counterparts and that their calibration was more in line concerning non anthropogenic risks, which are easier to establish a plausible base rate for empirically.
However, I would suggest that there are also reasons to weigh the opinions of the domain experts more highly than the superforecasters, especially on a domain specific question like this. I was personally involved in the XPT study (although my predictions are not directly included in this paper, as I am neither a biosecurity domain expert or a superforecaster), and I feel one of the shortcomings was how many questions we were asked (I think something close to 30 as a minimum, with the option to discuss more), which spread out the discussion and diluted the study’s usefulness for changing opinions on the most key risks and questions. I believe that there was a tendency (both for myself and others) to focus on the questions that we had the most experience or interest with, and that means that the superforecasters may be more spread out in their analysis and focus versus a domain expert concerning their field, with implications for their accuracy on specific questions. In addition, the high level of base calibration of a superforecaster may be less relevant on a specific emerging risk prediction that needs domain knowledge and lacks past observations, although I know others may disagree.
Together, this pushes me to rate the domain expert opinion higher than the author does on a narrow domain question. This is not critical when only catastrophic risks are considered, as the predictions of both groups result in positive returns to science. However, when existential risks are considered this suggests that accelerating science may be a net negative. The author does allow the reader to view the sensitivities of the results to these assumptions, which I feel is to its credit, and my intuitions here are not very certain, both given the complexity of the subject and because I am not a biosecurity expert.
This does mean that one key conclusion of the paper - that the current pace of science is expected to be net positive despite its risks - may not be shared by those who have a perception of existential risks closer to the domain experts sampled rather than superforecasters. The author discusses this, and why they lean more towards the superforecaster predictions and their reasons for assuming the pace likely is net good. However, I feel I have more uncertainty than the author in this conclusion even after reading the discussion, and it does suggest that a significant portion of biosecurity experts may wish to slow the pace of science based upon the logic of the paper. This may give us pause before confidently acting on its conclusions and proceeding with the status quo until more uncertainty can be resolved. Possibly the author may wish to discuss this a bit more, for example if the differing assessments have implications in the context of a unilateralist’s curse (likely this would not be the case for actions that cause small changes to the rate of science, but could be an issue for larger changes).
Comments on the methodology’s applicability for AI risks
An obvious further piece of work (almost certainly in a subsequent paper) would be taking the methodology of the paper in relation to biosecurity risks, and applying it to AI. Here, existential risks would be far more central to the narrative, and the choice of domain experts versus the superforecasters in the XPT would likely result in a more significant divergence in the advice given (based upon my reading of the XPT).
I believe the structure presented in this paper would plausibly be able to cope with assessing accelerating or slowing AI progress at the margin instead of more general science/bio risks, although this would likely involve some significant tweaks. For example, slowing AI progress would also influence the discount factor assumed, both via changes to the probability of transformational growth and AI existential risk. In addition, it would also necessitate including analysis of the plausible utility states following transformational AI being achieved and us exiting the current regime, in order to assess the ethics of delaying or accelerating this outcome, which would definitely go well beyond the scope of what the current paper could include.
Comments on complexity, and concluding remarks
One concern I do have is that the paper’s conclusions need to be placed into the context that they deal with a complex system, and that while it is high quality, further work would be needed before an actor can have confidence when taking larger scale actions.
Firstly, I would suggest scientific progress, innovation and technology as a whole are a fairly clear example of a complex adaptive system: with one definition being where the interaction of its parts create new dynamics which cannot be explained by the characteristics of its individual parts (Centeno et al. 2023, pg 7)[1]. This makes the risks of the system non linear and aspects are arguably impossible to predict (Helbing 2009)[2], and this has been the case for many past scientific and technological advancements. For example, technologies can profoundly change our culture, and that new culture changes how our society operates and innovates, taking us to very different places to what was imagined by their creators. Everything from the returns to physical labor, intellectual labor, gender relations, birth rates and the dynamics of conflict have all experienced a strong component of scientific and technological influence, all of which was hard to predict and much of which is still debated today.
Highlighting that scientific progress is complex in itself isn’t a critique to the quality or logic of the paper. We still need to reason around the risks of scientific innovation and organizations need to make concrete decisions about what to study and fund. Simply because a system is complex doesn’t mean that we can’t make sensible predictions about the likely result of an action, especially at the margin. I think this paper achieves this at the scale needed for an organization such as Open Philanthropy at its current ranges of influence, where a donor needs to consider the implications of a funding change with marginal implications for the rate of general scientific progress. This is at least one of the stated aims of the paper (to advise those who may take actions that influence the rate of science), and as a tool at this scale I feel it offers plenty of utility as a framework, as well as some flexibility to be adapted to individual assumptions and the conditions.
The problem with scientific progress’ complexity instead emerges when acting at larger scales, for example when evaluating actions that could bring significant changes to the rate and nature of certain innovation streams, and this is where I feel more caution could be expressed in relation to the conclusions the paper presents. For example, for a society that truly halts all science for a year there would be many serious knock on implications - what would the researchers be doing for this time, and would this cause a loss of fundamental skills or capacity? How would they be paid? What would a society that is able to impose this even look like, and would it be desirable for an actor to possess this degree of power to limit forms of thinking and activities? These questions don’t need to be answered in the paper, but the author could perhaps suggest the limits at which their analysis would break down, at least in principle, or discuss these limitations.
While the paper does include a factor for uncertainty in the form of its discount factor (which partly takes into account the fact that predictive power around our actions breaks down over time), this doesn’t fully address the challenge of complexity. Instead of signals reliably diminishing over time like ripples in a pond, the risk is that at larger levels of impact they may interact to magnify or even flip, which could completely change any conclusions versus the initial analysis.
Given this complexity, an actor making a decision that actually has the potential to restrict a portion of scientific activity in a field would need to consider far more factors than are discussed in the paper before being confident in the positive or negative nature of their choice. This may seem like an abstract concern given this scale, but I worry it is becoming more salient over time. Organizations such as Open Philanthropy are making increasingly influential decisions, and straightforward analysis and advice that works at the margin may start to break down in unexpected ways when scaled, and the exact point at which advice breaks when shifting from micro to macro is not easy to determine.
The paper is already very long without being bloated, and provides detailed background for all its calculations, how figures were chosen and the sensitivities of the conclusions to each. This is greatly to its credit, and I don’t think much more could be included without compromising its usefulness for advising at the margin and focus.
However, like all similar efforts to predict complex systems this naturally suffers from a “streetlight effect” - where factors which are easier to measure and observe are what get studied, with other elements more likely to be excluded. For example it seems clear that scientific and technical progress has radically altered the number of humans alive today - both due to changing mortality and birth rates. It is simpler to model the impact from reductions to mortality due to better technology, which is included in the paper, but it also seems clear shifting technologies radically influenced birth rates directly and indirectly via altering the resources, culture and structure of societies - which is not included.
These indirect dynamics and feedback loops are nearly impossible to unpack and therefore really hard to predict, and it is understandable why a robust linear relationship could not be determined and therefore included. However, many such relationships exist and may be relevant to decision making, and further work here would both be very interesting and of potential use for those seeking to act within the space. Again, this would be especially relevant for actions with larger scales of impact.
This also links into another observation, that the paper focuses nearly exclusively on a utilitarian perspective for its evaluation of the arguments for or against accelerating scientific progress. This is not a critique, all analysis must take some value system as its base. However, it does mean that those who are not utilitarians are likely to feel that the paper answers questions that they feel are not core to their cruxes of why science is or isn’t moral to expand or pursue. These could include truth seeking as a fundamental virtue, personal liberty, what it is personal worth in an increasingly specialized world or one where different types of labor are needed (driven by science and technology), and so on. I am mostly a consequentialist, and so I find it hard to fully articulate all of this opposing viewpoint in its true detail, and my examples may be weak. However, I am certain others outside of the utilitarian framework will find the arguments of the paper less convincing than those within it, and efforts to try and bring empirical measurements or quantifiable data to the above questions from other disciplines in further work will only ever partially bridge this gap. This I’m sure is known by the author, but needs to be considered by those using it and building upon its work.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",80,60,88,85,70,90,65,85,75,65,75,85,70,85,90,90,80,95,75,65,83,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",,,,,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

","I have worked as an economist for a decade now, and I have spent approximately the last three or so years working on catastrophic risks.","4-5 papers in total, maybe another 10 or so proposals and more general projects.",False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.","It is likely that scientific progress has a positive return to welfare when considering its benefits versus its implications for a range of biotech risks. This is very likely to be the case even after possible biotech catastrophic risk probabilities are considered, and may be the case based upon potential biotech existential risks, although this is more ambiguous and depends upon the forecasts chosen and the response to existential risks to changes to the current rate of science.",,"I am less certain that the current pace of science is safe, and based upon the logic of the paper I would suggest that there is a 15-25% chance that existential risks from biotech exceed the potential reductions in mortality - as I place a higher weight on the opinions of the domain experts sampled than the superforecasters versus the author. Given the severity of existential outcomes this may give us pause before taking actions that significantly accelerate the pace of biological science as a whole.","Further information on biorisks would help reduce the uncertainties. This would cover both the absolute catastrophic and existential risks, as well as the other dynamics that the author highlights (is there a time of perils? How long is this likely to last? How responsive is the start and end of this time of perils to our rate of science? Is it already too late?).
In addition, the conclusions of the paper are sensitive to changes in the probability of both transformational AI and AI existential risks via the discount factor, and more accurately determining these values could influence the expected value of scientific progress. 
The paper does not have enough space to cover other factors that are relevant, such as the interaction between science/technology and birth rates in detail, its interaction with culture and human welfare via changing the nature of work and our environments. These are complex factors and not easily resolvable, but should be considered before an actor can have confidence when taking large scale actions in the space (if this is even possible).","For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
","I think this paper is high quality and I recommend it, although I feel there are some important comments worth highlighting - especially before significant real world actions are taken on its conclusions.",,False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,The Returns to Science In the Presence of Technological Risks,https://unjournal.pubpub.org/pub/evalsumtechnologicalrisks,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-05-28T15:58:20.056-04:00,
The Effect of Public Science on Corporate R&D,,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",,Josh,"The paper provides evidence on a recently relevant issue in science policy: the spillover effects of public science. However, these effects are not well-identified. There are numerous issues likely distorting the identification, including bad controls and a shift-share design with endogenous shares.","This paper tries to identify the impact of three forms of public science on corporate R&D: [1] public invention (i.e., university patents), [2] human capital (i.e., PhD dissertations), and [3] public knowledge (i.e., non-corporate publications). The headline results are that [1] crowds out corporate innovation, [2] boosts corporate innovation, and [3] has no meaningful effect. The identification strategy relies on a shift-share design, where the shifts are based on U.S. congressional subcommittee spending on public science and the shares are based on firm exposure to specific subcommittee’s shocks (based on firms’ exposure to expenditures from specific government agencies and the subcommittees responsible for those agencies’ funding). To address potential exclusion restriction violations, the paper does not use raw changes in subcommittee appropriations as its primary shock variable, but instead uses the changes in appropriations predicted by those subcommittees’ party shares. This report identifies two key issues in the identification strategy.

First, the paper’s shift-share design likely yields biased results due to the endogeneity of the shares. The primary IV models shown in the paper instrument firms’ exposure to public science using a series of shift-share formulas that, generally speaking, instrument a firm’s exposure to public innovation with a sum of the products of [1] the share of a firms’ exposure to a specific class of innovation (e.g., publication subfields, patent subclassifications, etc) and [2] the predicted R&D expenditures from all federal agencies for that innovation class. [1] is likely endogenous; the paper indeed finds a positive correlation between firm R&D stock and agency R&D funding. Of course, this is only one source of endogeneity, and many more may exist. This is a general gripe with shift-share designs; much is discussed about how exogenous the shocks are but little attention is paid to whether the shares are endogenous, which they often are.

The paper addresses this second issue by controlling for lagged firm-level R&D stock, but this exposes the paper to a third identification issue: the main specifications all control for colliders. An often-overlooked issue in control specifications is whether the controls are themselves outcomes of the main exposures of interest (in this case, public science). Cinelli, Forney, & Pearl (2024) show that if a covariate is (1) influenced by an exposure of interest and (2) shares a common unobserved confounder with the outcome, then controlling for the covariate will bias the estimated relationship between the outcome and the exposure of interest. To illustrate, consider the ‘birth weight paradox’ (Hernández-Díaz, Schisterman, & Hernán 2006). Though maternal smoking is associated with higher infant mortality and more incidence of low birth weight, when controlling for birth weight, maternal smoking appears to decrease infant mortality. This is not because maternal smoking actually decreases infant mortality, but rather because newborns who are low birth weight because of maternal smoking likely exhibit lower mortality rates than newborns who are low birth weight because of other health complications. Applying this concept to the paper’s specifications, firm exposure to public science may not decrease firm innovation; rather, it may be the case that firms with more R&D stock due to public science innovate less than firms who have more R&D stock due to the inherent innovative nature of their activities.

These issues together place the entire identification strategy in a bind. Firms with innovation profiles similar to those invested into by public agencies likely have different levels of R&D investment for reasons unrelated to public science, but controlling for these levels of R&D investment yields bad control problems because private R&D investment is inherently influenced by public science. The clear endogeneity issues with the shift-share design do not seem like they can be controlled away with observable covariates.

With public science funding under threat both in the United States and globally, the paper’s findings could mislead policymakers on the importance of public science for the private sector given the credibility issues with the empirical design. The paper’s findings effectively imply that the research function of universities is, if anything, negative for the private sector. Given that the only positive relationship between university research activity and firm outcomes is found in the graduation of new PhDs, the policy implications of this paper taken at face value would imply that universities should prioritize training PhDs while simultaneously cutting back on university invention. Given that universities conduct the bulk of all basic scientific research, such a reorientation of university priorities should not arise without more credible evidence.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",64,58,70,57,44,70,36,44,28,46,42,50,68,62,74,65,62,68,67,62,71,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",2.8,3.6,2.4,3.2,3.1,4.1,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",3 years,8,False,5 hours,8/10,Yes,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",Increased exposure to public invention decreases a firm’s private R&D expenditure.,,"In a binary sense, I believe neither the sign nor magnitude of this estimated relationship. My prior is, however, moved by the theoretical possibility that the relationship could be negative.",,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,True,Yes,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",Applied econometrics,The Effect of Public Science on Corporate R&D,https://unjournal.pubpub.org/pub/evalsumpublicscience/release/1,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-05-06T11:23:32.152-04:00,5
Population ethical intuitions,https://www.sciencedirect.com/science/article/abs/pii/S0010027721003644,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Stijn Bruers,Stijn,"Highly policy relevant, original research with clear hypothesis tests and valid statistical analyses.
Biases and framing effects are mitigated but remain difficult to avoid in such surveys. Some results (e.g. the negative utilitarian judgments and a lack of asymmetry) seem to be contradictory. One possible explanation for an apparent contradictory result is not addressed. ",,"Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",77,56,94,79,64,93,71,91,55,86,76,93,82,73,93,90,84,95,90,84,96,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4.6,4.7,4.5,5,4.3,5,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",+10 years,+10,True,,very good,yes,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.","People do not hold the neutrality and procreation asymmetry intuitions. They believe that adding a happy person is good and is as good as adding an equally intense unhappy person is bad. This ‘non-neutral symmetry’ result is observed in studies 2a and 2b of the paper. If people expect future populations have net positive welfare levels (that are at least as high as past and present generations), the symmetry view entails that policymakers should prioritize existential risk reduction above suffering reduction.
",,"I have weak confidence in the result: I expect the neutrality and (a)symmetry views strongly depend on the context (e.g. on the choice set: the possible populations that one could choose). In some contexts, such as situations where one could avoid a repugnant conclusion, people may hold the asymmetry view more strongly (especially after reflection). In future research, one could investigate such possible context or choice-set dependence. ",A survey that measures people’s population ethical judgments in choice-set dependent contexts and under more reflection. E.g. do people still prefer population A over population B when population C becomes an option and when they learn about e.g. repugnant or sadistic conclusions that could arise in such contexts or choice sets?,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",Evaluation report Population Ethical Intuitions.pdf,I consider it as (weak) evidence to prioritize existential risk reduction over suffering reduction.,True,not sure,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",welfare economics and normative ethics,Population ethical intuitions.,https://unjournal.pubpub.org/pub/evalsumpopintuitions/,Cognition,"50_published evaluations (on PubPub, by Unjournal)",,,2025-05-06T04:35:55.551-04:00,
Choose Your Moments: NIH Peer Review and Scientific Risk Taking,https://jeffreyshrader.com/papers/choose_your_moments.pdf,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",B. Ian Hutchins,B. Ian Hutchins,"“Choose Your Moments: NIH Peer Review and Scientific Risk Taking” is an interesting study on review decision-making. Authors examine the effect of NIH peer review score distribution on decision-making about funding particular grants, using a portfolio from NIH 2016 RePORTER data, using a BIBD approach. Their approach is sound and I am persuaded by their conclusion that reviewers have a preference for grants with increased variance in review scores, and that there is an asymmetry in preference for high-variance applications in response to funding shocks. 
",,"Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",75,60,80,75,60,80,60,70,50,80,75,90,45,35,55,30,15,45,80,75,89,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4,3.5,2.5,4.5,2.5,4,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",N/A,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",12 years,I’d have to review my records,True,"Including reading time, 5 hours. Excluding reading time, an hour and a half","I have little confidence in the numeric ratings, but I’ll be curious to see if there’s any predictive power there. Let me know if there is!",Sure thing,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.","Reviewers have a preference for grants with increased variance in review scores, and that there is an asymmetry in preference for high-variance applications in response to funding shocks in the positive vs. negative direction",,"I am persuaded unless I see contrary evidence. I have seen primary internal NIH data, and this comports with that experience.","Inter-rater reliability information, if available","For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,More details on randomization would be much appreciated,"As far as I can tell, there is no open data or code, which is becoming the norm",,"Given the wild variation in journal decision-making and author incentives, I have little confidence that you will get useful data out of this question","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",Review Choose Your Moments.docx,"Program Officers should be provided with summary statistics about review variance. They do not have time to calculate them themselves, and it could be easily incorporated into existing internal data systems (iSearch)",True,"Nope, I’ve got a lot of manuscripts on my plate right now.",,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",Science-of-science,Choose Your Moments: [NIH] Peer Review and Scientific Risk Taking,,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-05-04T14:29:54.233-04:00,5
PUBPUB submission - E2 - The Effect of Public Science on Corporate R&D,https://www.nber.org/papers/w31899,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Ioannis Bournakis,,,"From “Research Quality (Additional Comments)”
The paper develops a systematic framework for analysing how public science capital affects corporate R&D, leveraging rich firm level data and extensive robustness checks to support credible causal inference. My main question is whether this instrumental variables strategy offers clear advantages over a difference-indifferences design.
The paper investigates how three components of public science— abstract knowledge (publications), human capital (PhD scientists), and public inventions (university patents)—affect corporate R&D, distinguishing upstream (research) from downstream (invention) activities within a simple profit-maximization framework that posits public knowledge and human capital lower the marginal cost of internal invention while public inventions may either serve as inputs or compete with firm-generated inventions.

Drawing on a novel firm-level panel of U.S. publicly traded, R&D-performing firms (1980–2015) linked to Compustat, PatentsView, Dimensions grant acknowledgments, ProQuest dissertations, and the AMWS directory, the authors measure firm-relevant public science via exposure-weighted stocks of non-corporate publications, dissertation–patent textual similarity, and university patents. To address endogeneity, they construct Bartik-style instruments based on exogenous shifts in federal agency R&D budgets— predicted by the partisan composition of House and Senate appropriations subcommittees—interacted with firm-specific exposure shares.

They find that “pure” public knowledge (journal publications) has little to no effect on corporate patents, publications, or hiring of top scientists, indicating limited non-rival spillovers; by contrast, a one-standard-deviation increase in relevant PhD dissertations boosts firm patents by ~53%, publications by ~22%, and AMWS scientist employment by ~9%, while a similar rise in university patents reduces firm patents by ~51%, publications by ~33%, AMWS hiring by ~8%, and lowers profits—evidence that public inventions largely substitute for and compete with corporate R&D. Technology-frontier firms benefit more from human capital and less from public inventions, whereas follower firms—especially in the life sciences—are more responsive to public inventions and knowledge.

These results challenge the notion of public science as a pure public good, highlighting that only excludable, rivalrous outputs (people and patents) drive private innovation, and they underscore the need to consider the competitive effects of university commercialization. My report applauds the paperʼs rigorous causal strategy and rich data integration while suggesting robustness checks—such as a difference-in-differences design—and clarifications on model specifications (e.g., inclusion of level dummies in interaction regressions, treatment of firm versus time subscripts, and the potential role of an international knowledge frontier)—refinements that could further enhance [and strengthen] insights into the relationship between public investment and corporate innovation.
1. Claims, Strength, and Characterization of Evidence
The paper clearly states its primary research question: How do different aspects of public science influence corporate innovation activities? The approach is grounded in a thorough theoretical framework and supported by strong empirical evidence. The empirical strategy addresses potential endogeneity bias between corporate innovation activities and public knowledge, primarily by leveraging political shifts affecting agency R&D budgets. Although tackling (unobserved) endogeneity bias is an extremely challenging task in economics research, the authors' attempt demonstrates a level of credibility that likely outweighs the potential limitations of the chosen approach. Providing results that establish causal inference adds significant value to the paper and strengthens the robustness of its empirical findings.
Assessment (Claims and Characterization of Evidence): 85/100
2. Methods: Justification, Reasonableness, Validity, Robustness
The paper applies an instrumental variables (IV) estimation to identify the causal impacts of public science on corporate innovation. Specifically, it uses a Bartik-style shift-share IV, where the “shifts” are changes in federal agency R&D budgets, expected to represent exogenous shocks driven mainly by the political composition of Congress. The “shares” are firm-specific exposures based on past patenting activity, publications, or dissertation overlaps. The validity of this instrument depends on the assumption that political shifts in R&D funding are exogenous to firm-specific innovation shocks. Admittedly, this is a reasonable assumption within this context, although the presence of weak identification might persist, particularly for large firms, especially considering spatial dimensions that may be interrelated with the determination of the federal budget. The authors are aware of this concern and address it by controlling for potential firm size effects through the inclusion of lagged R&D stock or annual sales in all relevant specifications. This is convincing and sufficient to argue that the presented results are not significantly biased, especially given the inherent challenges of identifying causal relationships in models of this kind.
Nonetheless, I would like to suggest that the authors consider whether a difference-in-differences (DiD) approach could also be used to address the endogeneity bias between public knowledge and in-house innovation efforts. I do not claim that a DiD approach would necessarily outperform the strategy they have already employed, but it could serve as an additional robustness check. For example, one could identify a subset of firms more exposed to changes in public science funding (e.g., firms active in fields funded by certain agencies) and another subset less exposed, and then compare their innovation outcomes before and after the shocks, assuming the timing of R&D budget changes is exogenous. From the top of my head, one could think of firms heavily linked to NASA subfields versus those linked to the Department of Agriculture. This could be an interesting extension if the firm panel allows for such an analysis.
In any case, the current identification strategy has strong merits and mitigates endogeneity concerns in a reasonable and convincing manner.
Assessment (Methods): 87/100
3. Overall Assessment: Contribution to Knowledge and Practiceundefined
This paper presents a significant and well-executed empirical study examining how public science, in its various dimensions, affects corporate research and development (R&D). The authors distinguish between three key components of public science: abstract knowledge (e.g., scientific publications), human capital (e.g., PhD scientists), and public inventions (e.g., university patents). Their findings that corporate R&D is driven more by embodied knowledge in human capital and inventions than by abstract knowledge itself offers important insights for science policy and innovation management.
The study contributes to our understanding of how knowledge transfer mechanisms operate in practice and challenges conventional wisdom that treats public science primarily as a non-rival, purely spillover-driven input to private innovation. By leveraging firm-level exposure to exogenous shifts in federal R&D budgets tied to political subcommittee composition, the paper advances causal inference in this domain. Its nuanced implications—for example, the crowding-out effect of public invention on corporate R&D—are particularly relevant in an era of increasing university-industry partnerships and policy attention to innovation ecosystems.
Assessment : 90/100
4. Logic and Communication
The goals and questions of the paper are clearly articulated: the authors seek to disentangle the distinct effects of public knowledge, human capital, and public inventions on corporate innovation activities. Key concepts are well defined, and the framework guiding the empirical analysis is transparent and logically developed.
Nonetheless, I have some specific queries that I would like to raise.
[4.1]undefined In Equation 3.1, is the variable r a stock variable (representing cumulative knowledge) or just a flow variable? I am aware that later in the paper, the authors treat public knowledge as a stock variable using the perpetual inventory method, but in the theoretical setup, this is not made explicit. It might be useful for the authors to clarify this point clearly from the very beginning.
[4.2] Additionally, regarding the function phi in the profit maximization problem: could it be subject to constant returns to scale (CRS) instead of diminishing returns to scale (DRS)? The authors implicitly assume that the productivity of investment falls with respect to both human capital and public knowledge. This is my understanding based on the setup of the model in Section 3.1, unless I am missing something. While this is a conventional assumption, would it not also be possible to allow for CRS or even increasing returns to scale (IRS) if the use of new knowledge capital (either internal or external to the firm) generates spillovers that lead to new ideas at a constant or increasing rate? In other words, is there any role for unintended spillovers in the model that could counteract the diminishing returns assumption? This is just a question and may not be crucial for the modelling approach, but it could be worth considering or clarifying.
[4.3] Furthermore, I was wondering whether the authors included the level dummy for high ability in the regressions of Table 10, since they are using the interaction term between public investment and human capital. Isn’t it the case that these regressions should also include the dummy variable itself?
[4.4] Regarding this point, do we need to report the overall effects of the variables of interest (public investment and human capital) in Tables 10 and 11 etc, given that they are interacted with industry fixed effects, especially in Table 11? Also, why does the dependent variable in Tables 10, 11, and 12 include only the time subscript but not the firm subscript? Are we missing something here?
[4.5] In Section 3.2.4, the authors distinguish between leaders and followers. While the paper specifically focuses on the U.S., I would like to raise a question about whether there could also be a role for a global frontier instead of just a national frontier, as well as the potential impact of international public knowledge capital. The authors have deliberately excluded non-U.S. PhDs from the empirical analysis. This choice merits further justification. Is this decision conceptually meaningful? Is the influence of international public knowledge capital on U.S. corporations truly negligible? Admittedly, U.S. firms are typically positioned at the international technological frontier, but there might be sectors where the U.S. lags behind other countries. Have the authors considered these possibilities?
Overall, the central point of the paper—that there is a potential trade-off between public invention as a competitor versus as an input—is carefully and convincingly developed.
Assessment: 83/1004
5. Replicability, Reproducibility, and Data Integrity
The paper demonstrates strong adherence to reproducibility standards. The dataset— DISCERN—is publicly available through Zenodo, and the variable construction is extensively documented in appendices. The use of machine learning (SPECTER) for textual similarity analysis between patents and PhD dissertations is clearly explained and appropriately validated.
The authors also conduct multiple robustness checks and use alternative operationalizations of their key variables (e.g., alternative definitions of public invention using SPECTER-based similarity). Data integrity appears solid, with comprehensive coverage of firm-level R&D activity over a long time span (1986-2015) and linkage across several high-quality data sources, including Compustat, PatentsView, Dimensions, and ProQuest. I recommend that the authors enhance reproducibility by releasing anonymized intermediate datasets and code snippets for key transformations, and by archiving their data‐processing scripts (e.g., on GitHub). This would enable other scholars to leverage the same data pipeline for related research in innovation, human capital, and public knowledge.
Assessment: 90/100
6. Relevance to Global Priorities and Usefulness for Practitioners
This paper offers valuable insights for both policymakers and practitioners concerned with the real-world impacts of public investment in science. The finding of this research that public inventions may substitute for internal R&D—especially among follower firms—has practical implications for how universities license technologies and for how governments design innovation policies.
The focus on firm heterogeneity, such as differences between technology frontier and follower firms, adds depth and policy relevance. Quantitative estimates (e.g., a one standard deviation increase in relevant public invention reduces firm patents by 51%) provide actionable metrics for prioritization. By highlighting that only embodied knowledge (in people or patents) translates into corporate R&D investment, the study guides more effective knowledge transfer strategies.
In sum, this paper represents a rigorous, transparent, and policy-relevant contribution to the field of corporate R&D investment and public knowledge capital. It challenges the commonly used assumption that there are spillovers from public investment in knowledge, systematically exploring the hypothesis of whether there is complementarity or substitutability between public investment and in-house innovation activity. This approach provides a more refined understanding of how public investment in research shapes private-sector innovation, which remains among the most important sources of economic growth.
Assessment: 88/100","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",75,75,87,80,80,90,80,90,80,90,90,95,65,65,83,85,75,85,72,72,88,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4,4.2,3.5,4.8,4,5,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",Nearly twenty years.,This is my second one.,False,,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.","Section 6 of the paper provides the most crucial remarks for policy designers and practitioners. I would further highlight three points.

First, the papers evidence on substitution versus complementarity is vital: a one-standard-deviation increase in university patents reduces corporate patenting by 51%, underscoring the need to carefully calibrate technology-transfer and licensing policies.

Second, embodied human capital in the form of PhD graduates plays a powerful complementary role—each one-standard deviation rise in relevant doctorates yields a 53% boost in firm patents—highlighting the high leverage of targeted doctoral- training initiatives.

Third, regional innovation strategies and university–industry partnership programs must balance the benefits of widespread technology diffusion against the risk of competitive displacement among incumbent firms.",,"Before reading this paper, I would have assumed that public knowledge and corporate innovation were largely complementary, with firmsʼ absorptive capacity determining who benefits from publicly available research. In that view, only frontier firms—or those with sufficient in-house expertise—could effectively incorporate external knowledge. Instead, this study shows that an increase in university patents actually crowds out private patenting, challenging the assumption of universal complementarity.

That finding raises a broader question… [moved to answer to question IV]","I recommend adding the robustness check I mentioned in my report: implementing a difference-in-differences identification strategy. Iʼm not suggesting it would necessarily outperform the current IV approach, but it would offer the authors a valuable alternative perspective. A well-designed Dif-in-Dif should be feasible in this context and would help verify whether the results hold under a different identification scheme.
It may not change the results at all—in which case, it would simply robustify the paperʼs existing findings. If the results do change, that would open a different line of inquiry. Either way, this suggestion is hypothetical at this stage but could strengthen the study’s contribution.","For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.","The paper clearly states its primary research question: How do different aspects of public science influence corporate innovation activities? The approach is grounded in a thorough theoretical framework and supported by strong empirical evidence. The empirical strategy addresses potential endogeneity bias between corporate innovation activities and public knowledge, primarily by leveraging political shifts affecting agency R&D budgets. Although tackling (unobserved) endogeneity bias is an extremely challenging task in economics research, the authors' attempt demonstrates a level of credibility that likely outweighs the potential limitations of the chosen approach. Providing results that establish causal inference adds significant value to the paper and strengthens the robustness of its empirical findings.","The paper applies an instrumental variables (IV) estimation to identify the causal impacts of public science on corporate innovation. Specifically, it uses a Bartik-style shift-share IV, where the “shifts” are changes in federal agency R&D budgets, expected to represent exogenous shocks driven mainly by the political composition of Congress. The “shares” are firm-specific exposures based on past patenting activity, publications, or dissertation overlaps.
The validity of this instrument depends on the assumption that political shifts in R&D funding are exogenous to firm-specific innovation shocks. Admittedly, this is a reasonable assumption within this context, although the presence of weak identification might persist, particularly for large firms, especially considering spatial dimensions that may be interrelated with the determination of the federal budget. The authors are aware of this concern and address it by controlling for potential firm size effects through the inclusion of lagged R&D stock or annual sales in all relevant specifications. This is convincing and sufficient to argue that the presented results are not significantly biased, especially given the inherent challenges of identifying causal relationships in models of this kind.
Nonetheless, I would like to suggest that the authors consider whether a difference-indifferences (DiD) approach could also be used to address the endogeneity bias between public knowledge and in-house innovation efforts. I do not claim that a DiD approach 2 would necessarily outperform the strategy they have already employed, but it could serve as an additional robustness check. For example, one could identify a subset of firms more exposed to changes in public science funding (e.g., firms active in fields funded by certain agencies) and another subset less exposed, and then compare their innovation outcomes before and after the shocks, assuming the timing of R&D budget changes is exogenous. From the top of my head, one could think of firms heavily linked to NASA subfields versus those linked to the Department of Agriculture. This could be an interesting extension if the firm panel allows for such an analysis.
In any case, the current identification strategy has strong merits and mitigates endogeneity concerns in a reasonable and convincing manner.","This paper presents a significant and well-executed empirical study examining how public science, in its various dimensions, affects corporate research and development (R&D). The authors distinguish between three key components of public science: abstract knowledge (e.g., scientific publications), human capital (e.g., PhD scientists), and public inventions (e.g., university patents). Their findings that corporate R&D is driven more by embodied knowledge in human capital and inventions than by abstract knowledge itself offers important insights for science policy and innovation management.
The study contributes to our understanding of how knowledge transfer mechanisms operate in practice and challenges conventional wisdom that treats public science primarily as a non-rival, purely spillover-driven input to private innovation. By leveraging firm-level exposure to exogenous shifts in federal R&D budgets tied to political subcommittee composition, the paper advances causal inference in this domain. Its nuanced implications—for example, the crowding-out effect of public invention on corporate R&D—are particularly relevant in an era of increasing university-industry partnerships and policy attention to innovation ecosystems.","In Section 3.2.4, the authors distinguish between leaders and followers. While the paper specifically focuses on the U.S., I would like to raise a question about whether there could also be a role for a global frontier instead of just a national frontier, as well as the potential impact of international public knowledge capital. The authors have deliberately excluded non-U.S. PhDs from the empirical analysis. This choice merits further justification. Is this decision conceptually meaningful? Is the influence of international public knowledge capital on U.S. corporations truly negligible? Admittedly, U.S. firms are typically positioned at the international technological frontier, but there might be sectors where the U.S. lags behind other countries. Have the authors considered these possibilities?
Overall, the central point of the paper—that there is a potential trade-off between public invention as a competitor versus as an input—is carefully and convincingly developed.","The paper demonstrates strong adherence to reproducibility standards. The dataset— DISCERN—is publicly available through Zenodo, and the variable construction is extensively documented in appendices. The use of machine learning (SPECTER) for textual similarity analysis between patents and PhD dissertations is clearly explained and appropriately validated.
The authors also conduct multiple robustness checks and use alternative operationalizations of their key variables (e.g., alternative definitions of public invention using SPECTER-based similarity). Data integrity appears solid, with comprehensive coverage of firm-level R&D activity over a long time span (1986-2015) and linkage across several high-quality data sources, including Compustat, PatentsView, Dimensions, and ProQuest. I recommend that the authors enhance reproducibility by releasing anonymized intermediate datasets and code snippets for key transformations, and by archiving their data‐processing scripts (e.g., on GitHub). This would enable other scholars to leverage the same data pipeline for related research in innovation, human capital, and public knowledge.","This paper offers valuable insights for both policymakers and practitioners concerned with the real-world impacts of public investment in science. The finding of this research that public inventions may substitute for internal R&D—especially among follower firms—has practical implications for how universities license technologies and for how governments design innovation policies.
The focus on firm heterogeneity, such as differences between technology frontier and follower firms, adds depth and policy relevance. Quantitative estimates (e.g., a one standard deviation increase in relevant public invention reduces firm patents by 51%) provide actionable metrics for prioritization. By highlighting that only embodied knowledge (in people or patents) translates into corporate R&D investment, the study guides more effective knowledge transfer strategies.
In sum, this paper represents a rigorous, transparent, and policy-relevant contribution to the field of corporate R&D investment and public knowledge capital. It challenges the commonly used assumption that there are spillovers from public investment in knowledge, systematically exploring the hypothesis of whether there is complementarity or substitutability between public investment and in-house innovation activity. This approach provides a more refined understanding of how public investment in research 5 shapes private-sector innovation, which remains among the most important sources of economic growth.","I can see this paper get published in European Economic Review or in Economic Journal. For a more field journal, Journal of economic growth can also be suitable for this paper but I believe the benefit from a more general field journal will be higher.","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,"That finding raises a broader question: is private innovation output more important to aggregate economic growth than the underlying public knowledge? Based on historical breakthroughs tied to basic research, one could argue that the non-commercialized, foundational discoveries generated by public institutions have a greater long-run impact on growth than the downstream patents produced by firms. If so, the fact that public knowledge displaces some private patenting may not warrant corrective policy; it might simply reflect an optimal division of labor between public basic research and private development.",False,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,The Effect of Public Science on Corporate R&D,https://unjournal.pubpub.org/pub/evalsumpublicscience/release/1,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-05-02T12:46:29.436-04:00,
A Welfare Analysis of Policies Impacting Climate Change,https://www.rmetcalfe.net/_files/ugd/fe9abe_8d051a83540640a4a1b53e0d0308f31b.pdf,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Frank Venmans,FrankV,"Under which policy does a dollar of subsidy create the largest impact on welfare? Similarly, under with environmental tax does a dollar collected tax create the lowest impact on welfare? The paper answers these questions by developing the Marginal Value of Public Funds (MVPF). This allows to rank government policies according to their welfare impacts while respecting a given government budget.  
The authors convincingly claim that their measure of the MVPF allows for a better ranking of public policies compared to standard measures of cost per tonne, be it the Resource cost per tonne, Government cost per tonne or Social cost per tonne. This is because the MVPF includes the role of inframarginal transfers which are important part of policies, often of larger magnitude than the externalities that the policies correct for.
The strong point of the paper is to make the different components of the impacts of a policy additive and easy to distinguish. The paper builds on a very large body of literature, and builds on causal inference of the 18 most relevant economics journal (general interest journals and environmental economics journals).
The paper has also a novel way of estimating the social value of learning by doing, in a way that requires very litter parameters. This makes it very policy-relevant. Overal, the paper does an extremely good job in giving an overview of all the effects of environmental policies over the last decades.",,"Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",90,85,94,85,80,90,85,95,75,86,80,90,90,80,95,80,75,90,90,85,96,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4.9,5,4.7,5,4.3,5,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",I’m puzzled by the fact that the MVPF for a pigouvian tax is 1 although it is welfare increasing. I think that does not really make sense. But I havent really found the solution. ,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",10 years,50,True,9 hours,,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",,,,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,Evaluating the welfare effects of environmental policies is intrinsically hard and will always come with large uncertainty intervals. Yet the paper does a very good job at synthesizing and reinterpreting the best information we have. ,"The Marginal Value of Public Funds is not new, it exists in Public Economics, but the application to  enviromnental economics is innovative. ",,"replicability has improved quite a bit in the last decade, this paper is no exception.",,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",welfare analysis of policies impacting climate change review.docx,,True,yes. Although i have already spend more time than planned on the review. Yet it is also a way to learn. The nice thing is that this is a very high quality paper. ,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",I work on integrated assessments and climate policy and cost-benefit analysis more in general.,"A Welfare Analysis of Policies
Impacting Climate Change",https://unjournal.pubpub.org/pub/evalsumwelfareclimatechange/,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-04-29T18:08:30.553-04:00,9
Global potential for natural regeneration in deforested tropical regions,https://www.nature.com/articles/s41586-024-08106-4#MOESM2,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Cannon Cloud,Cannon,"Referee report comments on Williams et al. (2024) - Global potential for natural regeneration in deforested tropical regions
Overall Assessment:
This paper aims to model and predict the global potential for natural regeneration in deforested tropical regions for 2016 and 2030, respectively, using a machine learning random forest (RF) approach. The study’s aim and methods are novel and important, given the potential benefits of natural regeneration on carbon sequestration (amongst many other ecological and environmental outcomes) and the lack of pan-tropical analysis. The paper's current estimates are the first we have for understanding the scope of natural regeneration as an effective climate change mitigation measure.
However, my review identifies several significant conceptual and methodological concerns that fundamentally affect the validity and interpretability of the results and predictions, particularly concerning the paper's goal of informing restoration prioritization. These major concerns center on:
The selection and use of predictor variables, including issues of data leakage and circularity, resulting in limitations in the model validation approach and its ability to assess predictive power for prioritizing future actions.
Issues related to the input data derived from Fagan et al. (2022), including reliance on older source data, conflation of regrowth types, and high omission errors, which means prioritizing based on this map risks targeting areas already restored.
The approach to estimating uncertainty, which overlooks key sources of error and potentially uses suboptimal methods for addressing known sources of error and uncertainty.
The focus on extensive tree growth (i.e., gains in forest area) while neglecting intensive regrowth (i.e., canopy regeneration within existing forest pixels), meaning prioritization based solely on extensive gain ignores significant potential within existing forest areas.
While the overall approach has merit, offering a novel, global, and scalable methodology for estimating natural regeneration potential, it is currently under-developed but can be strengthened. Readily available data and techniques exist to address these issues and help create better estimates.
Major Concerns:
1. Data Leakage Concerns:
Definition: Data leakage, also known as data contamination, is a critical issue in machine learning that can lead to overly optimistic estimates of model performance and poor generalization to new data. It occurs when information from outside the training dataset (e.g., from the validation or test set) is inadvertently used during the training process. This can take many forms, such as including predictor variables that are derived from the outcome variable itself, using future information to predict past events, or accidentally including validation data in the training data. Leakage leads to a model that appears to perform well during training and validation but fails to generalize to unseen data, as it has essentially ""memorized"" patterns that won't exist in a real-world predictive setting (see, e.g., Kuhn & Johnson, 2019, Feature Engineering and Selection; Kaggle Wiki on Leakage). The implications of data leakage are particularly severe when the goal is prediction, as is the case in this study. In-sample performance metrics, such as those derived from cross-validation, will be unreliable indicators of how well the model will perform on truly independent out-of-sample data or in a future predictive scenario. Therefore, identifying and addressing potential sources of leakage is essential for ensuring the validity of any predictive model.
Specific Concerns with Predictor Variables: I have some concerns about the predictor variables that fall into three categories:
Variables potentially derived using outcome data: The SoilGrids predictor variable appears to be from 2014 or 2016, potentially created using land cover classification data from 2010 that includes forest classifications (related to the outcome variable, albeit from a different source than the primary Hansen GFC data). Extended Fig 3 shows this is the third most important variable.
Variables potentially incorporating post-2000 outcome data: Predictor variables averaged or derived from periods overlapping or extending beyond the 2000 start date of the regeneration outcome (e.g., NPP 2000-2015, burned area 2001–2017) may incorporate information influenced by the regeneration itself, creating temporal leakage. It is not explicitly stated they include data from recent years, but given they only update 3 covariates (forest density, distance, and land cover) for prediction, it seems like not.
Variables potentially limiting future prediction: Predictors like human settlement and roads from post-2000 (arguably have less reverse causality though probably not none), would still threaten future prediction with the model, as they reflect conditions during the observed regeneration period. Using these to predict future (e.g., 2030) potential assumes these socioeconomic patterns remain stable or change predictably, which might not hold.
Implication: Given the above issues with leakage, the reported model accuracy derived from validation likely does not reflect true predictive performance. Specifically:
Using predictors potentially derived from the outcome (Category 1) creates circularity, artificially inflating model performance metrics and compromising the model's ability to explain even the retrospective (2000-2016) regeneration patterns, as it may simply be learning the correlation rather than underlying drivers.
Using predictors whose values during the outcome period are influenced by the outcome itself (Category 2, e.g., NPP affected by regrowth) introduces temporal leakage and potential reverse causality within the model training. This inflates the apparent retrospective accuracy but obscures the true drivers of regeneration initiation and limits the model's explanatory power for the 2000-2016 period.
Using predictors reflecting conditions only during the outcome period (Category 3, e.g., post-2000 roads) fundamentally limits the reliability of the model for predicting future (2030) scenarios, as these conditions are likely to change.
Collectively, these issues mean that any references to accuracy derived from the current validation should not be extrapolated to future predictions (e.g., 2030) or necessarily be interpreted as reflecting true explanatory power even for the past (e.g. 2016), requiring these concerns to be addressed. While the mechanisms differ (circularity vs. temporal feedback), both Category 1 and Category 2 leakage significantly compromise the model's explanatory power and predictive reliability.
2. Input Data Concerns (GFC Gain & Fagan et al. Application):
(a) Reliance on Older GFC Gain Data: The study relies on an older version of the Global Forest Change (GFC) gain data (v1.5, covering 2000-2012/2016). Since the initial analysis for Fagan et al. (2022), improved GFC datasets have been released (e.g., GMD 2022 update using GEDI lidar, offering better canopy prediction; subsequent versions with multiple gain epochs). Link: https://www.globalforestwatch.org/blog/data-and-tools/new-gfw-tree-cover-gain-net-change-data/ There is even a newer version showing gains for 2005, 2010, 2015, and 2020.
Recommendation: Re-running the analysis using the most current and improved GFC gain data (potentially over multiple distinct time periods like 2000-2005, 2005-2010, 2010-2015, etc.) is recommended. This would provide more accurate input data and allow for testing model sensitivity to predictor variables potentially causing leakage (e.g., excluding soil data for earlier periods where contemporaneous versions aren't available).
(b) Definition of Natural Regrowth (Inherited from Fagan et al.): The model relies on Fagan et al.'s definition, which treats all non-plantation gain as ""natural regrowth,"" potentially conflating truly natural processes with human-assisted regeneration or diverse tree plantings not classified as monoculture plantations. Fagan et al. acknowledged this limitation: “Our product was also not designed to distinguish rarer land uses intermediate between tree plantations and natural regrowth, such as assisted natural regeneration.” Given the scale of restoration commitments involving active planting (e.g., Initiative 20x20 claims 53 Mha planted in Latin America Link: https://initiative20x20.org/publications/initiative-20x20-infographic), this conflation significantly affects the interpretation. The authors must either attempt to disaggregate these categories or significantly temper their interpretation and claims regarding purely natural regrowth potential.
(c) Overestimation due to Omission Error (Inherited from Fagan et al.): The analysis excludes areas mapped as regrowth by Fagan et al. to define the domain for potential regeneration. However, Fagan et al. reported very high omission errors for regrowth (low area-based Producer's Accuracy of ~18.7%). This means the Fagan map used for exclusion missed a large area (~26 Mha) of actual regrowth (estimated at 31.6 Mha). Consequently, the Williams et al. potential area (215 Mha) includes these already-regrown areas, substantially overestimating the area truly available for future regeneration. This requires correction. While a precise spatial correction is challenging, this numerical adjustment highlights the scale of the overestimation, and the uncertainty associated with this correction must be addressed (see Point 3a). Prioritizing based on this map risks targeting areas already restored.
3. Estimating Uncertainty:
(a) Underestimation of Uncertainty from Omission Error: The necessary correction for the omission error (subtracting the ~26 Mha of omitted-but-actual regrowth, see Point 2c) carries significant uncertainty derived from the Fagan et al. accuracy assessment (31.6 ± 11.9 Mha for total regrowth). Propagating this uncertainty would result in a much wider confidence interval for the corrected potential regeneration estimate (~189 Mha) than implied by the analysis of the 215 Mha figure. The current approach (reporting just minimal uncertainty from the RF model) ignores this driving source of uncertainty.
(b) Stratification for Uncertainty Reduction: The paper does not appear to leverage stratification techniques known to mitigate the impact of omission errors on area estimates, particularly for rare classes like forest gain (as demonstrated in Olofsson et al. 2020 for mitigating omission error impacts). While Fagan et al. suggested this was impossible for regrowth, Olofsson (2020) does not support that conclusion (from my reading). Williams et al.'s own finding that most potential regrowth occurs near forest edges (within 300m: “For example, out of a random sample of 62,493 grid cells (of all 30 × 30 m grid cells) across the study region, 98.1% of cells with a potential of >0.5 (for illustrative purposes) occur within 300 m of a forest edge.”) suggests a buffer-based stratification strategy could be feasible and potentially significantly reduce the uncertainty associated with omission errors by isolating them within smaller-area strata. Implementing such a stratification is recommended to improve the reliability of the underlying area estimates which compromise the bulk of the uncertainty (as compared to the minimal uncertainty from the RF model).
(c) Confidence Interval Construction for Rare Classes: Additionally, while perhaps a secondary concern compared to the points above, the standard practice of constructing confidence intervals using the design-based SE and the Wald method (assuming normality) may be unreliable for rare land-use classes or metrics with low accuracy (like the Producer's Accuracy for regrowth). The underlying sampling distributions for these estimates are often skewed. Using bootstrap percentile confidence intervals, which empirically derive the interval from the data's distribution without assuming normality, can provide more realistic uncertainty bounds and likely achieve better coverage probability (i.e., capture the true value closer to the nominal 95% level). This is particularly important when estimates serve as baselines for monitoring, where reliable CIs are needed to detect future changes (example code demonstrating this comparison can be provided upon request).
4. Neglect of Intensive Margin Regrowth:
The study focuses exclusively on the extensive margin (new forest area gain). It completely overlooks the intensive margin – canopy regeneration and forest recovery within existing, potentially degraded, forest pixels (e.g., areas increasing from <30% to >60% canopy cover as an arbitrary threshold). Data exists to analyze this (e.g., Hansen GFC tree cover percentage layers for 2000/2010 Link: https://glad.umd.edu/dataset/global-2010-tree-cover-30-m; Sexton et al. canopy data for multiple epochs Link: https://e4ftl01.cr.usgs.gov/MEASURES/GFCC30TC.003/). Looking at extensive gains alone does not allow for a complete estimate of natural regrowth potential. Perhaps this is outside the scope of this paper, but at a minimum the paper needs to reframe the results with this extensive-intensive caveat. For example, the abstract statement “We estimate that an area of 215 million hectares—an area greater than the entire country of Mexico—has potential for natural forest regeneration, representing an above-ground carbon sequestration potential of 23.4 Gt C (range, 21.1–25.7 Gt) over 30 years…” makes it sound like this is all potential regrowth, not just extensive gain potential, and prioritization based solely on extensive gain ignores significant potential within existing forest areas.
Synthesis of Potential Impact on Headline Estimate:
The combined effect of the major concerns outlined above, particularly the omission error and definition conflation inherited from the input data (Points 2b, 2c, 3a), suggests the headline estimate of 215 Mha for potential natural regeneration requires significant downward adjustment and carries much greater uncertainty than reported. A rough back-of-the-envelope calculation illustrates the potential magnitude: subtracting the omitted area (~26 Mha) from the initial potential (215 Mha) yields ~189 Mha. Further adjusting this based on a hypothetical (but plausible, given planting initiatives) assumption that only 50-90% of Fagan's regrowth category was purely natural gives a central estimate around ~132 Mha (~95-170 Mha range). Directly propagating Fagan's statistical uncertainty (~±12%) onto this adjusted estimate suggests a plausible range of perhaps ~83 to ~182 Mha. Furthermore, this calculation does not yet incorporate additional uncertainty stemming from potential model over-performance due to data leakage (Point 1). Assuming the RF model performs 10% points less well once the leakage issue is revised, the possible range could be ~70 to ~195 Mha. While illustrative, this drastically reduced estimate and expanded range highlights the need for the authors to perform a rigorous correction and uncertainty propagation for a reliable estimate of purely natural regeneration potential.
Minor Points:
Sensitivity Analysis: To assess the robustness of the potential regeneration predictions, it would be valuable to conduct a sensitivity analysis by comparing the results obtained from the Random Forest model with those from alternative high-performing algorithms. Could the authors provide a brief comparison of predictions using either Gradient Boosting Machines or Neural Networks for subsets of the study area? This comparison would help evaluate the stability of the predictions and the extent to which they are dependent on the specific modeling approach employed.
Socioeconomic Context: The authors provide a clear rationale for basing their final spatial predictions of natural regeneration potential solely on biophysical variables, citing similar predictive accuracy compared to models including socioeconomic factors, alongside the greater temporal stability and spatial resolution of the biophysical data. This approach is understandable for mapping the biophysical potential for regeneration. However, translating this potential map directly into prioritization for restoration interventions requires careful consideration of socioeconomic context, including opportunity costs and factors influencing the persistence of regenerated areas, which the authors acknowledge in the discussion. For future refinements or applications aiming more directly at prioritization, it could be valuable to explore the integration of higher-resolution, spatially explicit socioeconomic proxy data that might overcome the limitations of the coarser datasets initially considered. For example, datasets such as nighttime lights, or potentially gridded indicators of relative wealth or sub-regional economic activity (where available at relatively fine scales, e.g. 375-750m), could offer additional spatial nuance regarding land-use pressure and the feasibility of regeneration persistence, complementing the biophysical potential mapped here.
Figure Reference: The text references Extended Data Fig. 1 for Producer's Accuracy values (""...dropped to 18.7 (±5.4) when based on estimated area (Extended Data Fig. 1).""). However, Extended Data Fig. 1 appears to show model accuracy versus the number of variables. Please verify the correct figure reference or provide the relevant accuracy data elsewhere.",,"Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",50,30,80,40,20,60,30,50,10,50,40,90,65,50,80,30,15,45,60,50,70,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4.5,5,3.5,5,5,5,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",None,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",6 years in a PhD program,0,True,20 hours,"The questions were annoying after I already spent a lot of time on the referee report. I was really ready to hit submit, but had to reason through a bunch of questions, where some were already answered in the referee report.",Yes,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.","""We estimate that an area of 215 million hectares... has potential for natural forest regeneration..."" (Williams et al. 2024, Abstract & Results section). They further estimate this area holds an ""above-ground carbon sequestration potential of 23.4 Gt C (range, 21.1–25.7 Gt) over 30 years.""

This claim is highly important because it provides the first spatially explicit, high-resolution estimate of the global biophysical potential for natural regeneration across the tropics. It quantifies a major, often overlooked, cost-effective pathway for achieving large-scale forest restoration, which is critical for meeting international climate change mitigation and biodiversity conservation targets (like the Bonn Challenge and the Global Biodiversity Framework). By mapping this potential, the research aims to guide policy, investment, and planning towards leveraging natural processes for restoration, complementing or potentially replacing more expensive active tree-planting efforts in suitable areas. It provides a crucial (though, as discussed in the review, potentially overestimated and uncertain) baseline figure for understanding the scale of this opportunity.",,"Based on the significant methodological concerns outlined in the referee report, particularly the unaddressed omission error in the input data and the potential conflation of regrowth types, my belief in the accuracy of the headline claim – 215 Mha of potential natural regeneration – is low. A very rough back-of-the-envelope calculation attempting to correct for these factors (detailed in the report) suggests a more plausible credible interval for the purely natural regeneration potential might be substantially lower, perhaps in the range of ~80 Mha to ~180 Mha, and carry more uncertainty. This reduced range reflects the expectation that correction for input data errors and appropriate uncertainty propagation would revise the authors' point estimate downwards.","The first minor point in the referee reports suggests using multiple kinds of machine learning algorithms, such as Gradient Boosting Machines or Neural Networks, for at least some regions, to compare sensitivity.","For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),"This rating reflects the paper's high potential and novelty tempered by severe methodological flaws that likely invalidate its main quantitative estimate (potential area) and compromise its predictive reliability. The midpoint below 50% reflects the current lack of credibility in the headline results. The wide CI reflects that the paper could be rated very poorly if the flaws are deemed insurmountable or very highly if major revisions successfully address the core issues, given the importance of the topic.","Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.","The paper clearly states its important questions and claims regarding natural regeneration potential. While the machine learning approach applied is novel and potentially powerful, the evidence provided for the main quantitative estimate (215 Mha) is significantly undermined by major methodological concerns, particularly the failure to correct for substantial omission errors in the input data and potential data leakage issues. Furthermore, the authors do not fully characterize the impact of these limitations on their results. This combination of clear goals but flawed evidence and characterization justifies a below-average midpoint rating (40%), while the wide credible interval (20-60%) reflects the tension between the paper's novelty and importance versus the current lack of credibility in its core findings, which could potentially be addressed through major revisions.","This rating reflects a significant disconnect between the clarity of the methods' explanation and their appropriateness and robustness in practice. While the Random Forest approach is standard and described adequately, its application suffers from serious issues, including potential data leakage and, critically, the use of input data with known, large omission errors that are not corrected for in the final estimate. This leads to results that are likely biased, not robust to underlying assumptions, and presented without adequate characterization of the substantial uncertainty inherited from input data errors. These methodological shortcomings significantly undermine the credibility of the paper's main quantitative findings, justifying a low midpoint rating, although the wide credible interval acknowledges the potential for improvement if these fundamental issues were rigorously addressed.","This rating reflects the paper's high relevance to critical global priorities like climate change and biodiversity loss, and its potential to inform impactful interventions through cost-effective natural regeneration. The novel global approach represents a potentially significant contribution. However, the current contribution to practice is severely limited by major methodological concerns outlined in this review, particularly those affecting the reliability of the headline quantitative estimates and the map's utility for prioritization. These flaws substantially reduce the paper's current impact compared to its potential, justifying a midpoint rating. The wide credible interval acknowledges that successful major revisions addressing the core methodological issues could elevate the paper's contribution considerably.","This rating primarily assesses the clarity and internal consistency of the paper's presentation. The authors clearly define their core concepts and transparently explain their analytical reasoning and workflow. The conclusions drawn, such as the 215 Mha potential area, are consistent with the direct outputs of the specific modeling approach presented, and the supporting data, figures, and tables are relevant to the argument. However, the paper could more accurately characterize the nature of its evidence alongside its main claims by giving greater prominence to the known limitations of the input data (particularly omission errors) and their potential impact on the results' interpretation, rather than primarily discussing these in the methods or supplementary information. Despite this weakness in fully contextualizing the evidence, the overall clarity and transparency warrant an above-average rating for this specific criterion.","The authors clearly explain their methodology and cite the sources for their input predictor variables. Furthermore, they make their final output maps (potential for regeneration) publicly available via Zenodo. However, direct replication of the analysis is severely hampered by the lack of publicly available code (available only upon request), and the absence of the derived training and validation dataset (the millions of points labeled as regeneration/non-regeneration). While the methods description is detailed, the inability for another researcher to easily access the exact code and training data to verify or build upon the modeling workflow significantly limits reproducibility and falls short. Therefore, despite the value of the output data, the overall support for replication and future analysis is rated below average.","The authors clearly define concepts as used in their study and transparently describe their analytical workflow. The main conclusions drawn, including the headline potential area estimate, are logically consistent with the evidence generated by their specific modeling approach, and the supporting data, figures, and tables are relevant and well-utilized (though sometimes missing).","I think this should be published in Nature if they can work through my main critiques. But if there is still some confusion about natural regrowth vs tree-planting, for example, or difficulty modeling uncertainty due to omission error, I would suggest this be moved down slightly in journal quality.","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
","Unjournal review 1.odt,ConfusionMatrixBootstrapSE.R,Unjournal review Williams 2024.odt","The most important implication of the claim, as presented, is that there exists a vast, globally significant area where cost-effective natural regeneration could be prioritized to meet climate and biodiversity goals, potentially redirecting substantial funding and policy focus towards facilitating this process over more expensive active planting in these regions. While the general principle that natural regeneration is a crucial and cost-effective tool is undoubtedly true and highly relevant for policy, my belief in the specific implication derived from the 215 Mha figure and the associated map is medium (i.e., ballpark correct but likely somewhat lower), due to the methodological concerns outlined previously (especially the likely overestimation from uncorrected omission errors and potential conflation). Therefore, while policy should absolutely recognize and create mechanisms to support natural regeneration, it should view the current 215 Mha estimate or the specific map with a bit of skepticism for any large-scale funding allocation or target setting.",True,"Maybe, I would need to see a reason why. I have already spent a decent amount of time on this.",,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",Development economics,Global potential for natural regeneration in deforested tropical regions,https://unjournal.pubpub.org/pub/evalsumnaturalregeneration/,Nature,"50_published evaluations (on PubPub, by Unjournal)",,,2025-04-23T09:52:56.558-04:00,20
"Towards best practices in AGI safety
and governance: A survey of expert opinion",https://arxiv.org/pdf/2305.07153,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",No,GC,"This insightful and policy-relevant paper brings empirical grounding to AGI safety measures by assessing expert consensus. It finds broad support for nearly all proposed safety practices across AGI labs, governments, and civil society. Key limitations include potential sampling bias - particularly overrepresentation of safety-minded respondents - and the challenge of interpreting abstract agreement as indicative of real-world implementation. Still, the study offers valuable input for policymakers and contributes meaningfully to discussions on AGI safety. ",,"Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",70,60,80,70,65,75,70,75,65,80,75,85,85,80,90,65,50,80,90,80,100,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4,3.5,3.5,4.5,3.1,4,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",No confidential comments. ,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",3 years,10,True,4 hours,"I would rate this very posively! The process supports a very thorough evaluation of the paper, and I enjoyed being able to provide point estimates for specific characteristics of the paper along with a more text-based peer review ",Yes,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.","The most important and impactful claim that this research makes is that a majority of AGI governance experts exhibit consensus in supporting 49 out of 50 specific safety-related policies and practices. This is summarized in this statement:
‘For every practice but one, the majority of respondents somewhat or strongly agreed that it should be implemented. Furthermore, for the average practice on our list, 85.2%
somewhat or strongly agreed it should be implemented.’
",,"While, as stated above, the paper mainly captures high-level, in principle agreement with specific safety practices and principles without actually evaluating trade-offs between these practices and rapid AGI development, I fully believe the claims presented (over 90% probability of the claim being true). ",,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",Paper Review - GC.docx,,True,"Yes, for sure. ",,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",AI Safety,Towards best practices in AGI safety and governance,https://unjournal.pubpub.org/pub/evalsumagisafety/,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-04-16T05:43:03.184-04:00,4
Global potential for natural regeneration in deforested tropical regions,https://doi.org/10.1038/s41586-024-08106-4,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",,capybara,"Strengths
- Proposes an empirically novel approach to answer highly policy-relevant questions
- Provides a sound overall methodology which is tractable and replicable
Critiques
- Trained model does not enable prediction into the future given currently observed conditions
- Predictions do not represent pure biophysical potential despite being framed as such
- Regrowth longevity is not addressed
- Policy targeting implications qualified by additionality and opportunity costs
",,"Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",50,40,60,20,15,25,20,25,15,60,50,70,50,41,60,95,90,100,90,80,100,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",3.5,5,3,4,5,5,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",-,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",5 years,2,True,2 working days,"This template is functiong well, although with long web questionnaires one always dreads the loss of responses due to some accident or connection failure.",Yes,-,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.","The authors estimate that biophysical conditions can support natural regeneration in tropical forests over 215 million hectares globally, and five countries (Brazil, Indonesia, China, Mexico and Colombia) account for 52% of this estimated potential (source: abstract and last paragraph in p. 132). The estimation is based on spatially explicit machine learning predictions of natural regeneration potential. The claim is important due to ambitious forest restoration targets requiring science-based guidance on spatial targeting.",,"I have low confidence in the estimated 215 Mha being an accurate figure on biophysical potential. It is likely a lower bound to the true value, due to anthropogenic factors which moderate the results. Other comparable estimates do not exist. However, the true value is certainly well below the maximum area of opportunity estimated at 678 Mha in a previous study. The spatial distribution of reported results is biased towards higher potential in regions with low anthropogenic disturbance.","The proposed method cannot uncover the pure biophysical potential which is how the authors interpret the result. The results do, however, represent locations where natural regeneration is likely to happen under a business-as-usual scenario. For this figure latter quantity to be accurately estimated, the most important modification to the method would be to formulate the prediction as a prediction of future outcomes conditional on currently observed covariates.","For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",The research questions are clearly identified and novel. The execution has a number of shortcomings which render the reliability of the results highly questionable. Results are incorrectly framed as pure biophysical potential.,"The overall methodology is well motivated and appropriate. However, shortcomings in implementation render the reliability of the results highly questionable.","Disregarding methodological critique, the study asks relevant research questions and provides a conceptually novel approach to answer them. As such, future research should build on the study and address the methodological shortcomings.",The method and its outputs are clearly defined. The conclusions and discussion are much more opaque and lack concrete policy prescriptions.,"All data sources are referenced, publicly available, and the method is described in detail.","Disregarding methodological critique and the resulting caveats, the topic and results are highly relevant for policymakers and practicioners.","The study is already published in a top journal. Impact potential and novelty clearly warrant this ranking, but methodological shortcomings and thin implications from the results do not carry that far. Methodologically better and more informative work is published in lower tier journals.","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",evaluation.zip,"The results imply information about the spatial distribution of low-cost forest restoration opportunities. The predictions likely correctly identify several regions with high potential (no false positives) but anthropogenic factors mask others (false negatives likely exist). An important caveat to interpreting the results is that the identified regions with high regeneration potential may also provide the least additionality from interventions. As such, these results alone should not guide policy choices.",True,"Yes, but only if this can be arranged anonymously.",,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",Land use and conservation,Global potential for natural regeneration in deforested tropical regions,https://unjournal.pubpub.org/pub/evalsumnaturalregeneration/,Nature,"50_published evaluations (on PubPub, by Unjournal)",,,2025-04-14T13:46:42.121-04:00,16
A Welfare Analysis of Policies Impacting Climate Change,https://www.rmetcalfe.net/_files/ugd/fe9abe_8d051a83540640a4a1b53e0d0308f31b.pdf,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Johannes Emmerling,Abulafia,"This is an excellent paper that makes significant contributions to climate policy evaluation. The authors' application of the MVPF framework to analyze 96 US environmental policies provides valuable insights that traditional cost-per-ton metrics often miss. The  key contribution is to develop and meticulously apply the MVPF framework to a wide range of policies, that take into account all impacts on individuals and the public. This approach gives a very nice sufficient statistic in the overall efficiency of a wide range of policies, and allows easily to compare and combine different policies. Also while the approach is marginal, it seems fairly well to allow also for non-marginal policies (albeit this point would be good to illustrate and demonstrate more clearly). Besides specific points, one thing I also wonder is whether the “efficiency” from a social point of view of policies could abe compares also across revenue generating and costing policies. Like a MVPF of 1.5 for a subsidy vs. 0.5 for a tax, would they be considered similar? It seems like a hyperbola at 1, but this of course depends on the MV of public funds in general e.g. through distortive taxes. But maybe using an average MVPF of revenues one would normalize all policies so they have the same “scale” in that overall efficiency can be measures on the same side of unity. But this is more of a suggestion or comment.
Finally, while the framework starts out generally across individuals (and generations) indexed by i. One issue that I find missing is the distributional implications or heterogeneity effects. While it is at some points alluded to (and explicitly when taking the mean of marginal income or consumption), it would be good to indicate assumptions (and where relevant, whether this has been considered in the underlying studies) about distributional incidence. For instance, for EV subsidies it depends largely if uptake is mostly by rich household (potentially compared to a representative population, or poorer households. I don’t mean to add this in the analytics but discuss assumptions and the issue where relevant.",,"Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",91,78,94,96,93,100,89,95,79,93,90,95,80,74,86,65,60,70,93,90,96,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4,4,3.9,4.3,3.9,4.1,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",13 years since PhD,200+,True,12 hours,"The template is very helpful, I hope it is saved rightly as there is no way to save it, so was worried as I filled it during several days.",Yes.,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.","The clear comparison based on very many causal studies with clear-cut values is a very strong result, and a clear ranking of climate-oriented policies with numbers up to 5 domestically is a very strong result. Also, the positive impact of revenue-generating policies with values around 0.7. 
Also, the measure is very salient and useful for communications and policy documents.",,"There is some uncertainty, but the important ones are discussed and shown, so overall of course some uncertainty but would say rather quite robust.",The robustness and importance of key variable notably SCC and VSL are important to highlight and their implications.,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,,"This is clearly a top field, with some potential for a Top Econ one, if the method would be a tad clearer.","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",Review report on MVPF Climate Policy for Unjournal.pdf,The findings in my view are immediately very relevant for policy makers when deciding to implement or abandon given policies. They can give very simple understandable and sound justification and estimates on the overall socioeconomic impact of these policies.,True,"Never done, but could be.",,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",Environmental Economics,"A Welfare Analysis of Policies
Impacting Climate Change",https://unjournal.pubpub.org/pub/evalsumwelfareclimatechange/,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-04-11T11:47:14.318-04:00,12
"Yellow Vests, Pessimistic Beliefs, and Carbon Tax Aversion",https://www.aeaweb.org/articles?id=10.1257/pol.20200092,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",yes,treich,See my report,I sent my review to David through email,"Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",85,,,90,,,93,,,,,,,,,,,,,,,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",5,,,,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",yes,,False,"About 3 hours, but I had presented this paper in a reading group before - overall it took me about two days of work, reading the paper, working on the presentation and then writing my evaluation",,,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.","This paper shows that many individuals do not support carbon policy, even when tax revenues are redistributed, and suggests that this lack of support stems from pessimistic beliefs about the policy’s impact.",,I strongly believe that claim,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,True,,"I know both authors (they are French). I have never co-authored a paper with the authors, but I have an ongoing project with one of them.","Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,,,,,,,2025-04-10T10:56:33.320-04:00,16
Choose Your Moments: NIH Peer Review and Scientific Risk Taking,https://jeffreyshrader.com/papers/choose_your_moments.pdf,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",James M. Zumel Dumlao,K0helet,"This paper includes two experiments aimed at informing reform in the NIH grant peer review process. The first experiment finds that biomedical scientists have a preference for studies with higher score variance (“dissensus”), interpreted as a desire for risk-taking. The second experiment finds that tightening budgets reduces scientists’ dissensus tolerance. Authors conclude that relaxed budgets and rewarding dissensus would encourage the production of more high-impact science.",https://docs.google.com/document/d/1moGbsXnWjg2lxboL_nK4rZWFY8zBHxfX6JpQ8oIiokY/edit?usp=sharing,"Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",70,50,80,56,45,61,76,85,68,72,66,84,55,45,70,65,50,79,50,40,60,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",3.8,3.9,3,4.1,3.5,4.3,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",3 years,3,True,7 hours,9/10,Yes!,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.","Authors claim that biomedical scientists, who are the experts relied on in NIH peer review, have a preference for “dissensus” among reviewers’ scores (i.e., higher variance). They also claim that a tighter budget will decrease the dissensus tolerance among scientists. Both claims are well substantiated in the experimental findings shown in Tables 2 and 5. It should be noted that the effect of the overall average score is 2-9 times larger than that of score variance on participants’ decision of which studies to fund.
They also claim decision makers at science funding institutions would benefit from taking the variance of peer reviewer scores into account, rather than considering the average alone. It is argued that higher score variance is correlated with higher risk research with greater potential to have positive impacts for the public. This claim is crucial to connecting the findings of the experiments to policy recommendations, however it lacks direct evidence from the studies conducted, “Our study evaluates the value researchers place on alternative peer review aggregation methods taking this conjecture [that greater dissensus in project scores can identify more radical projects] as given.” (p. 3). Results in Table A8 show that the risk-averse subgroup has a stronger preference for score variance than average, undermining the assumption that the variance preference is a risk preference.
",,"I believe the first claim is 95% true, that scientists prefer higher dissensus work controlling for quality. The second claim is much less tenable to me, maybe 65% true, that higher variance scores means riskier research that would translate into greater economic growth or something else important to the public interest.",,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",Some overclaiming in connecting findings to policy recommendations. ,Experimental methods and robustness checks make for high internal validity.,"Studies advance knowledge well, but findings lack direct connection to policy.","Major logical leap from score variance to risk tolerance to high-impact science not well-motivated. Otherwise, writing style is very accessible and clear.","Procedures clearly states, experiment conditions/instruments provided, no data shared","Variance could be useful for breaking ties, but not clear that it is the information to incorporate into funding decisions over other forms of information. More information is likely always beneficial, some comparison to other possible policy interventions would make relevance more apparent.",,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,True,"Yes, since I’m very early into my career, I’m not extremely confident about my expertise yet (although I feel qualified to write this review). Discussion with other evaluators would help me adjust my standards.",,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Choose Your Moments: [NIH] Peer Review and Scientific Risk Taking,,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-04-06T18:19:38.116-04:00,7
The Effect of Public Science on Corporate R&D,https://www.nber.org/papers/w31899,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",,Josh,"The paper provides evidence on a recently relevant issue in science policy: the spillover effects of public science. However, these effects are not well-identified. There are numerous issues likely distorting the identification, including bad controls, unreliable 2SLS results, and a shift-share design with endogenous shares.","This paper tries to identify the impact of three forms of public science on corporate R&D: [1] public invention (i.e., university patents), [2] human capital (i.e., PhD dissertations), and [3] public knowledge (i.e., non-corporate publications). The headline results are that [1] crowds out corporate innovation, [2] boosts corporate innovation, and [3] has no meaningful effect. The identification strategy relies on a shift-share design, where the shifts are based on U.S. congressional subcommittee spending on public science and the shares are based on firm exposure to specific subcommittee’s shocks (based on firms’ exposure to expenditures from specific government agencies and the subcommittees responsible for those agencies’ funding). To address potential exclusion restriction violations, the paper does not use raw changes in subcommittee appropriations as its primary shock variable, but instead uses the changes in appropriations predicted by those subcommittees’ party shares. This report identifies three key issues in the identification strategy.

First, the IV estimates for public invention appear to take an incorrect sign. Intuitively, IV estimates are ratio estimates, specifically the respective ratios of the reduced-form estimates to the first-stage estimates. We can see from Tables A9 and A10 that predicted R&D budgets positively predict public invention, and the OLS results from Tables 5-8 show that public invention has a positive reduced-form relationship with corporate R&D. However, the 2SLS coefficients on public invention in Tables 5-8 are negative. This should not be possible. 

Second, the paper’s shift-share design likely yields biased results due to the endogeneity of the shares. The primary IV models shown in the paper instrument firms’ exposure to public science using a series of shift-share formulas that, generally speaking, instrument a firm’s exposure to public innovation with a sum of the products of [1] the share of a firms’ exposure to a specific class of innovation (e.g., publication subfields, patent subclassifications, etc) and [2] the predicted R&D expenditures from all federal agencies for that innovation class. [1] is likely endogenous; the paper indeed finds a positive correlation between firm R&D stock and agency R&D funding. Of course, this is only one source of endogeneity, and many more may exist. This is a general gripe with shift-share designs; much is discussed about how exogenous the shocks are but little attention is paid to whether the shares are endogenous, which they often are.

The paper addresses this second issue by controlling for lagged firm-level R&D stock, but this exposes the paper to a third identification issue: the main specifications all control for colliders. An often-overlooked issue in control specifications is whether the controls are themselves outcomes of the main exposures of interest (in this case, public science). Cinelli, Forney, & Pearl (2024) show that if a covariate is (1) influenced by an exposure of interest and (2) shares a common unobserved confounder with the outcome, then controlling for the covariate will bias the estimated relationship between the outcome and the exposure of interest. To illustrate, consider the ‘birth weight paradox’ (Hernández-Díaz, Schisterman, & Hernán 2006). Though maternal smoking is associated with higher infant mortality and more incidence of low birth weight, when controlling for birth weight, maternal smoking appears to decrease infant mortality. This is not because maternal smoking actually decreases infant mortality, but rather because newborns who are low birth weight because of maternal smoking likely exhibit lower mortality rates than  newborns who are low birth weight because of other health complications. Applying this concept to the paper’s specifications, firm exposure to public science may not decrease firm innovation; rather, it may be the case that firms with more R&D stock due to public science innovate less than firms who have more R&D stock due to the inherent innovative nature of their activities.

The second and third issues together place the entire identification strategy in a bind. Firms with innovation profiles similar to those invested into by public agencies likely have different levels of R&D investment for reasons unrelated to public science, but controlling for these levels of R&D investment yields bad control problems because private R&D investment is inherently influenced by public science. The clear endogeneity issues with the shift-share design do not seem like they can be controlled away with observable covariates.

With public science funding under threat both in the United States and globally, the paper’s findings could mislead policymakers on the importance of public science for the private sector given the credibility issues with the empirical design. The paper’s findings effectively imply that the research function of universities is, if anything, negative for the private sector. Given that the only positive relationship between university research activity and firm outcomes is found in the graduation of new PhDs, the policy implications of this paper taken at face value would imply that universities should prioritize training PhDs while simultaneously cutting back on university invention. Given that universities conduct the bulk of all basic scientific research, such a reorientation of university priorities should not arise without more credible evidence.

References
Cinelli, C., Forney, P., & Pearl, J. (2024). “A crash course in good and bad controls.” Sociological Methods & Research 53(3), 1071-1104. https://doi.org/10.1177/00491241221099552.

Hernández-Díaz, S., Schisterman, E. F., & Hernán, M. A. (2006). “The birth weight ‘paradox’ uncovered?” American Journal of Epidemiology 164(11), 1115-1120.https://doi.org/10.1093/aje/kwj275.","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",46,32,60,34,26,42,22,26,18,46,42,50,67,56,78,65,62,68,67,62,72,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",2.3,3.4,2,3,3,4,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",3 years,8,True,4 hours,7/10,Yes,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",Increased exposure to public invention decreases a firm’s private R&D expenditure.,,I believe neither the sign nor magnitude of this estimated relationship. My prior for this relationship is not swayed by the paper.,,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,True,Yes,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,The Effect of Public Science on Corporate R&D,https://unjournal.pubpub.org/pub/evalsumpublicscience/release/1,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-03-30T06:58:02.525-04:00,4
Pharmaceutical Pricing and R&d as a Global Public Good,w31272.pdf,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",,XXX,"This paper provides a comprehensive analysis of pharmaceutical research and development (R&D) as a global public good, highlighting the contributions made by various countries. It successfully gathers valuable data and employs innovative methods to calculate marginal costs. However, the empirical approach could be improved by addressing omitted variable bias and incorporating general equilibrium effects. Furthermore, the conceptual framework could be broadened to include dynamic factors and supply-side considerations.",,"Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",51,35,65,20,5,40,55,75,35,85,75,95,70,55,85,100,100,100,95,89,100,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",2,2,1,3,1,3,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

","The paper examines a highly interesting topic with considerable potential. To better align it with the standards of a top economic journal, I recommend incorporating some economic and econometric improvements. These adjustments would enhance the paper's rigor and make it more compelling for the intended audience.","Responses to these will be public unless you mention in your response that you want us to keep them private.

",6 years,2,True,3 days,Very useful,Yes,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.","For these reasons, US officials could raise these issues at international negotiations and advocate for higher prices than presently set in high-income ROW 35 countries. A multi-country agreement in this direction would represent a serious effort to support improved world health.",,"The paper’s recommendation to push for higher prices in the rest of the world is not fully supported from an economic or econometric standpoint. The empirical analysis is based on a simple linear estimation that fails to consider potential endogeneity or general equilibrium effects. Since prices result from market equilibrium, a general equilibrium approach is necessary to conduct a counterfactual analysis on how changing contributions across countries would impact prices. Without modeling the complete market structure, the framework cannot accurately predict how price increases in the rest of the world would influence global innovation, access, or welfare. ",See attached report.,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",Unjournal-Pharmaceutical_Prices_RD.tex,,True,Sure,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Pharmaceutical Pricing and R&D as a Global Public Good,https://unjournal.pubpub.org/pub/evalsumpharmpricing/release/3,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-03-27T18:07:51.673-04:00,24
Urban Forests: Environmental Health Values and Risks,https://static1.squarespace.com/static/56034c20e4b047f1e0c1bfca/t/660336a26b15135b9df29449/1711486630687/XHXXZ_urbanforest_2024-3-26.pdf,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",,Anonymous Papaya,"This is an excellent, thorough, and well-written paper on an important topic of urban greenery and health. The methodology builds on well-established methods and is convincing with ample placebo and robustness checks. The authors could further improve policy impact by expanding on the external validity of these findings by describing, exploring, and empirically characterizing the setting and MMP further: what contributes to its success, what greenery is used, and how is the greenery planted?",,"Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",90,86,94,92,85,95,92,96,88,88,82,94,92,87,95,88,84,92,88,80,92,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4.5,4.4,4.3,4.7,4.2,4.6,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",6 years,8 ,True,10 hours,The template and instructions were clear and helpful! I found it interesting to think about these metrics. ,Yes,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.","Urban tree planting provides net health benefits by reducing air pollution (4.2%) despite meaningful increases in pollen exposure (7.4%). Overall, the health value of Beijing’s Million Mu Project (55 billion CNY over the past decade) will likely exceed its cost (75 billion CNY) in the next decade. This is important due to the popularity of urban greening in the face of climate change and urbanization. The empirical evidence on pollen exposure and its comparison to air pollution is especially novel.",,"I am quite convinced by the claim as I found the research framework and empirical strategy convincing, with ample robustness and placebo checks (outside vs. inside MMP areas, upwind vs. downwind, etc.).",,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",evaluation_Unjournal-XHXXZ_urbanforest.docx,,True,Yes,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Urban Forests: Environmental Health Values and Risks,,Review of Economics and Statistics,"50_published evaluations (on PubPub, by Unjournal)",,,2025-03-21T13:08:28.725-04:00,10
Stagnation and Scientific Incentives,http://www.nber.org/papers/w26752,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Daniel Lee,morty,"I think this paper raises an excellent point, but lacks sufficient support of its claims. I think the most important aspect of this paper is its overall ethos to do right by science, but struggles in terms of accuracy, novelty, and generalizability—all of which are discussed further in my report below."," 
Dear Authors,
Thank you for the opportunity to review your manuscript. I am quite glad that you are working on it, because it echoes several of my own concerns about incentives in science. To that end, forgive me in advance as I’m afraid you’ve given me a bit of a soapbox ;-).
In this paper, you develop a model of scientific progression and discuss the slowdown in scientific progress. In linking that slowdown to the increase in the use of bibliometrics to evaluate scientists, you further provide insights for how to expand scientific evaluation in a way that rewards exploration, play, and novelty.
I think there is a lot to admire about this paper. Notably, I appreciate the deep sense of responsibility you feel for the state of science, and from that, the drive to make it better, both for the producers of scientific knowledge as well as the general public as recipients of scientific benefit. Further, I thought your discussion around figure 2 was inspired and showed a great deal of foresight in thinking about how others approach research.
However, I had several concerns in reviewing the manuscript that would make this contribution currently unfit for publication in a traditional science-of-science or innovation journal. In particular, my concerns tended to fall into 3 closely related main areas: accuracy, generalizability, and novelty.
 
Accuracy Concerns
My biggest concern with the manuscript is, strictly speaking, I don’t have a sense of how “correct” it is. This is not to say it is incorrect, but rather, many of the claims it makes (both trivial and significant) feel asserted rather than proven or demonstrated. I think some of these concerns are underscored by the atheoretical nature of the manuscript, which hinges on key terms that lack a formal definition such as “scientific play.” This can lead the manuscript to feel tautological at times (this early work is exploratory because it is early work).
In fairness to the authors, part of this feeling may be their convention of footnoting references rather than including in-text or parenthetical citations, but I believe there is a deeper concern here as well. By way of a simple example – on p.6 the authors describe Science as “the most highly cited scientific journal,” but I believe this is untrue with Nature, NEJM, and possibly the Lancet surpassing Science in citation counts. Overall, there is limited data to back up these claims, the paper presents a conceptual model, but not a formal one, with few testable implications and falsifiable hypotheses (cf. above). This is especially concerning given the Unjournal’s  core focus on open, communicable, and replicable science.
These accuracy concerns are most prominent in the overall story of the paper. For instance, why the focus on citation counts and indices? I may be naïve here, but in my (admittedly limited) experience, researchers are (at least in the short term) more concerned with journal name than citation count (Heckman and Moktan, 2020). Now, to be clear, I think journal obsession still creates a stagnation in science, but I want to note that this distinction is not merely pedantic as it has important implications for your modeling (if the focus is on top journals rather than high citations, do we still see herding behavior at incremental science? ) and your policy implications (while Google Scholar or Web of Science can relatively easily calculate and report alternative bibliometrics, changing T&P doctrine is not done at a collective level and will require buy-in from multitudes of stakeholders).
 
Generalizability Concerns
Closely related to accuracy is generalizability—do these findings hold true across all sciences? Are there any sciences that are more (or less) susceptible to stagnation? Are there any mediators or moderators of stagnation How does interdisciplinarity come in to play?
If “citation obsession” is driving scientific stagnation, would we expect to see similar effects in the arts and humanities? Why or why not?
Novelty Concerns
Finally, the authors rightly note the fact that “scientists too respond to incentives is well demonstrated.” I was pleased to see work by Pierre Azoulay, Brian Uzzi, and Ben Jones (among others), all of whom have done amazing work, in the list of references. However, the contribution of this manuscript was less clear to me, especially given the concerns raised above.
 
 
 
 ","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",70,55,85,50,40,60,,,25,,67,,80,70,90,50,40,60,50,40,60,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",3,3.5,2.3,3.7,3,4,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

","Depends on how you count it, but at least 10 years",at least 30,True,3-4 hours,Favorably,Yes,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",The stagnation in science is correlated with the rise of bibliometrics and other citation measurements,,"I believe this is a credible claim, but requires more to be a believable claim (see below)","1a) a discussion of the difference between citation indices and journal lists and 1b) a discussion of how they impact scientist evaluations
2) a comparison to non-scientific fields
3) any sort of regression model or statistical test
4) formal definitions of the terms at hand (e.g. scientific play, exploration, breakthrough, giant, etc.)","For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,True,Yes,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Stagnation and Scientific Incentives,https://unjournal.pubpub.org/pub/e1scientificincentives/release/1,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2025-03-12T21:55:21.354-04:00,3.5
Urban Forests: Environmental Health Values and Risks,https://static1.squarespace.com/static/56034c20e4b047f1e0c1bfca/t/64cc6e2433011927e704dd75/1691119145031/XHXXZ_urbanforest_2023-8-3.pdf,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",,whiskeyjack,"The paper tackles a very interesting and relevant question about the environmental and health impact of urban forests i.e how human-led greening of urban areas affects well-being through its impact on pollution and pollen. The paper’s strength lies in bringing together several data sources with high spatial resolution and decomposing the policy impact into vegetation density  & greenery, air quality and finally health. Moreover the discussion on health and environment trade off is also new and insightful. The main critique is that claims of causality in the paper are not substantiated within the current estimation framework. However, I believe with the excellent data sources available to the authors this can be addressed in the future.  
",,"Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",60,45,65,45,40,50,55,60,50,67,60,70,51,50,55,50,45,60,70,60,80,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",2.5,2.5,,,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",5,10+,True,One day,A tad bit long but thorough. ,yes,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.","Document a substantial greening up of Beijing by the MMP programme, specifically NDVI growth to accelerate after 2012, the year of the project implementation
 Quantify the impact of urban forests on downwind air quality improvement using a quasi-experimental research design i.e. increased vegetation growth by MMP reduces average PM2.5 concentration at city population hubs by 4.2 percent and led to a 7.4 percent increase in pollen exposure. ",,"I don’t believe these claims are causal as stated by the authors. The estimation framework is not rigorous enough to identify these impacts. Currently, they are correlational evidence.",,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,,This is based on the assumption that comments are incorporated,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",UnjournlReview_UrbanForests.pdf,,True,yes,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Urban Forests: Environmental Health Values and Risks,,Review of Economics and Statistics,"50_published evaluations (on PubPub, by Unjournal)",,,2025-03-11T15:43:12.548-04:00,8
"The Macroeconomic Impact of Climate Change: Global vs. Local Temperature 
(November 2024 version)",https://drive.google.com/file/d/1zyOAE8D2zxb85Z6L4izb2Eot2HLXt-t2/view,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",,useful-fish,"The paper’s empirical innovation is simple: estimating the impacts of temperature on GDP using global time-series data, implicitly capturing the relationship between global temperatures, extreme events, and GDP without modelling all intermediate pathways. This idea, combined with their rigorous analytic approach, may have far-reaching implications due to the huge magnitude of the resulting estimates.
Given the small sample size used to estimate their main results (N<60), robustness checks and cautious inference are essential. I recommend additional checks, which may increase their uncertainty estimates, and could result in either higher or lower estimated impacts.",,"Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",90,80,100,92,85,100,88,100,75,98,95,100,100,100,100,100,100,100,95,90,100,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4.7,4.7,4.5,5,4.5,5,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",7 years,Around 5,True,2 days,Good,Yes,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.","“the macroeconomic impacts of climate change are six times larger than previously documented.” (p1)
see review text for details",,"85%: I’m pretty convinced that the paper makes an important contribution. The mechanism they suggest (extreme events) is plausible, and consistent with other climate impacts literature that shows large impacts of windspeed and extreme rainfall. 
There are some limitations to using historical estimates to project future damages, as there could be substantial adaptation, technological progress, or climate tipping points which have not been observed. But any empirically based study will not be able to account for these potential factors for which we do not have experience observing.
Whilst technological progress or adaptation could mean that the study is overstating damages, there are also reasons to think this study is an underestimate – existing social cost of carbon estimates find that their largest component is mortality damages, which are not captured in this GDP focused study.
Overall, given the small sample size, some more robustness checks and simulation evidence to show that their standard error estimator is likely to be working well could help to improve confidence. See report for more details.","​Discussed in report, including specific suggestions for analysis.","For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),"Possibly hugely influential, as long as the findings are robust. I think the are likely to be robust, but that some more checks and ","Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,Very clearly written,"Data sets are all public and clearly described. Replication code for the key results has been made public on Github ahead of publication, which is great and very transparent.","Extremely relevant to practitioners. Some more context on some of the results is warranted, especially around what the results don’t capture (e.g.adaptation, possibility of technological change, climate tipping points, and non-market damages), since this paper could be quite influential for policymakers. ",,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,The key implication is that unilateral decarbonisation policy could be cost effective. See text of review for more details.,True,Yes,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,The Macroeconomic Impact of Climate Change: Global vs. Local Temperature ,https://unjournal.pubpub.org/pub/evalsummacroclimatechange/release/4,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2024-12-20T15:40:19.510-05:00,16
The Macroeconomic Impact of Climate Change: Global vs. Local Temperature,https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fdkaenzig.github.io%2Fdiegokaenzig.com%2FPapers%2Fbk_micc.pdf&data=05%7C02%7Cncook%40wlu.ca%7C65de24d511944f3114c208dd02739083%7Cb45a5125b29846bc8b89ea5a7343fde8%7C1%7C0%7C638669420252518565%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=za1XJlMP1UHP9jzmImdEAqgOi%2BDo056CWsIOcJ2hcoE%3D&reserved=0,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",Anonymous,Pizza Margherita,"If I were reviewing this manuscript for an economics journal, I would recommend revise and resubmit, with only minor revisions. Overall, the manuscript is well written in the sense that, since the reader understands immediately what is being estimated and why, that they could be lulled into the trap of thinking that the research is ""obvious."" The final line before the conclusion is ""We conclude that climate change poses a substantial threat to the world economy."" But it is the careful empirical construction of the argument, and the identification that a literature has been mechanically missing (netting out) the effect of global temperature shocks  while focussing exclusively on local temperature shocks, that gives this manuscript its evident worth. The manuscript consists of two parts. First, a reduced form impact of global temperature shocks on economic activity at the wold and country level. Second, a structural model to convert those estimates into welfare losses and a valuation of the social cost of carbon. The first is the ""heart"" of the paper in my estimation, while the second half is where many articles stop short - the ""so what.""","This is a reviewer report for ""The Macroeconomic Impact of Climate Change: Global vs. Local Temperature."" The version reviewed is labelled November 2024.
If I were reviewing this manuscript for an economics journal, I would recommend revise and resubmit, with only minor revisions.
The manuscript's main claim is that macroeconomic damages from climate change are six times larger than previously thought (around 12% instead of 2% reduction in world GDP following 1 degree Celsius of warming). This is because previous work does not account for (or indeed misses the effect of) global shifts in warming. The effect of global (rather than local) warming is proposed to occur through many mechanisms (considered via the top-down rather than bottom-up analysis), while the warming increases extreme climate events mechanism is specifically evidenced. A suite of robustness checks, at all stages of the comprehensive analysis, assuage many of the concerns the applied economics reader may have. These previously underestimated damages, when used to inform a standard neoclassical growth model, paint policy makers a dire portrait. The 2024 world GDP is around 20% lower than it would have been absent the already observed warming. Further, 2040's GDP will be an estimated 25% lower than it would have been under business-as-usual projections. The related Social Cost of Carbon, which is often used as a benchmarking for whether an emissions-reducing policy is ""worth it"" is $1,367 per ton of abated CO2. This is striking when compared to the fact that most decarbonization policies cost around $80 per ton. 
Overall, the manuscript is well written in the sense that, since the reader understands immediately what is being estimated and why, that they could be lulled into the trap of thinking that the research is ""obvious."" The final line before the conclusion is ""We conclude that climate change poses a substantial threat to the world economy."" But it is the careful empirical construction of the argument, and the identification that a literature has been mechanically missing (netting out) the effect of global temperature shocks  while focussing exclusively on local temperature shocks, that gives this manuscript its evident worth. The manuscript consists of two parts. First, a reduced form impact of global temperature shocks on economic activity at the wold and country level. Second, a structural model to convert those estimates into welfare losses and a valuation of the social cost of carbon. The first is the ""heart"" of the paper in my estimation, while the second half is where many articles stop short - the ""so what.""
In my estimation, the result - that global temperature shocks have large and persistent negative effects on GDP - has the potential to reframe a literature that began with Dell, Jones, and Olken (2012). I hope that this work has the same effect as Goodman-Bacon for the DID literature. Empirically, it reads as the inevitable next step of the climate change - temperature - extreme event - GDP literature. 
The manuscript contains numerous robustness tests, including those which involving much work (such as re-estimation with a temperature measure independently created than the primary one), and clear explanations about why each was implemented. 
The data and methods used in the estimation of warming damages are state-of-the-art and conducted in a clear and reasonable manner. The use of the Berkeley Earth Surface Temperature Database is compared against using National Oceanic and Atmospheric Administration data, to little difference. Literature standard data are taken from ISIMIP, the Penn World Tables, and Jordà-Schularick-Taylor Macrohistory database. The methods used are approachable and appropriate - and robust to alternative research choices such as, when appropriate, not using the Hamilton projections. 
In section 5.2, it is tempting to make the pessimistic argument that because we are not infinitely lived households, who begin saving anticipating lower income going forward, but instead myopically operate, these estimates are a ""best case"" for our behaviour, under business as usual warming. I would prefer, however, a smaller uncertainty in the SCC - even if it need not be a 95% confidence interval - looking forward on to how this work will be applied when it is published. If the reader-policy-maker must make use of the lower bound number of the reported confidence interval, it is temping to offer instead a higher 90% lower bound.
My primary request of the authors is to provide a comparison between the business-as-usual and the other pathways ahead. Give us, the readers, a section which tells us just ""how bad"" things can be, yes, but also under different IAM's and pathways, such as in Burke, Hsiang, Miguel (2015) Figure 5. 
My secondary request is to increase the lags applied throughout (but most specifically in section 2.2) to at least 7 (the appendix goes to 4) given the cited geo-physical literature noting cyclicality of solar cycles being as long or longer. 
A taste request is to characterize - up front - just how out of sample a 1C change truly is. The largest in the sample is 0.3C, calibrating response damages to this largest, but not particularly uncommon, shock might add to the credibility of the paper rather than take away from its message.
Also, on page 19, there is a typo. ""Consistently"" is likely to be meant in this context as ""Consistent"".","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",85,80,90,85,75,95,80,90,70,90,89,91,85,75,95,25,10,40,85,80,90,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4.5,4.5,4,5,4,5,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

","I would like to remain anonymous because I do not want to alter the probability of this manuscript coming across my desk in future. If a journal editor knows of my position, and that knowledge affects the probability of my being selected as a referee (whether up or down) this is a disservice to the profession. I WILL however, disclose that I know WHO wrote the paper if the journal believes the review would have been anonymous.","Responses to these will be public unless you mention in your response that you want us to keep them private.

",5 complete years,More than 30,True,10 Hours,"Rigorous and unfamiliar, but good",Yes,"Thank you for the invitation. This is a good paper, and I am glad to have read it.","Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",The manuscript's main claim is that macroeconomic damages from climate change are six times larger than previously thought (around 12% instead of 2% reduction in world GDP following 1 degree Celsius of warming). This is because previous work does not account for (or indeed misses the effect of) global shifts in warming. ,,I believe this claim.,See above body.,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),A careful capstone or next step for a mature and important literature.,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",See my primary request.,Additional expansion on comparisons and evaluations using previous methodologies - a slight tweak in focus - will go a long way for the reader who has followed this literature evolve.  ,"The better assessment of a social cost of carbon is a first-order problem for humanity, with strong down-stream effects through the policy process.","The concepts are clearly defined and reasoning is first-rate. The nature of the evidence is detailed, for a sophisticated reader perfectly explained.","The current replication is incomplete, does not cover all of the analysis (e.g. no panel estimations), and is far from comprehensive. Hopefully the authors offer a full replication package in time. ",,I use economics journals only in my estimation here. ,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,"An independent replication of this work should be commissioned, and if they find a similar magnitude for the SCC with equally or more open-science practices, it should be presented alongside the publication of this academic work.",True,"I would be interested out of professional interest, yes.",,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,The Macroeconomic Impact of Climate Change: Global vs. Local Temperature ,https://unjournal.pubpub.org/pub/evalsummacroclimatechange/release/4,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2024-12-17T15:10:07.370-05:00,10
The animal welfare cost of meat: evidence from a survey of hypothetical scenarios among Belgian consumers,https://lirias.kuleuven.be/3946788?limo=0,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",No,Reviewer ,"This study explores animal welfare costs of meat consumption, using stated preference methods from a sample of Belgians to elicit willingness to pay (WTP) and willingness to accept (WTA) measures for animal welfare. While the theoretical framework is robust and the research attempts to mitigate several biases, the small sample size and reliance on open-ended WTP elicitation limit reliability. The study provides valuable insights into relative animal welfare costs and consumer preferences, serving as a foundation for future research.
",,"Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",80,70,90,70,65,75,65,70,60,90,85,95,90,85,95,80,75,85,80,70,90,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",2.5,,2,3,,,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",5-10 years ,"Dozens of peer-reviewed articles, dozens of peer-reviewed government/intergovernment reports, dozens of grants ",False,4-6 hours ,Above average ,This work has already been published.,No.,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.","The most impactful claim in Bruers (2023) is the following: the median estimate of the
animal welfare cost of chicken meat is 10 euro/kg, whereas its mean
estimate is several orders of magnitude higher. The evidence underlying this comes from a survey of Belgian consumers conducted in 2022, who are responding to a hypothetical valuation question that involves how much money they would be willing to accept for getting the experience of chickens from a conventional farm. This claim is the most important because it 1) provides further quantification of animal welfare externalities, 2) suggests a high degree of variability, and 3) indicates that some consumers have valuations that are very large by most measures.
",,"Since the underlying survey was performed in only one country, using a small sample, and with methods that are not completely free of hypothetical bias, I do not find the point value itself credible. However, it could help inform non-zero lower bounds on the intrinsic value of animal welfare for use in animal-related regulations.",,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
","Review, The Animal Welfare Cost of Meat v2.docx",,True,"If others are interested, yes.",,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,The animal welfare cost of meat: evidence from a survey of hypothetical scenarios among Belgian consumers,https://unjournal.pubpub.org/pub/evalsumanimalwelfarecost,Journal of Environmental Economics and Policy,"50_published evaluations (on PubPub, by Unjournal)",,,2024-12-01T16:26:09.942-05:00,5
Adaptability and the Pivot Penalty in Science and Technology,https://www.dropbox.com/scl/fi/r1qf2mnf07zifk7glz7tc/MS_SI_combined.pdf?rlkey=ucbeagmu3951u7busb2654mtr&e=2&dl=0,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",,evaluator,"This paper presents a novel framework for measuring the degree of departure from prior research in new works, documenting the ""pivot penalty,"" where the impact of research declines as scientists venture further from their past focus. The strengths include its broad applicability, strong data, and policy implications. Concerns include under-explored benefits of pivoting, alternative explanation such as timing of rewards, evolving journal preferences, and the choice of journals over fields for analysis.",,"Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",80,70,90,82,74,90,85,92,78,85,78,92,80,71,89,90,85,95,80,70,90,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4.5,4.5,4,5,4,5,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",3 years,6,False,3 hours,8/10,yes,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.","“pivot penalty,” where the impact of new research steeply declines the further a researcher moves from their prior work.",,"As I mentioned in the evaluation, I feel the claim is still too strong, even with some empirical support. There should be something happening but overlooked in the measurement framework.",It might be helpful to see the promotion/funding these pivoting scientists [get].,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",Review_Adaptability_Pivot_Penalty.docx,,True,"maybe, but not sure about my availability at that time",,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Adaptability and the Pivot Penalty in Science,https://unjournal.pubpub.org/pub/evalsumpivotpenalty/release/1,Nature,"50_published evaluations (on PubPub, by Unjournal)",,,2024-11-30T21:26:25.792-05:00,3
"How Effective Is (More) Money? Randomizing Unconditional Cash Transfer Amounts in the US
",https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4154000,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",,NR,"Overall, I find this paper competently executed. I like the inductive approach to theorizing after the findings, the multiple tests for mechanism, extensive robustness checks, and the use of transaction-level data to check what participants used the money for. The paper has external validity limitations, and can be framed better since the unconditional cash transfers were given during the Covid pandemic. Several further tests to test the main mechanism, and improve the mediation analysis.","Overall, I find this paper competently executed. I like the inductive approach to theorizing after the findings, the multiple tests for mechanism, extensive robustness checks, and the use of transaction-level data to check what participants used the money for. Overall, I have several suggestions for improving the paper.
 
-The paper is framed, and the contribution posed mainly about unconditional trash transfers (UCT). However, the experiment was conducted during the COVID-19 pandemic. Therefore, the paper is also about UCT during a pandemic. The authors are probably the first ones to study this, but this contribution should be made upfront. The authors, therefore, should relate to further literature on cash transfers during the pandemic.
-Related to the above, the paper has limited external validity given the end of the pandemic/conducting the experiment under very particular circumstances. This is, of course, beyond the control of the authors, so an alternative approach is to recognize upfront the context/circumstances under which the experiment was conducted and not wait until the conclusion.
-As the paper speculates, cash treatments might have been too small to reach any positive effect. The problem is that testing higher-value transfers is limited, given the costs. UCTs may be more viable in developing contexts, where the values can be larger compared to annual income, as the authors suggests.
-The main mechanism behind the potential negative effects of the treatment suggests that the UCT can remind receipts of the debts that they can't cover. Beyond the model, this could be explored further empirically; according to the stock of debt pre-treatment, do people with higher debt stock have more negative effects? Some heterogeneous effects with levels of debt could be done.
-The auxiliary 15 participants' interviews could be improved by improving the sample size and ideally having representative groups in treatment and control groups. If the size of the interviewees can't be scaled up, given that significant time has passed since the experiment, at least the authors should do their analysis separating interviews from the treatment and control group.
-The mediation analysis can be improved in two ways:
                -The mediators can be further disaggregated; the variables, Money Mind – Cost, Overwhelmed Others' Needs, and Spending Stress can be used in Table H.11 separately to see if main treatment effects change, under using only one of those.
                -Most importantly, the mediation analysis can improve beyond just controlling for mediators and checking how the mani effects change. For example, see the proposal by Acharya, Blackwell, and Sen (2016).
 
-Did the authors try exploring het effects according to the previous level of income? The treatment effects might depend on where the participants were in the income distribution/how big the cash transfer is with respect to their income.
References:

Acharya, Blackwell, and Sen (2016), American Political Science Review , Volume 110 , Issue 3 , August 2016 , pp. 512 - 529
","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",85,80,90,85,80,90,95,100,90,75,70,80,95,90,100,85,80,90,75,70,80,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4.5,4,4,5,3.5,4.5,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",5 years,Over 20,False,6 hours (reading the paper and writing the evalution),"9/10, all clear",No,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",There are no positive effects of small unconditional cash transfers.,,"Very likely, 90% probability ",The main claim has numerous robustness checks leading to a high credibility. My suggestions in the review are regarding improving the mechanism analysis for the main claim ,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,"The paper is framed, and the contribution posed mainly about unconditional trash transfers (UCT). However, the experiment was conducted during the COVID-19 pandemic. Therefore, the paper is also about UCT during a pandemic. The authors are probably the first ones to study this, but this contribution should be made upfront. The authors, therefore, should relate to further literature on cash transfers during the pandemic.The paper has limited external validity given the end of the pandemic/conducting the experiment under very particular circumstances",,"Methods and data collection is quite clear, data is not yet widely available since the paper has not been published. Beyond related literature, there are no resources that can enable future research.","The paper relevance is limited for global priorities since the experiment was conducted during the COVID pandemic, weakening its external validity, and applications in other contexts. Nevertheless the question is important, given the wide use of conditional cash transfers. The transfers given in the experiment might have been low, 500 and 2000 USD, to find a positive effect, however larger cash transfers might be difficult to do practically, therefore the usefulness to practitioners might be limited. However, it is useful to know at least that smaller unconditional cash transfers have limited effects suggestion these should not be done. There is no assessment of quantified costs/benefits.",,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,"There are no positive effects of small unconditional cash transfers (UCT). Implication: Policy makers should not use small UCT, given the lack of positive effects.   I believe this claim 80%, the caveat is the experiment was done during the covid-19 pandemic and its unclear if the findings could be using during regular circumstances. However, independent of the context, I do believe such small transfers are not likely to substantially change outcomes, more so under a context of high inflation.",True,No,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,How Effective Is (More) Money? Randomizing Unconditional Cash Transfer Amounts in the US,https://unjournal.pubpub.org/pub/evalsumrandomizingcashtransfer,SSRN Electronic Journal,"50_published evaluations (on PubPub, by Unjournal)",,,2024-11-11T14:43:01.964-05:00,6
Building Resilient Education Systems: Evidence from Large-Scale Randomized Trials in Five Countries,https://www.nber.org/papers/w31208,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",,TabbyCat,"> Provides important evidence on effectiveness of a low-tech intervention to stem learning losses from school closures (likely to increase in coming years) by scaling it up vertically & across diverse geographies
> Note that results only apply to numeracy skills and for temporary & abrupt school closures. Future work should see if it is effective for other skills & circumstances (displaced people, girls), and if contextualizing content/methods improves outcomes
",,"Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",83,73,93,80,72,88,78,88,68,88,78,98,79,71,87,62,52,72,90,85,95,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",3.9,4.5,3.5,4.4,4,5,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",1 year,"None, have reviewed proposals for ethical review though - around 10 or 20 of them",False,A little over a month,"Not bad, I’d crib about it but that’s just me trying this out for the first time",i don’t want to but I am obligated to,"The rating sliders are a bit wonky and think it would be better to have like a text-box in there, and the process to upload a file of the written evaluation just stacks documents instead of replacing the previous one or having the option to delete one – and I’m not re-entering all of these details again (both are identical with just one minor formatting difference).","Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.","SMS messages containing educational content along with nudges to engage in educational activities combined with short-duration tutoring sessions over a phone-call, the content of which was adjusted through regular assessment of the student’s learning levels, raised numeracy skills (measured as ‘levels’ and later standardised using values from the control group) by 0.30 to 0.35 standard deviations.",,"My subjective 90% interval for this is (0.10, 0.50) standard deviations and is primarily based on the results from table 4 of this paper which shows the country-wise effects of the interventions. An additional factor for this confidence interval is that this study addresses some of the concerns pointed out by Crawfurd et al., 2021 in its pilot, that of households self-selecting into the treatment.",The confidence for this claim would be marginally improved by robustness checks that check all or a minimum of more than half the countries included in the study. At least three robustness checks are conducted in only one country each,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,,,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
","Building Resilient Education Systems review.docx,Building Resilient Education Systems review.docx",,False,Sure,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,Building Resilient Health Systems: Experimental Evidence from Sierra Leone and The 2014 Ebola Outbreak,,The Quarterly Journal of Economics,"deprioritized bc. of journal-publication status, authors' permission, age, etc.",,,2024-10-22T05:09:03.456-04:00,
Intergenerational Child Mortality Impacts of Deworming: Experimental Evidence from Two Decades of the Kenya Life Panel Survey,https://www.nber.org/papers/w31162,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",No,Evaluator 2,"This is a highly credible and important analysis of the intergenerational effects of a Kenyan deworming intervention. Such effects are important to document because they could meaningfully affect the cost-benefit analysis of such programs and also teach us about the mechanisms through which parents transmit advantage to their children. However, the analysis would be more credible if the authors were transparent about their deviations from the pre-analysis plan, which include the addition of (and exclusion of) outcomes and changes to the empirical specification. Also, more information on the “first stage”—the impact of randomization on receipt of deworming treatment—would be helpful.",See attached files,"Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",90,80,100,90,80,100,85,95,75,80,70,90,90,95,100,80,90,100,90,95,100,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4.499999999999998,4.499999999999998,4,5,4,5,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",Thanks for the opportunity to review this paper. I found it interesting. All of my comments are in the report and may be made public.,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",10+ years,100+ (I have not tracked),False,"Including reading the paper, about 4 hours",Some of the questions were repetitive of what was included in my report,Yes,This is an interesting project—thanks for including me in it. ,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.",The primary claim is that exposure to additional years of deworming treatment in primary school results in lower child mortality of the children of those exposed. The evidence provided is from analysis of an intervention randomized at the school level that compares the mortality of the children of those exposed earlier vs later via the intervention. The outcome data is derived from long-term follow up survey data of these experiment participants. I discuss the importance of this claim in detail in my report.,,"I believe with fairly high certainty the general claim that mortality of the children of those randomly exposed to the intervention fell and believe it likely falls within the confidence intervals implied by the standard errors reported in the paper. However, some uncertainty may remain due to the fact that not all sampled participants completed the follow-up surveys. ",See report,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",The randomized design makes the results highly credible,Changes from pre-analysis plan should be noted,"This paper makes a strong advancement, although other papers suggest such intergenerational effects were likely",Very well-written and clear,The authors have posted the data: https://dataverse.harvard.edu/dataverse/KLPS,Highly relevant,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
","UnjournalReport.pdf,UnjournalReport.docx",See report,True,"No, I do not have additional time available to devote to this",,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,"Intergenerational Child Mortality Impacts of Deworming: Experimental Evidence from Two Decades of the Kenya Life Panel Survey
",https://unjournal.pubpub.org/pub/evalsumintergendeworming,WP,"50_published evaluations (on PubPub, by Unjournal)",,,2024-09-25T12:55:27.387-04:00,4
How Effective Is (More) Money? Randomizing Unconditional Cash Transfer Amounts in the US,https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4154000,"This form pertains to our main (academic) stream. See our overall ‘guidelines for evaluators’ here. We ask evaluators in this stream to:

Write an evaluation, up to the standards of a high-quality referee report for a traditional journal.  Consider standard guidelines as well as The Unjournal’s emphases.  Please try to address any specific considerations mentioned in our bespoke evaluation notes, and any specific requests from the evaluation manager. 
Give quantitative metrics and predictions. 
Identify and assess the ‘most important claim’ of the research. 
Answer a short questionnaire about your background and our processes.

Our full evaluation guidelines are here.

Compensation: As a sign that we value this work, we compensate invited evaluators for prompt evaluations. Oct. 2025: we currently target $350 in compensation per on-time complete evaluation on average, including incentives and prizes. If you have been invited, the specific honorarium details should have been made clear to you. Consult and query our knowledge base for our updated compensation policies.

The present form does not save your work; however, you can copy and work in this Google Doc (~approximate copy of this form) if you like. 

(Internal note 22 June 2025: We’ve been building a better-formatted form via the  PubPub Platform tool. However, their development is paused/frozen due to funding issues.)


“Independent evaluators”: We’re  exploring an “independent evaluations” trial (see linked details). If you are doing this as an independent evaluation (i.e., you were not explicitly commissioned by The Unjournal to evaluate this paper), please see the explicit guidance here  on how to use this form as an independent evaluator.

Remember:  Please try to evaluate the most recent version of the paper that you can find.

",No (but please contact me if other reviewers have publicly identified themselves—I just don’t want to be the only one!),Jazzy Jeff,"The paper presents and interprets the results of a well-powered one-time unconditional cash transfer experiment for low-income people in the US. The cash significantly increased recipient spending, but decreased survey measures of self-assessed wellbeing relative to control. Participant attrition limits the strength of the conclusions we can draw from this high-quality study. Overall, the study modestly shifted my views against one-time transfers and toward other policy tools for reducing poverty and inequality in wealthy societies.","SUMMARY
 
This is an evaluation of the paper “How Effective Is (More) Money? Randomizing Unconditional Cash Transfer Amounts in the US” for Unjournal.
 
The paper presents and interprets the results of a well-powered one-time unconditional cash transfer experiment in the US during 2020 and 2021. The experiment randomly assigned low-income people to receive either $2000, $500, or zero dollars, and then measured various outcomes pre- and posttreatment using participants’ bank data and a four-wave survey (with one wave being pretreatment).
 
The results suggest that $2000 and $500 treatments each significantly increase recipient spending, especially on digital cash transfers and retail, with no effect on spending on ‘harmful’ items such as alcohol, tobacco, or gambling. However, the results also show significant decreases in the survey measures of financial, psychological, cognitive, and physical health in the cash groups relative to the control group.
 
The paper presents its main interpretation of the negative results on the survey measures using a model that shows that people who 1) have negatively biased expectations of the size of a negative financial shock and 2) are induced to monitor their finances more closely due to receiving a positive financial shock (e.g., the one-time cash transfer treatments in this study) might end up unhappier than they would have if they had not received the cash transfer. More concisely, the argument is that the “unhappy surprise” of learning more about negative shocks to one’s own finances can outweigh the “happy surprise” of receiving the one-time cash transfer.
 
Overall, the paper makes a strong contribution to the cash transfer literature for a number of reasons, including the study’s multiple treatment arms, its geographic context, its timing during the height of the COVID pandemic, and its measurement of a broad range of outcomes. The study also has some important limitations, including participant attrition and challenges in nailing down the mechanism driving the survey results.
 
 
ATTRITION
 
Sample attrition is the main limitation of the study. The Manski bounds for treatment effects are correspondingly large. As one example, the lower and upper possible bounds for the treatment effect of the $2000 transfer on participants’ financial health are -1.30 to 1.02, respectively. Lee (2009) bound coefficients are also wide, with 95% CIs often approximately [-0.35, 0.25]. These are, of course, very conservative bounding exercises, but they put the main estimates in helpful context.
 
While the bounding exercises give pause, the consistently significant negative estimates after multiple imputation (as proposed in the study’s preanalysis plan) increase my confidence in the results a bit. Still, the fact that the treatment effect results are smaller after multiple imputation is consistent with attrition indeed biasing the paper’s main estimates in a negative direction.
 
The paper also offers helpful theorization of the mechanisms by which attrition might bias the results in a negative direction in Section C.4. Possibilities include things like treated and control individuals having different baseline conditions, or living in areas that are trending differently with respect to COVID or economic conditions. There remain many additional possible mechanisms based on unobservables, but the evidence presented in this section helps to allay concerns about some of the more straightforward possibilities for how attrition would negatively bias the results.
 
In summary, the paper does a nice job of dealing with attrition as best it can, but it remains the main obstacle to learning more about the true treatment effects. I don’t have any great insights to offer on this front, but I hope that more work is done in future studies to find ways of improving response rates in the field.
 
 
SOCIAL WELFARE EFFECTS – SELF-ASSESSED WELLBEING
 
What does this study teach us about policy solutions to poverty and inequality? In this section I focus on broader interpretations related to self-assessed wellbeing. This study’s results suggest that one-time unconditional cash transfers are not effective at improving self-assessed quality of life among poor people within 15 weeks (it’s possible, though I would not say likely, that the one-time transfers positively affected the longer-term wellbeing of treated participants).
 
However, the social welfare effects of one-time transfers like these remain ambiguous. As I described earlier, the study argues that the non-positive effects on self-reported wellbeing stem from treated individuals being induced to more “actively” monitor their finances, which leads to “unhappy surprises” about negative shocks to one’s finances that outweigh the “happy surprise” of the one-time cash transfer. Readers will vary in how normatively good or bad they think this story is.
 
Another ambiguity of the broader welfare effects involves network effects. Treated individuals often shared or spent the money with/on family and friends, but we don’t have measures of self-reported wellbeing of the friends and families. Section D.6 shows that the treatment didn’t affect respondents’ relationships, but only with people “outside the household.” If the paper’s main mechanism is correct, then it is plausible that individuals close to the treated individual get an indirect version of the “happy surprise” of a one-time cash transfer while avoiding the “unhappy surprise” of learning about unexpected negative financial shocks.
 
Taken together, we still have a lot to learn about the broader effects of one-time cash transfers on aggregate (self-understood) wellbeing.
 
 
SOCIAL WELFARE EFFECTS – REDUCING POVERTY AND INEQUALITY
 
Does this study change my view of one-time cash transfers as a policy solution to poverty and inequality in wealthy country contexts? It is worth noting that cash transfers mechanically make people less poor, at least for some amount of time. The study is limited by its measurement timeframe, but recipients spent significantly more over the period (often some months) for which the study has bank account data. These outcomes reflect important policy goals in and of themselves.
 
The policy importance of cash transfers matters even if the cash does not increase recipients’ net worth. For example, the new Bartik et al (2024) study has a different treatment (monthly cash transfers for three years), but finds no effect of the transfers on net worth because recipients took on more debt (often car loans) to match their newly increased incomes—but if recipients were better positioned or freer to access credit after receiving cash transfers, this would represent a policy success for the many who have called for greater equality in credit provision across class, race, and gender. For more discussion of the normative implications of access to credit and debt, see the work of legal scholar Abbye Atkinson. (It is also worth noting here that we do not know much about the very long term effects, on the order of decades, lifetimes, or across generations, of cash transfers.)
 
 
CONCLUSION
 
The “How Effective Is (More) Money?” paper makes a contribution to our understanding of one-time cash transfers to the poor in wealthy society contexts. The experimental design was strong, with a well-powered sample and many important repeated outcome measures. The paper also does a really nice job squeezing every possible ounce of inference out of the available data. Respondent attrition and uncertainty about mechanisms are common in field experiments, and scholars can use this paper as a guide for how to gauge the size of potential bias in estimates and adjudicate between potential mechanisms driving unexpected results.
 
The paper did indeed shift my beliefs about the medium-term effects of moderately sized one-time cash transfers on self-assessed wellbeing downward a bit, from an expectation of positive effects toward an expectation of zero effects. Attrition does weaken the signal conveyed by the estimates. But there still is a signal. A true positive effect is still possible in this case, but this would require the respondents who dropped out of the survey to have pretty exceptionally distinct trends in self-assessed wellbeing.
 
In terms of informing policy, this study did not change my views very substantially on the role of cash transfers as a tool for improving society. As I described earlier in this review, even a true negative effect of cash transfers on self-assessed wellbeing could be normatively offset by the spending effects or if there exist positive network effects for recipients’ family and friends. Cash transfers, both by giving money to low income families or by financing them through taxation, can be a tool to reduce economic inequality and the negative externalities that come with it. Furthermore, current and proposed cash transfers seem to work well in the US. This study doesn’t present direct evidence against the efficacy of a child tax credit, which, in one new prominent national proposal in the US, would be a one-time $6000 transfer. A plethora of difference-in-differences analyses of the Earned Income Tax Credit, such as those by Hilary Hoynes and her collaborators, show positive effects on infant and child health, educational outcomes, future earnings, and even reductions in children’s long term chances of being incarcerated.
 
However, societies’ policy choice is not simply a binary yes or no to cash transfers. There are other policy tools to increase spending by the poor and reduce economic inequality, and their tradeoffs relative to cash transfers should be compared. This study’s findings modestly shift my views away from unconditional one-time cash transfers toward other policy tools that would increase worker bargaining power, like sectoral bargaining, other welfare state tools that increase redistribution while also reducing rent extraction, like universal single payer health insurance, or industrial policy. Still, as global wealth continues to increase, I see an important role for unconditional cash transfers.
","Claim identification, assessment, & implications (optional but rewarded)

This section is meant to help practitioners use this research to inform their funding, policymaking, and other decisions. It is not intended as a metric to judge the research quality per se.
This is mainly relevant for empirical research. If 'claim assessment’ does not makes sense for this paper, please consult the evaluation manager, or skip this section. 
We provide some examples of precisely-stated claims here, also drawing from the SCORE project, the Social Science Prediction Platform, and Metaculus. 
",80,65,95,85,75,95,90,95,85,70,50,90,90,85,95,90,85,95,80,70,89,"Below, we ask you to consider this research on a different scale, considering ‘journal ranking tiers’ as follows:
0.0: Marginally respectable/Little to no value
1.0: OK/Somewhat valuable
2.0: Marginal B-journal/Decent field journal
3.0: Top B-journal/Strong field journal
4.0: Marginal A-Journal/Top field journal
5.0: A-journal/Top journal
See the discussion of “journal ranking tiers” in our guidelines for more details, with some linked examples. We ask you to not just use integers, but to consider these to be on a continuous scale. E.g., “3.5” represents a journal that ranks halfway between a ‘top field journal' and a ‘strong field journal’.

",4.2,4.200000000000001,3.5,4.9,3.500000000000003,4.899999999999999,"Please write any confidential comments here

Your comments here will not be public or seen by authors. Please use this section only for comments that are personal/sensitive in nature. Please place most of your evaluation in the public section.

",,"Responses to these will be public unless you mention in your response that you want us to keep them private.

",12 years,Hundreds,False,One workday (not including initial reading and thinking about the paper),"Many strengths! The numeric measures will likely not be useful until there are repeated measures for individual reviewers. I also liked the push for discussion of policy relevance. However, I think the separate smaller questions could be collapsed into the main review.",Yes,,"Structured ratings/metrics
Responses will be public. 

Percentile ratings: For each of the criteria below, please rank this paper relative to all serious research in the same area that you have encountered in the last three years. For example, choose 51% if you think this paper is better than 51% of such work. We define and discuss this in more detail here.

Criteria: For each of the criteria described briefly below, please see the descriptions given in our guidelines for further definition and guidance. Not all categories may be relevant for each paper; when in doubt, please ask the evaluation manager.

Why are we asking for structured ratings? See here.","The most important factual claim is that one-time unconditional cash transfers of $500 or $2,000 to poor people in the US increases personal spending (Figure 5), but not self-reported well-being (financial, psychological, cognitive, and physical)—and may even decrease self-reported wellbeing (Table 2). The evidence comes from comparing bank data and survey responses between the randomly assigned treatment groups among the sample of 5,243 individuals.
This claim is important in that it updates our estimates of the causal effects of one-time unconditional cash transfers on self-reported wellbeing. Given the cost of administering and studying these programs, and likely heterogeneous effects across study populations and treatments, a well-powered RCT with multiple treatment levels and longitudinal outcomes measurement moves the ball forward in our understanding.",,"The paper is attentive to uncertainty due to attrition (Section 4.6). Correspondingly, the paper expresses confidence in the lack of a positive effect on self-reported wellbeing, but not necessarily the negative point estimates. The paper calculates Lee bounds (Table H9) and Horowitz-Manski bounds (Section C5) to assess the possibility that missingness is driving the apparent negative treatment effects on self-reported wellbeing.
I believe the claim that treatment effects on self-reported wellbeing are not positive with a probability of over 90%. I believe the claim that the treatment effects on spending are positive with a similarly high probability. I believe the more ambitious claim that there is a negative effect on self-reported wellbeing with a probability of 70%. However, as I describe elsewhere in this review, we know much less about spillover effects from this treatment—and therefore I remain uncertain about the effects of these one-time cash transfers on aggregate (self-assessed) wellbeing.
",,"For all of the criterion below, please use the slider to indicate your rating (center slider), and the lower and upper endpoints of your 90% credible interval (CI). You should think that the bounds have a 90% chance of containing the “ideal rating” - the rating someone would give who had complete knowledge of the topic, the methods, etc., and who devoted a large amount of time to considering this. For example, if you think there is a 90% chance that this research is better than 60-80% of other research, choose 60 and 80 as your credible interval. 

For more guidance on this see the “credible intervals: expressing uncertainty” discussion in our guidelines. If you have further questions about this, or if you want to state your uncertainty in a different way, please contact contact@unjournal.org.

If you find the sliders themselves difficult to use, you can put your ratings and 90% CIs in the comment section below ""(lower, midpoint, upper)"".","Judge the quality of the research heuristically.  Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.","Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?","Are methods clearly justified and explained? Are methods and their underlying assumptions reasonable and appropriate? Are the results likely to be robust to changes in the assumptions? Have the authors avoided bias and questionable research practices? 
","To what extent does the project contribute to the field or to practice, particularly in ways that are directly or indirectly relevant to global priorities and impactful interventions?","Are concepts clearly defined? Is the reasoning transparent? Are conclusions consistent with the evidence (or formal proofs) presented?  Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis, including tables and figures, relevant to the argument? 

","Would another researcher be able to replicate the analysis? Are the method and its details explained sufficiently?  Is the source of the data clear?  Is the data made as widely available as possible, with clear labeling and explanation? Do the authors provide resources that are likely to enable future research and meta-analysis?","Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? 
Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic?  Do the authors report results that are relevant  to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.)?
","Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.",Final section (survey & comments),,"Evaluation report
Please aim to write a report up to the standards of a high-quality referee report for a traditional journal. Consider standard guidelines as well as The Unjournal’s emphases.  
Remember to address any specific considerations mentioned by the evaluation manager or The Unjournal, including in our bespoke evaluation notes.",,,,,"My rating here is based on the data and its use, not the study’s experimental design in the field, which (like many experiments) likely has many nuances in implementation.",,,"Rating (0.0–5.0).  Please consider this as a continuous measure, not just an integer.
",,,True,,,"Please tick the box below to confirm you have no strong conflict of interest in evaluating this paper because all of the following are true:

You have never co-authored a paper with any of the authors of this research
You are not at the same institution as any of these authors
You have no other strong personal, professional,  or financial ties to these authors or the results of their research
","Feedback

Responses to the questions below here will not be public or seen by authors.
",,How Effective Is (More) Money? Randomizing Unconditional Cash Transfer Amounts in the US,https://unjournal.pubpub.org/pub/evalsumrandomizingcashtransfer,SSRN Electronic Journal,"50_published evaluations (on PubPub, by Unjournal)",,,2024-08-20T17:16:06.686-04:00,8
