---
affiliation:
- id: 0
  organization: University of Wisconsin-Madison
article:
  doi: 10.21428/d28e8e57.df86328a/b800ea6d
  elocation-id: e2chooseyourmoments
author:
- B. Ian Hutchins
bibliography: /tmp/tmp-60IBdUAQdNOnZU.json
copyright:
  link: "https://creativecommons.org/licenses/by/4.0/"
  text: Creative Commons Attribution 4.0 International License
  type: CC-BY
csl: /app/dist/server/server/utils/citations/citeStyles/apa-6th-edition.csl
date:
  day: 01
  month: 06
  year: 2025
journal:
  publisher-name: The Unjournal
  title: The Unjournal
link-citations: true
title: "Evaluation 2 of \"Choose Your Moments: NIH Peer Review and
  Scientific Risk Taking\""
uri: "https://unjournal.pubpub.org/pub/e2chooseyourmoments"
---

# Abstract 

"Choose Your Moments: NIH Peer Review and Scientific Risk Taking" is an
interesting study on review decision-making. Authors examine the effect
of NIH peer review score distribution on decision-making about funding
particular grants, using a portfolio from NIH 2016 RePORTER data, using
a BIBD approach. Their approach is sound and I am persuaded by their
conclusion that reviewers have a preference for grants with increased
variance in review scores, and that there is an asymmetry in preference
for high-variance applications in response to funding shocks.

# Summary Measures

We asked evaluators to give some overall assessments, in addition to
ratings across a range of criteria. *See the *[*evaluation summary
"metrics"*](https://unjournal.pubpub.org/pub/evalsumchooseyourmoments#metrics "null")*
for a more detailed breakdown of this. See these ratings in the context
of all Unjournal ratings, with some analysis, in our *[*data
presentation
here.*](https://unjournal.github.io/unjournaldata/chapters/evaluation_data_analysis.html#basic-presentation "null")[^1]*
*

+-------------------+-------------------+---+
|                   | **Rating**        | * |
|                   |                   | * |
|                   |                   | 9 |
|                   |                   | 0 |
|                   |                   | % |
|                   |                   | C |
|                   |                   | r |
|                   |                   | e |
|                   |                   | d |
|                   |                   | i |
|                   |                   | b |
|                   |                   | l |
|                   |                   | e |
|                   |                   | I |
|                   |                   | n |
|                   |                   | t |
|                   |                   | e |
|                   |                   | r |
|                   |                   | v |
|                   |                   | a |
|                   |                   | l |
|                   |                   | * |
|                   |                   | * |
+===================+===================+===+
| **Overall         | 75/100            | 6 |
| assessment **     |                   | 0 |
|                   |                   |   |
|                   |                   | - |
|                   |                   | 8 |
|                   |                   | 0 |
+-------------------+-------------------+---+
| **Journal rank    | 4.0/5             | 2 |
| tier, normative   |                   | . |
| rating**          |                   | 5 |
|                   |                   |   |
|                   |                   | - |
|                   |                   | 4 |
|                   |                   | . |
|                   |                   | 5 |
+-------------------+-------------------+---+

**Overall assessment **(See footnote[^2])

**Journal rank tier, normative rating (0-5): ** On a 'scale of
journals', what 'quality of journal' should this be published in?[^3]
*Note: 0= lowest/none, 5= highest/best. *

# Claim identification and assessment

## I. Identify the most important and impactful factual claim this research makes[^4] {#i-identify-the-most-important-and-impactful-factual-claim-this-research-makes}

Reviewers have a preference for grants with increased variance in review
scores, and that there is an asymmetry in preference for high-variance
applications in response to funding shocks in the positive vs. negative
direction.

## II. To what extent do you \*believe\* the claim you stated above?[^5] {#ii-to-what-extent-do-you-believe-the-claim-you-stated-above}

I am persuaded unless I see contrary evidence. I have seen primary
internal NIH data, and this comports with that experience.

## III. Suggested robustness checks[^6] {#iii-suggested-robustness-checks}

Inter-rater reliability information, if available.

## IV. Important 'implication', policy, credibility[^7] {#iv-important-implication-policy-credibility}

Program Officers should be provided with summary statistics about review
variance. They do not have time to calculate them themselves, and it
could be easily incorporated into existing internal data systems
(iSearch).

# Written report

"Choose Your Moments: NIH Peer Review and Scientific Risk Taking" is an
interesting study on review decision-making. Carson, Zivin, and Shrader
examine the effect of NIH peer review score distribution on up-or-down
decision-making about funding particular grants, using a grant portfolio
from NIH 2016 RePORTER data, using a balanced incomplete block design
approach. Their approach is statistically sound and I am persuaded by
their final conclusion that not only do reviewers have a preference for
grants with increased variance in review scores, but also that there is
an asymmetry in preference for high-variance applications in response to
funding shocks in the positive vs. negative direction.

To begin with, I believe the balanced incomplete block design approach
is well-suited to the data structures that are actually present in the
internal NIH data systems. As noted by the authors, there are typically
three primary reviewers for a grant application, followed by a vote by
the entire panel. However, this pattern is not always adhered to. I have
seen as few as two primary review scores for particular grant
applications and as many as five. This is an appropriate approach for
analyzing such data. Furthermore, the analysis includes fixed effects
for reviewer behavior. Controlling for individual propensities is good
practice.

Additionally, there are some sections that included discussion about
future directions that were good, but did not go far enough. With
respect to the part noting that internal NIH data are not available to
researchers in the Extramural community, there is a proposal from the
Senate Committee on [Health, Education, Labor, and
Pensions](https://www.help.senate.gov/imo/media/doc/nih_modernization_5924pdf.pdf "null")
(p. 5-6)[@ns3ast91ftv] to pilot studies sharing such data with trusted
extramural researchers. Because this is already under consideration,
this point could be emphasized. Likewise, in the section discussing
linking review scores to research outputs, NIH has a suite of
bibliometric tools that include not only publication counts and citation
influence, but also bench-to-bedside translation of basic research into
clinical studies with their iCite web platform, including predictive
analytics of such clinical translation. These are in use internally for
policymaking, but could also be leveraged by the Extramural community at
scale.

There are two issues that I believe should be addressed in a revised
manuscript. First, the assertion that grants can be resubmitted twice is
out of date. The current policy (last I checked as of 2024) is that one
formal resubmission is allowed (known as an A1). After that, applicants
can revise and submit again, but their application will not thereafter
be linked to their previous submissions or the reviews they received,
and may be sent to a different study section.

Second, the asterisks denoting a p \< 0.10 in the tables will definitely
reduce confidence by biomedical audiences, including at NIH, as this is
strongly discouraged in those communities. I recommend relabeling the
asterisks and changing the prose to indicate that these results were not
significant.

I also have recommendations about interpretability, especially with
regard to NIH employees or policymakers who might be a target audience.

First, studies like this one often fail to gain traction inside
government because of differences with the review process compared to
those used in government. In particular, there seems to be no discussion
among participants as there is in review panels. I understand why this
design choice was made. However, this should be more clearly explained,
focusing on the strength this lends to the statistical analysis.

Likewise, randomization of peer review scores is a double-edged sword.
Again, I understand the design choice. However, the strong effect size
of average review score on randomized data can be interpreted as
reviewers blindly following scores they are given. This interpretation
will be particularly offensive to the Center for Scientific Review.
Adding language to ameliorate this will be helpful. If inter-rater
reviewer data is available, publishing this would be particularly
helpful.

Finally, more attention should be given to the fact that this is not a
representative data sample. The data in RePORTER includes only
application materials that were deemed to be in the top 10-20%, typical
paylines for NIH ICs as the authors noted. There are some real clunkers
in the triaged applications. I once saw a triaged application that was
formatted as a poem. More caveats should be applied here.

## References

[@n2wkwu0g0fi] Cassidy, Bill. 2024. 'NIH in the 21st Century: Ensuring
Transparency and American Biomedical Leadership'. Senate Committee on
Health, Education, Labor & Pensions.
<https://www.help.senate.gov/imo/media/doc/nih_modernization_5924pdf.pdf>.

[@ncd0jqljvg7] Carson, Richard T., Joshua Graff Zivin, and Jeffrey
Shrader. \"Choose Your Moments: NIH Peer Review and Scientific Risk
Taking.\" *Available at SSRN 5195216* (2024).

# Evaluator details

1.  How long have you been in this field?

    -   12 years.

[^1]: Note: if you are reading this before, or soon after this has been
    publicly released, the ratings from this paper may not yet have been
    incorporated into that data presentation.

[^2]: We asked them to rank this paper "heuristically" as a percentile
    "relative to all serious research in the same area that you have
    encountered in the last three years." We requested they "consider
    all aspects of quality, credibility, importance to knowledge
    production, and importance to practice.

[^3]: See ranking tiers discussed
    [here](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#journal-ranking-tiers).

[^4]: The evaluator was given the following instructions: Identify the
    most important and impactful factual claim this research makes --
    e.g., a binary claim or a point estimate or prediction.

    Please state the authors' claim precisely and quantitatively.
    Identify the source of the claim (i.e., cite the paper), and briefly
    mention the evidence underlying this. We encourage you to explain
    why you believe this claim is important, either here, or in the text
    of your report.

[^5]:

[^6]: *We asked:*

    \[Optional\] What additional information, evidence, replication, or
    robustness check would make you substantially more (or less)
    confident in this claim?

    Feel free to refer to the main body of your evaluation here; you
    don\'t need to repeat yourself. Please specify how you would perform
    this robustness check (etc.) as precisely as you are willing. E.g.,
    if you suggest a particular estimation command in a statistical
    package, this could be very helpful for future robustness
    replication work.

[^7]: *We asked:* \[Optional\] Identify the important \*implication\* of
    the above claim for funding and policy choices? To what extent do
    you \*believe\* this implication? How should it inform policy
    choices? Note: this 'implication' could be suggested by the
    evaluation manager in some cases. As an example of an
    \'implication\' \... in a global health context, the \'main claim\'
    might suggest that a vitamin supplement intervention, if scaled up,
    would save lives at a \$XXXX per life saved.
