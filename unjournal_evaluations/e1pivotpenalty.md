---
article:
  elocation-id: e1pivotpenalty
author:
- Evaluator 1
bibliography: /tmp/tmp-59Y7lRDS2JC9Fc.json
copyright:
  link: "https://creativecommons.org/licenses/by/4.0/"
  text: Creative Commons Attribution 4.0 International License
  type: CC-BY
csl: /app/dist/server/server/utils/citations/citeStyles/apa-6th-edition.csl
date:
  day: 21
  month: 05
  year: 2025
journal:
  publisher-name: The Unjournal
  title: The Unjournal
link-citations: true
title: Evaluation 1 of "Adaptability and the Pivot Penalty in Science
  and Technology"
uri: "https://unjournal.pubpub.org/pub/e1pivotpenalty"
---

# Abstract 

This paper presents a novel framework for measuring the degree of
departure from prior research in new works, documenting the \"pivot
penalty,\" where the impact of research declines as scientists venture
further from their past focus. The strengths include its broad
applicability, strong data, and policy implications. Concerns include
under-explored benefits of pivoting, alternative explanation\[s\] such
as \[the\] timing of rewards, evolving journal preferences, and the
choice of journals over fields for analysis.

# Summary Measures

We asked evaluators to give some overall assessments, in addition to
ratings across a range of criteria. *See the *[*evaluation summary
"metrics"*](https://unjournal.pubpub.org/pub/evalsumpivotpenalty#metrics "null")*
for a more detailed breakdown of this. See these ratings in the context
of all Unjournal ratings, with some analysis, in our *[*data
presentation
here.*](https://unjournal.github.io/unjournaldata/chapters/evaluation_data_analysis.html#basic-presentation "null")[^1]*
*

+-------------------+-------------------+---+
|                   | **Rating**        | * |
|                   |                   | * |
|                   |                   | 9 |
|                   |                   | 0 |
|                   |                   | % |
|                   |                   | C |
|                   |                   | r |
|                   |                   | e |
|                   |                   | d |
|                   |                   | i |
|                   |                   | b |
|                   |                   | l |
|                   |                   | e |
|                   |                   | I |
|                   |                   | n |
|                   |                   | t |
|                   |                   | e |
|                   |                   | r |
|                   |                   | v |
|                   |                   | a |
|                   |                   | l |
|                   |                   | * |
|                   |                   | * |
+===================+===================+===+
| **Overall         | 80/100            | 7 |
| assessment **     |                   | 0 |
|                   |                   |   |
|                   |                   | - |
|                   |                   | 9 |
|                   |                   | 0 |
+-------------------+-------------------+---+
| **Journal rank    | 4.5/5             | 4 |
| tier, normative   |                   | . |
| rating**          |                   | 0 |
|                   |                   |   |
|                   |                   | - |
|                   |                   | 5 |
|                   |                   | . |
|                   |                   | 0 |
+-------------------+-------------------+---+

**Overall assessment **(See footnote[^2])

**Journal rank tier, normative rating (0-5): ** On a 'scale of
journals', what 'quality of journal' should this be published in?[^3]
*Note: 0= lowest/none, 5= highest/best. *

# Claim identification and assessment

## I. Identify the most important and impactful factual claim this research makes[^4] {#i-identify-the-most-important-and-impactful-factual-claim-this-research-makes}

"Pivot penalty," where the impact of new research steeply declines the
further a researcher moves from their prior work.

## II. To what extent do you \*believe\* the claim you stated above?[^5] {#ii-to-what-extent-do-you-believe-the-claim-you-stated-above}

As I mentioned in the evaluation, I feel the claim is still too strong,
even with some empirical support. There should be something happening
but \[it seems to be\] overlooked in the measurement framework.

## III. Suggested robustness checks[^6] {#iii-suggested-robustness-checks}

It might be helpful to see the promotion/funding these pivoting
scientists \[received\].

# Written report

> *Management note: *We shared this report and the original paper with
> ChatGPT o1, and asked it to consider "1. Any
> mistakes/misunderstandings in the evaluation? 2. Any suggestions in
> the evaluation that seem to have been addressed by the February update
> (relative to August?". We added (some of) these as footnotes in the
> evaluation below, giving the evaluator a chance to respond; they
> responded in a few places, also noted below. (See llm conversation
> [here](https://chatgpt.com/share/67db35be-85d0-8002-abad-aa7862117a25).)

This paper introduces a measurement framework to quantify how far
researchers move from their existing research when producing new works.
This paper applies this framework to scientific publications and patents
and documents a phenomenon called the "pivot penalty," which is the
impact of new research steeply declining the further a researcher moves
from their prior work. This finding holds across different researchers,
fields, and measurements of research impact.

## Strengths of this paper

**Important question and valid question:**

Scientists keep changing research topics and directions, yet the
consequences of these changes remain unclear. Remote search theory
predicts the benefits of pivoting, whereas the burden of knowledge
theory and the current design of the funding/review process seem\[s\] to
encourage local search.[^7] This paper addresses this theoretical puzzle
in a timely manner.

**Strong policy implication:**

This paper also has practical implications. Should the government and
universities encourage researchers to address important but remote
questions? Should scientists pursue the hotspots of the scientific
landscape and adapt to demands driven by funding or other reasons? This
paper could serve as a starting point for policymakers to consider these
questions.

**Flexible and replicable measurement framework:**

The measurement framework is reasonable and relatively easy to
understand. In general, it compares the similarity between the focal
work and the scientist\'s prior work in terms of citing journals or CPC
(Cooperative Patent Classification) classes. This approach can be
broadly applied to other fields, for example, to examine whether an
organization, rather than an individual scientist, pivots to a new
scientific field or similar areas.

**Excellent data and clever empirics:**

This paper uses comprehensive publication data and patents data to
justify the measurement framework. The large-scale data enables the
validity of findings across different settings and fields.

## Some concerns and thoughts:

1.  **There must be something else happening:**[^8]

It is surprising that the pivot penalty exists for all scientists, and I
find it difficult to understand what is happening here. In other words,
if everyone is penalized for pivoting their research toward unfamiliar
fields, why do scientists continue to pivot and combine remote concepts?
Perhaps it is simply because the measurements do not fully capture the
value of pivoting work. Pivoting may result in more funding, better
opportunities for their students, and other benefits. This might be
beyond the scope of this paper, but my concern is that the paper
emphasizes the costs (i.e., negative consequences) of pivoting while
overlooking some potential benefits of such changes.[^9]

*\[Evaluator followup\]* I think this paper characterizes just one
consequence of the pivoting process, yet it might overlook the
decision-making process behind it.

2.  **Missing Katalin Karikó -- a huge loss:**

Continuing from the previous point, another potential explanation is the
norm in academia, which does not reward remote search. A recent and
prominent example of the "pivot penalty" might be Katalin Karikó, whose
mRNA research was too novel to be appreciated by academia. How many
\"Katalin Karikós\" are missing in academia? What are the welfare
implications of this misallocation? It might be worthwhile for the
authors and the audience of this paper to think about it.

3.  **Penalty or long time waiting for rewards?**

The Covid case study is a nice one, but the authors focus on the penalty
to scientists who are pushed into the new fields. Returning to Katalin
Karikó\'s example, could this demand factor reward early entrants to
these fields? Is the pivot penalty simply a timing issue? Would it be
possible to show how the penalty changes over time, even if only for the
COVID or mRNA case?[^10]

*\[Evaluator follow-up\]* By saying "timing", I mean when to pivot. I
understand that this paper characterizes the average penalty, but I
would like to take it as another heterogeneity test.

4.  **Journals are also evolving:**

This might be a minor issue, but the implicit assumption of this paper
is that journals have a constant preference for publishing certain types
of research.[^11] While this might be true, another possibility is that
journals also pivot over time. It would be helpful to examine how
aggregate journal-level citations change and use this to adjust the
current measurement.

**Why journals not fields?**

I am not sure why the authors do not use fields to calculate pivoting
for scientists. Perhaps I missed something, but I suggest they use
fields for this analysis, especially since they have already done so for
inventors.[^12]

*\[Evaluator follow-up\]:* What I am not sure about here is why the
authors do not use fields in the main analysis for scientists,
especially considering they have done so for inventors.

# Evaluator details

1.  How long have you been in this field?

    -   3 years.

2.  How many proposals and papers have you evaluated?

    -   6\.

[^1]: Note: if you are reading this before, or soon after this has been
    publicly released, the ratings from this paper may not yet have been
    incorporated into that data presentation.

[^2]: We asked them to rank this paper "heuristically" as a percentile
    "relative to all serious research in the same area that you have
    encountered in the last three years." We requested they "consider
    all aspects of quality, credibility, importance to knowledge
    production, and importance to practice.

[^3]: See ranking tiers discussed
    [here](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#journal-ranking-tiers).

[^4]: The evaluator was given the following instructions: Identify the
    most important and impactful factual claim this research makes --
    e.g., a binary claim or a point estimate or prediction.

    Please state the authors' claim precisely and quantitatively.
    Identify the source of the claim (i.e., cite the paper), and briefly
    mention the evidence underlying this. We encourage you to explain
    why you believe this claim is important, either here, or in the text
    of your report.

[^5]: We asked II. To what extent do you \*believe\* the claim you
    stated above?

    Feel free to express this either a. in terms of the probability of
    the claim being true, b. as a credible interval for the parameter
    being estimated, or c. however you feel comfortable.

[^6]: *We asked:*

    \[Optional\] What additional information, evidence, replication, or
    robustness check would make you substantially more (or less)
    confident in this claim?

    Feel free to refer to the main body of your evaluation here; you
    don\'t need to repeat yourself. Please specify how you would perform
    this robustness check (etc.) as precisely as you are willing. E.g.,
    if you suggest a particular estimation command in a statistical
    package, this could be very helpful for future robustness
    replication work.

[^7]: Manager's note: The paper itself did not mention 'remote search
    theory'. ChatGPT o1 offers a discussion of the link to remote search
    theory
    [here](https://chatgpt.com/share/67db09ee-f4fc-8002-8cf5-606e6c761c89).
    I (David Reinstein) have a hard time understanding the relevance of
    the \'jumps\' involved in remote search theory to this context.

[^8]: Manager: numbering added

[^9]: o1: *"Treating the pivot penalty as purely a short‐run "negative
    consequence," missing that the paper does allow for intangible or
    longer‐horizon benefits.*

    The \[evaluator\] remarks that pivoting might yield other rewards
    (e.g., fresh funding, new collaborations, or intangible career
    benefits) not captured by near‐term citations. The paper does not
    deny that possibility; it simply finds that---on short‐ or
    medium‐horizon citation and publication metrics---big pivots do
    worse. This is less a "mistake" than a misunderstanding of the
    scope: the paper confines itself to demonstrable
    publication/citation/market‐value outcomes rather than intangible or
    multi‐decade payoffs."

[^10]: o1: *"Penalty or long time waiting?*

    > The \[evaluator\] suggests the pivot penalty might vanish if one
    > waits long enough (citing the example of Katalin Karikó). While
    > that is a valid hypothesis, the original text already acknowledges
    > that big breakthroughs can come from big pivots---and that the
    > data show such "outsider" advantages to be rare but not
    > impossible. The \[evaluator\] remarks thus partly misread the
    > scope: the paper documents the average empirical penalty, not
    > claiming absolutely no one ever eventually succeeds with an
    > extreme pivot."

    *Manager*: I'm not confident that this LLM response gets at the
    evaluator's critique here; they clarify it below.

[^11]: o1: *"Implicit assumption that the paper treats journal
    preferences as constant over time."*

    > The \[evaluator\] worries that "journals are also evolving,"
    > implying the paper's approach might ignore the evolution of
    > journal norms. In reality, the authors explicitly track five
    > decades of data, show how the pivot penalty steepens over time,
    > and use field/year normalization for citation‐impact measures.
    > Hence, it is not simply assuming static journal preferences."

    Manager: I'm not confident that this response gets at the
    evaluator's critique here, but they did not clarify it.

[^12]: ChatGPT o1 disputes the *"Claim that the authors never measure
    pivoting by field (for scientists)."\
    "*The reviewer suggests the paper should "use fields for this
    analysis, especially since they \[already\] do so for inventors" but
    implies it was not done for scientists. In fact, even the August
    version already included (in the SI) robustness checks measuring
    pivot distance by field‐level codes rather than journals. So, the
    assertion that fields were never used is mistaken."
