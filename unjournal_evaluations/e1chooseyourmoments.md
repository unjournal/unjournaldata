---
affiliation:
- id: 0
  organization: PhD Candidate at the University of Michigan School of
    Information
article:
  doi: 10.21428/d28e8e57.df86328a/19138d0f
  elocation-id: e1chooseyourmoments
author:
- James M. Zumel Dumlao
bibliography: /tmp/tmp-60NYXonm8fqR30.json
copyright:
  link: "https://creativecommons.org/licenses/by/4.0/"
  text: Creative Commons Attribution 4.0 International License
  type: CC-BY
csl: /app/dist/server/server/utils/citations/citeStyles/apa-6th-edition.csl
date:
  day: 01
  month: 06
  year: 2025
journal:
  publisher-name: The Unjournal
  title: The Unjournal
link-citations: true
title: "Evaluation 1 of \"Choose Your Moments: NIH Peer Review and
  Scientific Risk Taking\""
uri: "https://unjournal.pubpub.org/pub/e1chooseyourmoments"
---

# Abstract 

This paper includes two experiments aimed at informing reform in the NIH
grant peer review process. The first experiment finds that biomedical
scientists have a preference for studies with higher score variance
("dissensus"), interpreted as a desire for risk-taking. The second
experiment finds that tightening budgets reduces scientists' dissensus
tolerance. Authors conclude that relaxed budgets and rewarding dissensus
would encourage the production of more high-impact science.

# Summary Measures

We asked evaluators to give some overall assessments, in addition to
ratings across a range of criteria. *See the *[*evaluation summary
"metrics"*](https://unjournal.pubpub.org/pub/evalsumchooseyourmoments#metrics "null")*
for a more detailed breakdown of this. See these ratings in the context
of all Unjournal ratings, with some analysis, in our *[*data
presentation
here.*](https://unjournal.github.io/unjournaldata/chapters/evaluation_data_analysis.html#basic-presentation "null")[^1]*
*

+-------------------+-------------------+---+
|                   | **Rating**        | * |
|                   |                   | * |
|                   |                   | 9 |
|                   |                   | 0 |
|                   |                   | % |
|                   |                   | C |
|                   |                   | r |
|                   |                   | e |
|                   |                   | d |
|                   |                   | i |
|                   |                   | b |
|                   |                   | l |
|                   |                   | e |
|                   |                   | I |
|                   |                   | n |
|                   |                   | t |
|                   |                   | e |
|                   |                   | r |
|                   |                   | v |
|                   |                   | a |
|                   |                   | l |
|                   |                   | * |
|                   |                   | * |
+===================+===================+===+
| **Overall         | 70/100            |   |
| assessment **     |                   | 5 |
|                   |                   | 0 |
|                   |                   | - |
|                   |                   | 8 |
|                   |                   | 0 |
+-------------------+-------------------+---+
| **Journal rank    | 3.8/5             | 3 |
| tier, normative   |                   | . |
| rating**          |                   | 0 |
|                   |                   |   |
|                   |                   | - |
|                   |                   | 4 |
|                   |                   | . |
|                   |                   | 1 |
+-------------------+-------------------+---+

**Overall assessment **(See footnote[^2])

**Journal rank tier, normative rating (0-5): ** On a 'scale of
journals', what 'quality of journal' should this be published in?[^3]
*Note: 0= lowest/none, 5= highest/best. *

# Claim identification and assessment

## I. Identify the most important and impactful factual claim this research makes[^4] {#i-identify-the-most-important-and-impactful-factual-claim-this-research-makes}

Authors claim that biomedical scientists, who are the experts relied on
in NIH peer review, have a preference for "dissensus" among reviewers'
scores (i.e., higher variance). They also claim that a tighter budget
will decrease the dissensus tolerance among scientists. Both claims are
well substantiated in the experimental findings shown in Tables 2 and 5.
It should be noted that the effect of the overall average score is 2-9
times larger than that of score variance on participants' decision of
which studies to fund.

They also claim decision makers at science funding institutions would
benefit from taking the variance of peer reviewer scores into account,
rather than considering the average alone. It is argued that higher
score variance is correlated with higher risk research with greater
potential to have positive impacts for the public. This claim is crucial
to connecting the findings of the experiments to policy recommendations,
however it lacks direct evidence from the studies conducted, "Our study
evaluates the value researchers place on alternative peer review
aggregation methods taking this conjecture \[that greater dissensus in
project scores can identify more radical projects\] as given." (p. 3).
Results in Table A8 show that the risk-averse subgroup has a stronger
preference for score variance than average, undermining the assumption
that the variance preference is a risk preference.

## II. To what extent do you \*believe\* the claim you stated above?[^5] {#ii-to-what-extent-do-you-believe-the-claim-you-stated-above}

I believe the first claim is 95% true, that scientists prefer higher
dissensus work controlling for quality. The second claim is much less
tenable to me, maybe 65% true, that higher variance scores means riskier
research that would translate into greater economic growth or something
else important to the public interest.

# Written report

This paper documents experiments aimed at testing whether scientists
prefer to fund studies with a higher variance in scores (dissensus). The
authors' discrete choice survey randomly varies the average and variance
of reviewers' scores presented to biomedical scientist participants and
carefully controls for confounders like manuscript title and abstract
(and thereby quality) and between-participant characteristics. They find
that scientists preferred to fund submissions with both a higher average
and variance of scores. Authors conclude that the current NIH policy of
primarily relying on reviewers' average score does not capture their
additional preference for higher score variance. It is assumed that
increased risk tolerance (operationalized as funding higher variance
proposals) would increase the likelihood of breakthrough work, although
this has not been formally tested.

Several other hypotheses regarding scientists' dissensus tolerance are
tested experimentally. This paper presents evidence that scientists
weigh positive and negative scores similarly, do not prefer dissensus in
the form of a bimodal distribution (signalling polarization among
reviewers), and \[for an\] increase in dissensus tolerance with domain
expertise (operationalized as MeSH term matching between participants'
and evaluated work). In another experiment, participants are randomly
assigned a budget to consider when funding submissions. Participants
with tighter budgets preferred to drop projects with a higher score
variance (and average), confirming concerns that funding cuts would
increase conservativeness in project funding. Lastly, findings from
prior research are used to calculate how the portfolio of funded
proposals would shift if reviewers' preference for higher variance were
accounted for in NIH decision-making, showing a five to ten percent
replacement of higher variance for lower variance proposals.

## General Evaluation

This paper has many merits. It advances our understanding of peer
reviewer preferences in a setting responsible for billions of dollars of
biomedical research funding and aims to inform and improve grant peer
review processes. The studies have a high internal validity thanks to
their experimental methodology and evidence is provided that findings
are robust. However, several theoretical links are underdeveloped
precluding a conclusive interpretation that justifies concrete policy
recommendations. Authors maintain a good level of intellectual modesty,
acknowledging that there are political and data roadblocks to ideal
studies on this topic. Still, alongside humble recommendations like the
use of variance to break ties in average score, some conclusions
overclaim what is gleaned from the results. A major example is that
score variance is assumed to proxy study risk, which is not
substantially motivated or demonstrated. Regarding openness and
replicability, the studies are described in enough detail to replicate
findings, but the experimental data does not appear to be shared
publicly. Overall, the paper presents clear findings from well-designed
experiments, but the framing of these findings could be improved.

## Detailed Feedback and Thoughts on Next Steps

### Sample Size and Power 

The studies in this paper are very well-powered with over 3,000
participants each, distributed into roughly 250-300 treatment clusters.
The primary concern with sample size is whether a study has enough power
to detect an effect if it is present. Effects measured in these studies
are consistently statistically significant, so power is not a concern.

### Construct and Ecological Validity

It is not clearly theorized how project feasibility, risk, potential, or
other characteristics might be encoded in a higher score variance. It is
possible that in real settings, higher variance may have multiple causes
(e.g., vague details) other than risk and reward. Some theoretical
motivation for why scientists' preference for dissensus would improve
published science would be helpful here, either using prior literature
or a formal model relating novelty to dissensus.

Two major threats that could be addressed are proposal and team
characteristics. While the experimental setting controls for proposal
quality, it is unknown whether dissensus is correlated with higher or
lower feasibility or higher or lower novelty research in practice. A
follow-up study could disambiguate the randomized scores into categories
like feasibility, novelty, etc., and test what kinds of disagreement is
preferred by scientists.

Research team identity is one of the five criteria for scoring in the
NIH process, but this study presents participants with anonymized
proposals. Higher variance may be partially caused by differences in
evaluators' biases or prior knowledge of research team members. Prior
research has found that underrepresented groups produce more novel
research, but receive less encouragement in the form of publication
likelihood and citations (Hofstra et al. 2020)[@n7yrujbe1nm]. It could
also be that well-established labs are more well-known and trusted by
evaluators, engendering less consensus than a newer team with little
record of prior success. A follow-up study could provide some basic team
information like ethnic or gender-coded names for corresponding author
or some vignette about the research teams prior record.

### Generalizability

This study focuses on the NIH R01 grant peer review process, which is
one of the most significant funding sources in biomedical research. The
background section explains that the NIH decision-making process is
quite particular. Furthermore, it is not discussed how the NIH process
compares and contrasts with those in other funding institutions. The
results in this setting are interesting and impactful in themselves, but
authors may want to be more explicit if their findings should be
interpreted as a general to all scientists (across fields) in all grant
peer review settings. I do acknowledge that the title (nice pun by the
way) implies findings should be interpreted mainly in the NIH context.

### Applicability/Policy Implications

While it is noted that NIH staff mostly make funding decisions
formulaically based on proposals' percentile of average scores, it is
not discussed why this is the case, which may be crucial to the argument
connecting the findings in this study to implications for policy.
Likely, the separation of the 3 stages of the NIH grant process is in
part due to a recognition of the varying interests of science and the
public. NIH staff's reliance on average scores may be political, rather
than historical or arbitrary.

First, it is not clear that proposals with greater dissensus are
actually riskier, "Our study evaluates the value researchers place on
alternative peer review aggregation methods taking this conjecture
\[that greater dissensus in project scores can identify more radical
projects\] as given." (p. 3). Results in Table A8 show that the
risk-averse subgroup has a stronger preference for score variance than
average, undermining the assumption that the variance preference is a
risk preference. Second, the reasons for scientists' dissensus
preference are explicitly studied. Last, the connection between funding
riskier research, which likely has a higher fail rate, and expected
outcomes relevant to the public like economic growth is undermotivated.
Scientists may prefer higher dissensus proposals due to their
*scientific* potential, rather than their potential for positively
impacting the public (Sarewitz 1996)[@nn7xfzvhcbw].

Thus, a political reason for only using average scores is to balance
expert reviews with *public* tolerance for risk. One interpretation of
your findings is that they confirm this interest divergence, which the
authors hint at: "The results show that our samples of experienced
biomedical scientists, on average, do not share the same objective
function as the NIH" (p. 4).

## References

[@n0h20qltb9x] Hofstra, Bas, Vivek V. Kulkarni, Sebastian Munoz-Najar
Galvez, Bryan He, Dan Jurafsky, and Daniel A. McFarland. 2020. "The
Diversity--Innovation Paradox in Science." *Proceedings of the National
Academy of Sciences* 117(17):9284--91. doi: 10.1073/pnas.1915378117.

[@n9c6isg81vw] Sarewitz, Daniel. 1996. "Ch. 2: The Myth of Infinite
Benefit." in *Frontiers of Illusion: Science, Technology, and the
Politics of Progress*. Rutgers University Press.

[@ngesv1lbrsi] Carson, Richard T., Joshua Graff Zivin, and Jeffrey
Shrader. \"Choose Your Moments: NIH Peer Review and Scientific Risk
Taking.\" *Available at SSRN 5195216* (2024).

# Evaluator details

1.  How long have you been in this field?

    -   3 years

2.  How many proposals and papers have you evaluated?

    -   3

[^1]: Note: if you are reading this before, or soon after this has been
    publicly released, the ratings from this paper may not yet have been
    incorporated into that data presentation.

[^2]: We asked them to rank this paper "heuristically" as a percentile
    "relative to all serious research in the same area that you have
    encountered in the last three years." We requested they "consider
    all aspects of quality, credibility, importance to knowledge
    production, and importance to practice.

[^3]: See ranking tiers discussed
    [here](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#journal-ranking-tiers).

[^4]: The evaluator was given the following instructions: Identify the
    most important and impactful factual claim this research makes --
    e.g., a binary claim or a point estimate or prediction.

    Please state the authors' claim precisely and quantitatively.
    Identify the source of the claim (i.e., cite the paper), and briefly
    mention the evidence underlying this. We encourage you to explain
    why you believe this claim is important, either here, or in the text
    of your report.

[^5]:
