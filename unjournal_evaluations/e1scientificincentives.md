---
article:
  doi: 10.21428/d28e8e57.36ba6211/4e91c4a0
  elocation-id: e1scientificincentives
author:
- Daniel Lee
bibliography: /tmp/tmp-60PKCOYHMj6AP6.json
copyright:
  link: "https://creativecommons.org/licenses/by/4.0/"
  text: Creative Commons Attribution 4.0 International License
  type: CC-BY
csl: /app/dist/server/server/utils/citations/citeStyles/apa-6th-edition.csl
date:
  day: 16
  month: 04
  year: 2025
journal:
  publisher-name: The Unjournal
  title: The Unjournal
link-citations: true
title: Evaluation 1 of "Stagnation and Scientific Incentives"
uri: "https://unjournal.pubpub.org/pub/e1scientificincentives"
---

# Abstract 

I think this paper raises an excellent point, but lacks sufficient
support of its claims. I think the most important aspect of this paper
is its overall ethos to do right by science, but struggles in terms of
accuracy, novelty, and generalizability---all of which are discussed
further in my report below.

# Summary Measures

We asked evaluators to give some overall assessments, in addition to
ratings across a range of criteria. *See the *[*evaluation summary
"metrics"*](https://unjournal.pubpub.org/pub/evalsumscientificincentives#metrics "null")*
for a more detailed breakdown of this. See these ratings in the context
of all Unjournal ratings, with some analysis, in our *[*data
presentation
here.*](https://unjournal.github.io/unjournaldata/chapters/evaluation_data_analysis.html#basic-presentation "null")[^1]*
*

+-------------------+-------------------+---+
|                   | **Rating**        | * |
|                   |                   | * |
|                   |                   | 9 |
|                   |                   | 0 |
|                   |                   | % |
|                   |                   | C |
|                   |                   | r |
|                   |                   | e |
|                   |                   | d |
|                   |                   | i |
|                   |                   | b |
|                   |                   | l |
|                   |                   | e |
|                   |                   | I |
|                   |                   | n |
|                   |                   | t |
|                   |                   | e |
|                   |                   | r |
|                   |                   | v |
|                   |                   | a |
|                   |                   | l |
|                   |                   | * |
|                   |                   | * |
+===================+===================+===+
| **Overall         | 70/100            | 5 |
| assessment **     |                   | 5 |
|                   |                   |   |
|                   |                   | - |
|                   |                   | 8 |
|                   |                   | 5 |
+-------------------+-------------------+---+
| **Journal rank    | 3.0/5             | 2 |
| tier, normative   |                   | . |
| rating**          |                   | 7 |
|                   |                   |   |
|                   |                   | - |
|                   |                   | 3 |
|                   |                   | . |
|                   |                   | 7 |
+-------------------+-------------------+---+

**Overall assessment **(See footnote[^2])

**Journal rank tier, normative rating (0-5): ** On a 'scale of
journals', what 'quality of journal' should this be published in?[^3]
*Note: 0= lowest/none, 5= highest/best. *

# Claim identification and assessment

## I. Identify the most important and impactful factual claim this research makes[^4] {#i-identify-the-most-important-and-impactful-factual-claim-this-research-makes}

The stagnation in science is correlated with the rise of bibliometrics
and other citation measurements.

## II. To what extent do you \*believe\* the claim you stated above?[^5] {#ii-to-what-extent-do-you-believe-the-claim-you-stated-above}

I believe this is a credible claim, but requires more to be a believable
claim (see below).

## III. Suggested robustness checks[^6] {#iii-suggested-robustness-checks}

1a) a discussion of the difference between citation indices and journal
lists and

1b) a discussion of how they impact scientist evaluations

2\) a comparison to non-scientific fields

3\) any sort of regression model or statistical test

4\) formal definitions of the terms at hand (e.g. scientific play,
exploration, breakthrough, giant, etc.)

# Written report

To start, I am quite glad that such a manuscript exists, because it
echoes several of my own concerns about incentives in science. To that
end, forgive me in advance as I'm afraid the process of reviewing has
given me a bit of a soapbox ;-).

In this paper, the authors develop a model of scientific progress and
discuss the slowdown in scientific progress. In linking that slowdown to
the increase in the use of bibliometrics to evaluate scientists, they
further provide insights for how to expand scientific evaluation in a
way that rewards exploration, play, and novelty.

I think there is a lot to admire about this paper. Notably, I appreciate
the deep sense of *responsibility* the authors feel for the state of
science, and from that, the drive to make it better, both for the
producers of scientific knowledge as well as the general public as
recipients of scientific benefit. Notably, I believe the drive to
broaden productivity measurements and incentive exploration will also
help democratize science, particularly for scientists who are less
well-resourced or working in teaching-focused institutions. Further, I
thought their discussion around figure 2 was inspired and showed a great
deal of foresight in thinking about how others approach research. It
adds a much-needed sense of nuance to the model of progress.

However, I had several concerns in reviewing the manuscript that would
make this contribution currently unfit for publication in a traditional
science-of-science or innovation journal like *Research Policy* or
*Technovation*. In particular, my concerns tended to fall into 3 closely
related main areas: accuracy, generalizability, and novelty.

## Accuracy Concerns

My biggest concern with the manuscript is, strictly speaking, I don't
have a sense of how "correct" it is. This is not to say it is incorrect,
but rather, many of the claims it makes (both trivial and significant)
feel asserted rather than proven or demonstrated. For example, even
after reading the manuscript, it is not clear to me that science is
indeed "fixated on citations". (I address this concern further below). I
think some of these concerns are underscored by the atheoretical nature
of the manuscript, which hinges on key terms that lack a formal
definition such as "scientific play." This can lead the manuscript to
feel tautological at times (e.g., it would not have meaning to say,
"this early work is exploratory because it is early work" ). While I
don't know of any extant references that formally define such terms, I
believe this presents an opportunity for the authors to write them,
which would broaden their impact and their thought leadership.

In fairness to the authors, part of this feeling may be their convention
of footnoting references rather than including in-text or parenthetical
citations, but I believe there is a deeper concern here as well. By way
of a simple example -- on p.6 the authors describe Science as "the most
highly cited scientific journal," but I believe this is untrue, with
Nature, NEJM, and possibly the Lancet surpassing Science in citation
counts. Overall, there is limited data to back up these claims, the
paper presents a conceptual model, but not a formal one, with few
testable implications and falsifiable hypotheses (cf. above). This is
especially concerning given *the Unjournal's* core focus on open,
communicable, and replicable science.

These accuracy concerns are most prominent in the overall story of the
paper. For instance, why the focus on citation counts and indices? I may
be na√Øve here, but in my (admittedly limited) experience, researchers
are (at least in the short term) more concerned with journal name than
citation count (Heckman and Moktan, 2020)[@nklw6gdevtq].

Now, to be clear, I think journal obsession still creates a stagnation
in science, but I want to note that this distinction is not merely
pedantic as it has important implications for your modeling (if the
focus is on top journals rather than high citations, do we still see
herding behavior at incremental science?) and your policy implications
(while Google Scholar or Web of Science can relatively easily calculate
and report alternative bibliometrics, changing T&P *\[tenure and
promotion\]* doctrine is not done at a collective level and will require
buy-in from multitudes of stakeholders). That is, it's not clear to me
that this advocacy will ultimately change any problematic scientific
incentives.

## Generalizability Concerns

Closely related to accuracy is generalizability---do these findings hold
true across all sciences? Are there any sciences that are more (or less)
susceptible to stagnation? Are there any mediators or moderators of
stagnation? For instance, do some fields attract more incremental or
risk averse scientists? How does interdisciplinarity come in to play?

If "citation obsession" is driving scientific stagnation, would we
expect to see similar effects in the arts and humanities? Why or why
not? (See e.g., Townsend, and Norman Bradburn, 2022)[@no8qh76s1ct].

## Novelty Concerns

Finally, the authors rightly note the fact that "scientists \[do\]
respond to incentives is well demonstrated." I was pleased to see work
by Pierre Azoulay, Brian Uzzi, and Ben Jones (among others), all of whom
have done amazing work, in the list of references. However, the
contribution of this manuscript was less clear to me, especially given
the concerns raised above. If this paper is intended to model scientific
progress, I would have appreciated a formalized model with comparative
statics and falsifiable hypotheses. If this paper is intended to be an
exploration of incentives, I would have appreciated a demonstration of
those incentives in practice. Finally, if this paper is intended to be a
demonstration of impact, I would have appreciated formal regression
modeling. Overall, in its current form, I feel the paper lacks an
ultimate punchline.

# Evaluator details

1.  How long have you been in this field?

    -   Depends on how you count it, but at least 10 years.

2.  How many proposals and papers have you evaluated?

    -   At least 30.

## References

[@nmg7hrcfmjh]Heckman, J. J., & Moktan, S. (2020). Publishing and
Promotion in Economics: The Tyranny of the Top Five. *Journal of
Economic Literature*, *58*(2), 419--470.
<https://doi.org/10.1257/jel.20191574>

[@nyxn2db5980]Townsend, R. B., & Bradburn, N. (2022). The State of the
Humanities circa 2022. *Daedalus*, *151*(3), 11--18.
<https://doi.org/10.1162/daed_a_01925>

[@10.3386/w26752] Bhattacharya, J., & Packalen, M. (2020). *Stagnation
and Scientific Incentives*. National Bureau of Economic Research.
<https://doi.org/10.3386/w26752>

[^1]: Note: if you are reading this before, or soon after this has been
    publicly released, the ratings from this paper may not yet have been
    incorporated into that data presentation.

[^2]: We asked them to rank this paper "heuristically" as a percentile
    "relative to all serious research in the same area that you have
    encountered in the last three years." We requested they "consider
    all aspects of quality, credibility, importance to knowledge
    production, and importance to practice.

[^3]: See ranking tiers discussed
    [here](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#journal-ranking-tiers).

[^4]: The evaluator was given the following instructions: Identify the
    most important and impactful factual claim this research makes --
    e.g., a binary claim or a point estimate or prediction.

    Please state the authors' claim precisely and quantitatively.
    Identify the source of the claim (i.e., cite the paper), and briefly
    mention the evidence underlying this. We encourage you to explain
    why you believe this claim is important, either here, or in the text
    of your report.

[^5]:

[^6]: *We asked:*

    \[Optional\] What additional information, evidence, replication, or
    robustness check would make you substantially more (or less)
    confident in this claim?

    Feel free to refer to the main body of your evaluation here; you
    don\'t need to repeat yourself. Please specify how you would perform
    this robustness check (etc.) as precisely as you are willing. E.g.,
    if you suggest a particular estimation command in a statistical
    package, this could be very helpful for future robustness
    replication work.
