---
affiliation:
- id: 0
  organization: University College London
- id: 1
  organization: The Unjournal
article:
  doi: 10.21428/d28e8e57.f1977256
  elocation-id: evalsumtemplateapplied
author:
- Evaluator 1
- Maximilian Maier
- David Reinstein
bibliography: /tmp/tmp-60osuCehkKO2eZ.json
copyright:
  link: "https://creativecommons.org/licenses/by/4.0/"
  text: Creative Commons Attribution 4.0 International License
  type: CC-BY
csl: /app/dist/server/server/utils/citations/citeStyles/apa-6th-edition.csl
date:
  day: 10
  month: 06
  year: 2024
journal:
  publisher-name: The Unjournal
  title: The Unjournal
link-citations: true
title: "Evaluation Summary and Metrics: \"Replicability &
  Generalisability: A Guide to CEA discounts\", Unjournal applied Stream"
uri: "https://unjournal.pubpub.org/pub/evalsumtemplateapplied"
---

# Abstract 

We organized two evaluations of the paper: \"[Replicability &
Generalisability: A Guide to CEA
discounts](https://docs.google.com/document/d/1eJBSmNG-iRJ-twoHaoztQUmB4pmEHut8_oT3xgtldnI/edit#heading=h.7om527x8q9t7 "null")\".
This paper was evaluated as part of our "applied and policy stream",
described
[here](https://docs.google.com/document/d/1RwkmGJtaOcryK-tJs3mrs6DIAj411YRnsauaj6EE6WY/edit#heading=h.o7xzpvh0h0b2 "null"),
and evaluators were asked to use the google doc template [provided
here](https://docs.google.com/document/d/1LRePcBxxnrNCnDW25yjHxmPVuf5hpFiqrZyGSHVxvxA/edit#heading=h.ysk4h740zxaf "null"),
specifically for this stream. To read these evaluations, please see the
links below. *COI considerations*: The author of this paper, Rosie
Bettle, is a member of The Unjournal's Management Team (discussed
further below).

## Note: Applied and Policy Stream

This paper was evaluated as part of our "applied and policy stream",
described
[here](https://docs.google.com/document/d/1RwkmGJtaOcryK-tJs3mrs6DIAj411YRnsauaj6EE6WY/edit#heading=h.o7xzpvh0h0b2 "null").
The ratings should not be directly compared to those in our main
academic stream.

## Evaluations

1\. [Anonymous
evaluator](https://unjournal.pubpub.org/pub/eval1ceadiscounts "null")

2\. [Max
Maier](https://unjournal.pubpub.org/pub/eval2ceadiscounts "null")

## Overall ratings

We asked evaluators to provide overall assessments as well as ratings
for a range of specific criteria. * *

### Note: Applied and Policy Stream {#note-applied-and-policy-stream}

This paper was evaluated as part of our "applied and policy stream",
described
[here](https://docs.google.com/document/d/1RwkmGJtaOcryK-tJs3mrs6DIAj411YRnsauaj6EE6WY/edit#heading=h.o7xzpvh0h0b2 "null").
The ratings should not be directly compared to those in our main
academic stream

**Overall assessment: **We asked them to rank this paper "heuristically"
as a percentile "relative to applied and policy research you have read
aiming at a similar audience, and with similar goals." We requested they
"consider all aspects of quality, credibility, importance to future
impactful applied research, and practical relevance and usefulness."

+---------------------+-------------------+
| * *                 | **Overall         |
|                     | assessment        |
|                     | (0-100)**         |
+=====================+===================+
| Anonymous           | 25                |
+---------------------+-------------------+
| Max Meier           | 40                |
+---------------------+-------------------+

*See
"*[*Metrics*](https://unjournal.pubpub.org/pub/evalsumceadiscounts#metrics "null")*"
below for a more detailed breakdown of the evaluators' ratings across
several categories. To see these ratings in the context of all Unjournal
ratings, with some analysis, see our *[*data presentation
here.*](https://unjournal.github.io/unjournaldata/chapters/evaluation_data_analysis.html#basic-presentation "null")[^1]*
*

*Caveat:**** ***This stream is in its early stages. These ratings may
not yet be reliable; e.g., one evaluator noted

> I am not sure what to compare this to, because I am unclear about
> scope. So please don't take these ratings seriously."

# Evaluation summaries

## Anonymous evaluator

Pro:

-   Raises important points and brings them to wider attention in simple
    language

```{=html}
<!-- -->
```
-   Useful for considering *individual* RCTs 

Con:

-   Not clear enough about intended use cases and framing. Writing
    should be clearer, shorter, more purposeful.

-   Guidelines need more clarity and precision before they can be
    genuinely used. I think best to reframe this as a research note,
    rather than a ready-to-use 'guideline'.

-   Unclear whether this is applicable to considering multiple studies
    and doing meta-analysis.

## Max Maier

The proposal makes an important practical contribution to the question
of how to evaluate effect size estimates in RCTS. I also think overall
the evaluation steps are plausible and well justified and will lead to a
big improvement in comparison to using an unadjusted effect size.
However, I am unsure whether they will lead to an improvement over
simpler adjustment rules (e.g., dividing the effect size by 2) and see
serious potential problems when applying this process in practice,
especially related to the treatment of uncertainty.

# Metrics

## Ratings

[See
here](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#quantitative-metrics "null")
for details on the categories below, and the guidance given to
evaluators.

+----------------------------------+---------------+----------+-----------+---------------+--------+----------+
|                                  | **Evaluator   |          |           | **Evaluator   |        |          |
|                                  | 1**           |          |           | 2**           |        |          |
|                                  |               |          |           |               |        |          |
|                                  | Anonymous     |          |           | Max Maier     |        |          |
+==================================+===============+==========+===========+===============+========+==========+
| **Rating category**              | **Rating      | **90% CI | *         | **Rating      | **90%  | **Co     |
|                                  | (0-100)**     | **       | *Comments | (0-100)**     | CI **  | mments** |
|                                  |               |          | **        |               |        |          |
|                                  |               | **(      |           |               | **(0-  |          |
|                                  |               | 0-100)\* |           |               | 100)\* |          |
|                                  |               | **       |           |               | **     |          |
+----------------------------------+---------------+----------+-----------+---------------+--------+----------+
| Overall assessment[^2]           | 25            | (0, 50)  | [^3]      | 40            | (10,   |          |
|                                  |               |          |           |               | 60)    |          |
+----------------------------------+---------------+----------+-----------+---------------+--------+----------+
| Advancing knowledge and          | 30            | (20, 80) | [^5]      | 50            | (20,   |          |
| practice[^4]                     |               |          |           |               | 70)    |          |
+----------------------------------+---------------+----------+-----------+---------------+--------+----------+
| Methods: Justification,          | N/A           | N/A      | [^7]      | 10            | (0,    | [^8]     |
| reasonableness, validity,        |               |          |           |               | 40)    |          |
| robustness[^6]                   |               |          |           |               |        |          |
+----------------------------------+---------------+----------+-----------+---------------+--------+----------+
| Logic & communication[^9]        | 30            | (10, 70) | [^10]     | 30            | (10,   | [^11]    |
|                                  |               |          |           |               | 50)    |          |
+----------------------------------+---------------+----------+-----------+---------------+--------+----------+
| Open, collaborative,             | 10            | (0, 50)  | [^13]     | N/A           | N/A    |          |
| replicable[^12]                  |               |          |           |               |        |          |
+----------------------------------+---------------+----------+-----------+---------------+--------+----------+
| Relevance to global priorities,  | 70            | (50, 95) | [^15]     | 50            | (10,   |          |
| usefulness for                   |               |          |           |               | 80)    |          |
| practitioners[^14]               |               |          |           |               |        |          |
+----------------------------------+---------------+----------+-----------+---------------+--------+----------+

##  {#r4724308342}

# Evaluation manager's discussion; process notes

## COI issues

As noted above, the author of this paper, Dr. Rosie Bettle, is a member
of The Unjournal's Management Team. To minimize the conflict of interest
issues, the author was kept at arms length. She was not made a part of
our deliberations over whether to prioritize this paper. We handled the
evaluations outside of our normal interface, so the author could not see
the process.

## Why we chose this paper

I (David Reinstein) first came across this paper in an earlier forum
where the author was requesting general feedback (this was before Dr.
Bettle joined our team). Joel Tan of
[CEARCH](https://exploratory-altruism.org/ "null") later independently
recommended we consider this work. I think it has strong potential to
impact cost-effectiveness analyses and funding. Founder\'s Pledge,
GiveWell and other organizations use fairly ad-hoc corrections and
\'discounts\' for external validity, publication bias/winner\'s curse,
etc. DrBettle aims to provide guidance towards using a more rigorous
framework for this. I think it\'s interesting, very decision, relevant,
and underprovided. Other researchers are doing careful work on the deep,
technical issues, and some (e.g. [Noah
Haber](https://www.metacausal.com/givewells-uncertainty-problem/ "null"))
are trying to bring this to applied contexts. But my impression is that
it\'s not being done in a way that is \'sticking\' and actually being
adopted in practice by relevant organizations like GiveWell. These
specific and less-technical guidelines may help.

This was not an academically-targeted paper, thus we evaluated it under
our \"[applied and
policy stream](https://docs.google.com/document/d/1RwkmGJtaOcryK-tJs3mrs6DIAj411YRnsauaj6EE6WY/edit?usp=sharing "null")\".

## Evaluation process

The author was eager to get feedback on this paper, and shared with us a
list of specific questions. These questions are embedded in the Google
Doc
[here](https://docs.google.com/document/d/17r2U4D5hiX1YtZHH8a_dLotB7GK6yQucJqDL9vy7TLM/edit?usp=sharing "null"),
along with some of the second evaluator's specific responses.

[^1]: Note: if you are reading this before, or soon after this has been
    publicly released, the ratings from this paper may not yet have been
    incorporated into that data presentation.

[^2]: Judge the quality of the research heuristically. Consider all
    aspects of quality, credibility, importance to future impactful
    applied research, and practical relevance and usefulness.

[^3]: I am not sure what to compare this to, because I am unclear about
    scope. So please don't take these ratings seriously.

[^4]: To what extent does the project contribute to improving the rigor
    and reliability of applied research and evidence-based policymaking
    in this area? Focus on 'improvements that are actually helpful' for
    global priorities and impactful interventions?

[^5]: Pro: It talks about important concepts that may not be known to
    decision makers\

    Con: I think it needs more work (to have workable guidelines) before
    it can actually impact practice. It would be good if this report
    could be related to existing literature on grading evidence etc.

    \

[^6]: Are methods clearly justified and explained? Are methods and their
    underlying assumptions reasonable? Are the results likely to be
    robust to changes in the assumptions? Have the authors avoided bias
    and questionable research practices?

[^7]: I think this is a report summarising some methodological
    literature, so I won't assess on this metric

[^8]: When applying the procedure, the conclusion seems very vulnerable
    to small chances in assumptions about how specific factors (e.g.,
    social desirability) influence effect sizes. Further, there are no
    guidelines provided for how to determine the impact of these
    specific factors.

[^9]: Are concepts clearly defined? Is the reasoning transparent? Are
    conclusions consistent with the evidence (or formal proofs)
    presented? Are the data and/or analysis, including tables and
    figures, relevant to the argument?

[^10]: Pro: right level of detail for decision makers, good use of plain
    language to communicate the concepts.

    Con: A lot of repetition, needs more clarity in some places. Figures
    and tables need reworking to be useful.

    \

[^11]: At several points a more detailed explanation is required.

[^12]: Would another researcher be able to replicate the analysis? Are
    the method and its details explained sufficiently? Is the source of
    the data clear? Is the data made as widely available as possible,
    with clear labeling and explanation? Do the authors provide
    resources that are likely to enable future research and
    meta-analysis?

[^13]: I think the main "replicable" part of this paper are guidelines,
    and as I indicated, I think these need to be revised to apply to
    particular cases. I don't think an average reader would be able to
    use them right off the bat.

[^14]: Are the paper's chosen topic and approach likely to be useful to
    global priorities, cause prioritization, and high-impact
    interventions?

    Does the paper consider real-world relevance and deal with policy
    and implementation questions? Are the setup, assumptions, and focus
    realistic? Do the authors report results that are relevant to
    practitioners. Do they provide useful quantified estimates (costs,
    benefits, etc.)?

[^15]: I think this is very relevant to global priorities research!
    Question marks over focus
