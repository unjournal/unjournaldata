---
article:
  doi: 10.21428/d28e8e57.6cfedd38
  elocation-id: evalsumpivotpenalty
author:
- Evaluator 1
- Andrew Kao
- David Reinstein
bibliography: /tmp/tmp-60fSlfZuqZZJsa.json
copyright:
  link: "https://creativecommons.org/licenses/by/4.0/"
  text: Creative Commons Attribution 4.0 International License
  type: CC-BY
csl: /app/dist/server/server/utils/citations/citeStyles/apa-6th-edition.csl
date:
  day: 21
  month: 05
  year: 2025
journal:
  publisher-name: The Unjournal
  title: The Unjournal
link-citations: true
title: "Evaluation Summary and Metrics: \"Adaptability and the Pivot
  Penalty in Science and Technology\""
uri: "https://unjournal.pubpub.org/pub/evalsumpivotpenalty"
---

# Abstract 

We organized one evaluation of the paper: \"Adaptability and the Pivot
Penalty in Science and Technology\"[@n492f3lvx8a]. From the evaluation:
"This paper introduces a measurement framework to quantify how far
researchers move from their existing research when producing new works.
\[It\] applies this framework to scientific publications and patents and
documents a phenomenon called the 'pivot penalty' ... the impact of new
research steeply declining the further a researcher moves from their
prior work. This finding holds across different researchers, fields, and
measurements of research impact."\
According to this evaluator, the paper's strengths include "its broad
applicability, strong data, and policy implications ... Should the
government and universities encourage researchers to address important
but remote questions? Should scientists pursue the hotspots of the
scientific landscape and adapt to demands driven by funding or other
reasons?" The evaluator's concerns include "under-explored benefits of
pivoting, alternative explanation\[s\] such as timing of rewards,
evolving journal preferences, and the choice of journals over fields for
analysis." To read this evaluation, please see the link below.

## **Evaluations**

1\. [Evaluator
1](https://unjournal.pubpub.org/pub/e1pivotpenalty/draft?access=lshm8knj "null")

# **Overall ratings**

We asked evaluators to provide overall assessments as well as ratings
for a range of specific criteria. * *

**I. Overall assessment **(See footnote[^1])

**II. Journal rank tier, normative rating (0-5): **On a 'scale of
journals', what 'quality of journal' should this be published in?[^2]
*Note: 0= lowest/none, 5= highest/best. *

+---+-------------------+---+
|   | **Overall         | * |
|   | assessment        | * |
|   | (0-100)**         | J |
|   |                   | o |
|   |                   | u |
|   |                   | r |
|   |                   | n |
|   |                   | a |
|   |                   | l |
|   |                   | r |
|   |                   | a |
|   |                   | n |
|   |                   | k |
|   |                   | t |
|   |                   | i |
|   |                   | e |
|   |                   | r |
|   |                   | , |
|   |                   | n |
|   |                   | o |
|   |                   | r |
|   |                   | m |
|   |                   | a |
|   |                   | t |
|   |                   | i |
|   |                   | v |
|   |                   | e |
|   |                   | r |
|   |                   | a |
|   |                   | t |
|   |                   | i |
|   |                   | n |
|   |                   | g |
|   |                   | ( |
|   |                   | 0 |
|   |                   | - |
|   |                   | 5 |
|   |                   | ) |
|   |                   | * |
|   |                   | * |
+===+===================+===+
| E | 80                | 4 |
| v |                   | . |
| a |                   | 5 |
| l |                   |   |
| u |                   |   |
| a |                   |   |
| t |                   |   |
| o |                   |   |
| r |                   |   |
| 1 |                   |   |
+---+-------------------+---+

*See
"*[*Metrics*](https://unjournal.pubpub.org/pub/evalsumpivotpenalty#metrics "null")*"
below for a more detailed breakdown of the evaluators' ratings across
several categories. To see these ratings in the context of all Unjournal
ratings, with some analysis, see our *[*data presentation
here.*](https://unjournal.github.io/unjournaldata/chapters/evaluation_data_analysis.html#basic-presentation "null")[^3]*
See
*[*here*](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#metrics-overall-assessment-categories "null")*
for the current full evaluator guidelines, including further explanation
of the requested ratings.*

# Evaluation summaries

## Anonymous evaluator 1

This paper presents a novel framework for measuring the degree of
departure from prior research in new works, documenting the \"pivot
penalty,\" where the impact of research declines as scientists venture
further from their past focus. The strengths include its broad
applicability, strong data, and policy implications. Concerns include
under-explored benefits of pivoting, alternative explanation\[s\] such
as timing of rewards, evolving journal preferences, and the choice of
journals over fields for analysis.

# Metrics

## Ratings

[See
here](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#quantitative-metrics "null")
for details on the categories below, and the guidance given to
evaluators.

+----------------------------------+-----------------+--------------------+
|                                  | **Evaluator 1** |                    |
|                                  |                 |                    |
|                                  | Anonymous       |                    |
+==================================+=================+====================+
| **Rating category**              | **Rating        | **90% CI **        |
|                                  | (0-100)**       |                    |
|                                  |                 | **(0-100)\* **     |
+----------------------------------+-----------------+--------------------+
| Overall assessment[^4]           | 80              | (70, 90)           |
+----------------------------------+-----------------+--------------------+
| Claims, strength,                | 82              | (74, 90)           |
| characterization of evidence[^5] |                 |                    |
+----------------------------------+-----------------+--------------------+
| Advancing knowledge and          | 85              | (78, 92)           |
| practice[^6]                     |                 |                    |
+----------------------------------+-----------------+--------------------+
| Methods: Justification,          | 85              | (78, 92)           |
| reasonableness, validity,        |                 |                    |
| robustness[^7]                   |                 |                    |
+----------------------------------+-----------------+--------------------+
| Logic & communication[^8]        | 80              | (71, 89)           |
+----------------------------------+-----------------+--------------------+
| Open, collaborative,             | 90              | (85, 95)           |
| replicable[^9]                   |                 |                    |
+----------------------------------+-----------------+--------------------+
| Real-world relevance [^10],[^11] | 80              | (70, 90)           |
+----------------------------------+-----------------+--------------------+
| Relevance to global              | 80              | (70, 90)           |
| priorities[^12], [^13]           |                 |                    |
+----------------------------------+-----------------+--------------------+

## Journal ranking tiers

[See
here](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#journal-ranking-tiers "null")
for more details on these tiers.

+-------------------------------+-------------+-------------------------------+
|                               | **Evaluator |                               |
|                               | 1**         |                               |
|                               |             |                               |
|                               | Anonymous   |                               |
+===============================+=============+===============================+
| **Judgment**                  | **Ranking   | **90% CI **                   |
|                               | tier        |                               |
|                               | (0-5)**     |                               |
+-------------------------------+-------------+-------------------------------+
| On a 'scale of journals',     | 4.5         | (4.0, 5.0)                    |
| what 'quality of journal'     |             |                               |
| *should* this be published    |             |                               |
| in?                           |             |                               |
+-------------------------------+-------------+-------------------------------+
| What 'quality journal' do you | 4.5         | (4.0, 5.0)                    |
| expect this work *will* be    |             |                               |
| published in?                 |             |                               |
+-------------------------------+-------------+-------------------------------+
| [See                          | *We         |                               |
| here](https://                | summarize   |                               |
| globalimpact.gitbook.io/the-u | these as:*  |                               |
| njournal-project-and-communic |             |                               |
| ation-space/policies-projects | -   0.0:    |                               |
| -evaluation-workflow/evaluati |             |                               |
| on/guidelines-for-evaluators# |  Marginally |                               |
| journal-ranking-tiers "null") |     respect |                               |
| for more details on these     | able/Little |                               |
| tiers.                        |     to no   |                               |
|                               |     value   |                               |
|                               |             |                               |
|                               | -   1.0:    |                               |
|                               |             |                               |
|                               | OK/Somewhat |                               |
|                               |             |                               |
|                               |    valuable |                               |
|                               |             |                               |
|                               | -   2.0:    |                               |
|                               |             |                               |
|                               |    Marginal |                               |
|                               |     B-jou   |                               |
|                               | rnal/Decent |                               |
|                               |     field   |                               |
|                               |     journal |                               |
|                               |             |                               |
|                               | -   3.0:    |                               |
|                               |     Top     |                               |
|                               |     B-jou   |                               |
|                               | rnal/Strong |                               |
|                               |     field   |                               |
|                               |     journal |                               |
|                               |             |                               |
|                               | -   4.0:    |                               |
|                               |             |                               |
|                               |    Marginal |                               |
|                               |     A-      |                               |
|                               | Journal/Top |                               |
|                               |     field   |                               |
|                               |     journal |                               |
|                               |             |                               |
|                               | -   5.0:    |                               |
|                               |     A-      |                               |
|                               | journal/Top |                               |
|                               |     journal |                               |
+-------------------------------+-------------+-------------------------------+

# Claim identification and assessment (summary)

For *the full discussions, see the* *corresponding sections in each
linked evaluation.*

+-------------------+--------------------------------+----------------------------+-------------------------+
|                   | **Main research claim**[^14]   | **Belief in claim**[^15]   | **Suggested robustness  |
|                   |                                |                            | checks**[^16]           |
+===================+================================+============================+=========================+
| **Evaluator 1     | \[\"Pivot penalty\": research  | \[Claim feels too strong;  | It might be helpful to  |
| **Anonymous       | impact declines sharply the    | likely missing factors in  | see the                 |
|                   | further one moves from         | the measurement            | promotion/funding of    |
|                   | previous work.\][^17]          | framework.\] [^18]         | these pivoting          |
|                   |                                |                            | scientists.             |
+-------------------+--------------------------------+----------------------------+-------------------------+

# Evaluation managers' discussion

## Why we chose this paper 

Global crisis events are likely to bring large welfare implications
(lives/years lost, suffering, productivity, threats to civilization,
etc.) Scientific research may help us prepare for, avoid, and moderate
the impact of these events. Sometimes (as in the case of COVID) pivoting
and adaptive responses are called for. We might want to anticipate and
pre-position researchers to address such potential crises, but
preparation brings costs, particularly as we may have unknown
unknowns/black swan events. We can plan better if we understand the
costs of research pivoting. This may also help us understand the best
approaches to funding research and fostering productive research careers
in a changing landscape. This could also inform governments and
philanthropic funders that are considering motivating researchers to
refocus their efforts on a concern (e.g., risks from artificial
intelligence) that was not in their original portfolio.

From our internal notes:

> Positives: novel, interesting and arguably counter-intuitive results
> which have wider implications
>
> Negatives: focus on Covid where a lot of things were happening so it's
> like a case study (but that should be part of the evaluation)

# Unjournal process notes, paper versions

The evaluation considered the August 2024 version of the paper, linked
[here](https://www.dropbox.com/scl/fi/r1qf2mnf07zifk7glz7tc/MS_SI_combined.pdf?rlkey=ucbeagmu3951u7busb2654mtr&e=1&dl=0).
Since then, the paper is reported to have been conditionally accepted in
Nature. We ended the evaluation process early, with a single evaluation,
in partial consideration of this. (See our discussion of when and how we
evaluate journal-published papers
[here](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/considering-projects/formats-research-stage-publication-status#publication-peer-review-status).)\
\
The authors shared an updated version, which we [link
here](https://www.dropbox.com/scl/fi/6gl0cxnqv01ex3ph44f9w/pivot_penalty_MS_feb.pdf?rlkey=suvrrzw8mdkcdju2fzijb9v9o&dl=0).
The authors note:

> The paper is largely very much as it was, but there is now a methods
> section, extended figures section, and a slightly revamped
> Supplementary Information. These are now in separate pieces, as
> opposed to a unified document (e.g., each figure is a separate file).
> But again it's all largely the same.

We asked ChatGPT o1 to consider the updates to the paper ([see
here](https://chatgpt.com/share/67db33c5-a260-8002-81e8-633904aef9df)),
and it basically confirmed that the changes were small; the main results
and methods seem unchanged. The most notable difference identified is
"the authors go from "13,456 retractions" and \~146k treated researchers
in the August version to "13,455 retractions" and \~165k treated
researchers in the February version". It is not clear what caused this
change.[^19]

We next asked ChatGPT o1 to consider "1. Any mistakes/misunderstandings
in the evaluation? 2. Any suggestions in the evaluation that seem to
have been addressed by the February update (relative to August?". We
added (some of) these as footnotes in the evaluation, giving the
evaluator a chance to respond; they responded in a few places, noted
below. (See o1 conversation
[here](https://chatgpt.com/share/67db35be-85d0-8002-abad-aa7862117a25).)

### How we chose the evaluators --- see footnote[^20]

## Authors' planned response

The authors have been in communication with us and aim to respond to
this evaluation. When they do, we will share it here. The paper has been
accepted for publication in Nature, and they have agreed to an embargo
on communication until its release.

# Issues potentially meriting further evaluation[^21]

We raised a number of issues in vetting this paper that might be worth
further consideration by readers and future evaluators and replicators.
We list these below.

***Caveat***: As evaluators have not considered these in detail, and
authors have not had a chance to respond yet, we are not asserting these
as substantive critiques of the paper, just as avenues for potential
inquiry.

1.  Does self-selection into pivoting (or into COVID topics) bias the
    estimated penalty (for use in more general future contexts)?

2.  (Relatedly) External generalisability beyond COVID: Do the pandemic
    funding conditions make the patterns special and unusual?

3.  Are citation-based bibliometrics the *most appropriate available
    *welfare metric for policy? Are they highly relevant? (Normative
    consideration.)

4.  *Statistical inference*: The main text of the paper presents very
    limited statistical inference (uncertainty bounds, hypothesis
    testing, etc.), although some of this is in the online appendix.
    This could merit further checking and consideration, considering the
    consistency of the test results and the narrative. \
    \
    Other diagnostic and robustness considerations could be relevant.
    For instance, adjusting for multiple-hypothesis adjustments/false
    discovery rates/familywise error rates. Parallel-trend tests seem
    particularly important in this context (including for retraction and
    COVID DiD setups). Some parallel-trend tests and graphics are
    presented in the appendix: evaluation could consider whether these
    appropriately and formally tested in every case.

# References

1\. Hill, R., Yin, Y., Stein, C., Wang, X., Wang, D., & Jones, B. F.
(2025). The pivot penalty in research. \*Nature\*.
https://doi.org/10.1038/s41586-025-09048-1 Preprint: arXiv:2107.06476.
https://doi.org/10.48550/arXiv.2107.06476

[^1]: We asked them to rank this paper "heuristically" as a percentile
    "relative to all serious research in the same area that you have
    encountered in the last three years." We requested they "consider
    all aspects of quality, credibility, importance to knowledge
    production, and importance to practice.

[^2]: See ranking tiers discussed
    [here](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#journal-ranking-tiers).

[^3]: Note: if you are reading this before, or soon after this has been
    publicly released, the ratings from this paper may not yet have been
    incorporated into that data presentation.

[^4]: Judge the quality of the research heuristically. Consider all
    aspects of quality, credibility, importance to knowledge production,
    and importance to practice.

[^5]: "Do the authors do a good job of (i) stating their main questions
    and claims, (ii) providing strong evidence and powerful approaches
    to inform these, and (iii) correctly characterizing the nature of
    their evidence?"\
    \
    This was on the newer form only

[^6]: To what extent does the project contribute to the field or to
    practice, particularly in ways that are directly or indirectly
    relevant to global priorities and impactful interventions?

[^7]: Are methods clearly justified and explained? Are methods and their
    underlying assumptions reasonable? Are the results likely to be
    robust to changes in the assumptions? Have the authors avoided bias
    and questionable research practices?

[^8]: Are concepts clearly defined? Is the reasoning transparent? Are
    conclusions consistent with the evidence (or formal proofs)
    presented? Are the data and/or analysis, including tables and
    figures, relevant to the argument?

[^9]: Would another researcher be able to replicate the analysis? Are
    the method and its details explained sufficiently? Is the source of
    the data clear? Is the data made as widely available as possible,
    with clear labeling and explanation? Do the authors provide
    resources that are likely to enable future research and
    meta-analysis?

[^10]: Does the paper consider real-world relevance and deal with policy
    and implementation questions? Are the setup, assumptions, and focus
    realistic and relevant to practitioners?

[^11]: The latter ratings were merged in the newer form\
    \
    "Are the paper's chosen topic and approach likely to be useful to
    global priorities, cause prioritization, and high-impact
    interventions?"

    "Does the paper consider real-world relevance and deal with policy
    and implementation questions? Are the setup, assumptions, and focus
    realistic? Do the authors report results that are relevant to
    practitioners? Do they provide useful quantified estimates (costs,
    benefits, etc.)?"

[^12]: Are the paper's chosen topic and approach likely to be useful to
    global priorities, cause prioritization, and high-impact
    interventions?

[^13]: The latter ratings were merged in the newer form\
    \
    "Are the paper's chosen topic and approach likely to be useful to
    global priorities, cause prioritization, and high-impact
    interventions?"

    "Does the paper consider real-world relevance and deal with policy
    and implementation questions? Are the setup, assumptions, and focus
    realistic? Do the authors report results that are relevant to
    practitioners? Do they provide useful quantified estimates (costs,
    benefits, etc.)?"

[^14]: The evaluator was given the following instructions:\
    \
    Identify the most important and impactful factual claim this
    research makes -- e.g., a binary claim or a point estimate or
    prediction.

    Please state the authors' claim precisely and quantitatively.
    Identify the source of the claim (i.e., cite the paper), and briefly
    mention the evidence underlying this. We encourage you to explain
    why you believe this claim is important, either here, or in the text
    of your report.

[^15]: Evaluators were asked: To what extent do you \*believe\* the
    claim you stated above? Feel free to express this either a. in terms
    of the probability of the claim being true, b. as a credible
    interval for the parameter being estimated, or c. however you feel
    comfortable.

[^16]: *We asked:*

    \[Optional\] What additional information, evidence, replication, or
    robustness check would make you substantially more (or less)
    confident in this claim?

    Feel free to refer to the main body of your evaluation here; you
    don\'t need to repeat yourself. Please specify how you would perform
    this robustness check (etc.) as precisely as you are willing. E.g.,
    if you suggest a particular estimation command in a statistical
    package, this could be very helpful for future robustness
    replication work.

[^17]: Evaluator wrote: "*"pivot penalty," where the impact of new
    research steeply declines the further a researcher moves from their
    prior work."*

[^18]: Evaluator wrote: "As I mentioned in the evaluation, I feel the
    claim is still too strong, even with some empirical support. There
    should be something happening but overlooked in the measurement
    framework."

[^19]: Context: "We first consider events that may push researchers away
    from an existing research stream. Specifically, prior research is
    sometimes revealed as incorrect or unreliable, which may encourage
    researchers who had been building on that work to move in new
    directions. Here we focus on paper retractions"

[^20]: We looked for

    -   Expertise in consideration and metrics/modeling of the
        similarity of research fields/topics/areas.

    -   Familiarity with the 'push to work on Covid topics' across a
        range of fields.

    -   Bibliometrics, grant-metrics, and measures of research impact.

    -   Interest in/work in metascience and research
        productivity/research production function.

    -   Understanding of statistics/statistical inference

[^21]: Evaluation managers: Use this section to list or explain issues
    that need further scrutiny, potentially by ["independent Unjournal
    evaluators"](https://coda.io/d/_d0KBG3dSZCs/Crowdsourcing-independent-evaluations_sufF1).
    These may be issues you suggested in your "manager's notes"/"bespoke
    evaluation notes" or issues evaluators identified that they
    acknowledged they were not fully able to address.
