---
article:
  doi: 10.21428/d28e8e57.1809195c/043dc6d9
  elocation-id: e2strongminds
author:
- Evaluator 2
bibliography: /tmp/tmp-60iy0WsDFKpLqi.json
copyright:
  link: "https://creativecommons.org/licenses/by/4.0/"
  text: Creative Commons Attribution 4.0 International License
  type: CC-BY
csl: /app/dist/server/server/utils/citations/citeStyles/apa-6th-edition.csl
date:
  day: 20
  month: 06
  year: 2025
journal:
  publisher-name: The Unjournal
  title: The Unjournal
link-citations: true
title: "Evaluation 2 of \"The wellbeing cost effectiveness of StrongMinds
  and Friendship Bench: review and meta-analysis with charity-related
  data\""
uri: "https://unjournal.pubpub.org/pub/e2strongminds"
---

# Abstract 

This is a thorough report on the cost-effectiveness of two charities
offering psychotherapy/ psychoeducation, which are found to be very cost
effective. The evidence, however, is highly uncertain, both due to the
quality of the studies and many of the somewhat arbitrary analytical
choices. I discuss some of these subjective decisions and chiefly
recommend a multiverse approach that transparently maps and compares
such decisions.

# Summary Measures

We asked evaluators to give some overall assessments, in addition to
ratings across a range of criteria. *See the *[*evaluation summary
"metrics"*](https://unjournal.pubpub.org/pub/evalsumstrongminds#metrics "null")*
for a more detailed breakdown of this. See these ratings in the context
of all Unjournal ratings, with some analysis, in our *[*data
presentation
here.*](https://unjournal.github.io/unjournaldata/chapters/evaluation_data_analysis.html#basic-presentation "null")[^1]*
*

+-------------------+-------------------+---+
|                   | **Rating**        | * |
|                   |                   | * |
|                   |                   | 9 |
|                   |                   | 0 |
|                   |                   | % |
|                   |                   | C |
|                   |                   | r |
|                   |                   | e |
|                   |                   | d |
|                   |                   | i |
|                   |                   | b |
|                   |                   | l |
|                   |                   | e |
|                   |                   | I |
|                   |                   | n |
|                   |                   | t |
|                   |                   | e |
|                   |                   | r |
|                   |                   | v |
|                   |                   | a |
|                   |                   | l |
|                   |                   | * |
|                   |                   | * |
+===================+===================+===+
| **Overall         | 87/100            | 8 |
| assessment **     |                   | 4 |
|                   |                   |   |
|                   |                   | - |
|                   |                   | 9 |
|                   |                   | 0 |
+-------------------+-------------------+---+
| **Journal rank    | 4.2/5             | 3 |
| tier, normative   |                   | . |
| rating**          |                   | 5 |
|                   |                   |   |
|                   |                   | - |
|                   |                   | 4 |
|                   |                   | . |
|                   |                   | 4 |
+-------------------+-------------------+---+

**Overall assessment **(See footnote[^2])

**Journal rank tier, normative rating (0-5): ** On a 'scale of
journals', what 'quality of journal' should this be published in?[^3]
*Note: 0= lowest/none, 5= highest/best. *

# Claim identification and assessment

## I. Identify the most important and impactful factual claim this research makes[^4] {#i-identify-the-most-important-and-impactful-factual-claim-this-research-makes}

Both charities evaluated at 5-6 times more cost-effective than cash
transfers at improving subjective wellbeing (as reported in abstract and
in report).

## II. To what extent do you \*believe\* the claim you stated above?[^5] {#ii-to-what-extent-do-you-believe-the-claim-you-stated-above}

I believe the general claim about cost-effectiveness of the charities in
a moderate to strong manner, as therapy provided by trained lay people
in LMICs can be affordable. I am uncertain about the specific estimates
due to the quality of the literature and the many stacking subjective
analytical decisions.

## III. Suggested robustness checks[^6] {#iii-suggested-robustness-checks}

More direct data (RCTs) independent evaluating these charities. For
robustness checks, see my discussion of multiverse.

# Written report

## Summary

McGuire and colleagues from the Happier Lives Institute (henceforth HLI)
(2024) report a cost-effectiveness analysis of psychotherapy
interventions for mental health in low-income settings using
wellbeing-adjusted life years (WELLBYs). Particularly they evaluate two
charities delivering psychotherapy (psychoeducation) in Africa:
Friendship Bench and StrongMinds. Their analysis combines evidence from
multiple sources: a meta-analysis of 84 RCTs of psychotherapy in LMICs,
charity-specific RCTs, and the charities\' own monitoring data. The
authors estimate initial effects, adjust for various validity concerns,
model decay over time, and add household spillover effects to calculate
cost-effectiveness. Both interventions are found to be very cost
effective, and approximately 5-6 times more cost-effective than
GiveDirectly cash transfers.

There are many things to like in this report. It is extensive and
multiple relevant factors are considered. It is clear that HLI, at every
point, have tried to adhere to rigorous practices relevant
methodological standards (e.g. such as by Cochrane for systematic
reviews) and have invited a lot of external feedback and scrutiny in
their process. I am very appreciative of their overall high transparency
in reporting their uncertainties in the present report, as well as their
very clear reporting on how their thinking has changed over time, since
previous version of this work. For instance, they make the issue of
effect dependency very clear and deal with this appropriately via a
multilevel meta-analysis. This is a very common issue in the literature
that many ignore, so it is a real positive to see this both reported
clearly and tackled head on. It is also a positive and reassuring sign
to see site visits took place. At the same time, I make a number of
critical observations about this report. I focus on many of the
decisions that appear arbitrary, such as choices of adjustments,
weights, removal of high risk studies and outliers. As I am responding
to Unjournal's applied stream (where one main goal is to help users of
such texts obtain practical benefits), in the next section I offer not
only statistical and empirical reservations but try to provide some more
practical suggestions that I hope will be of use to HLI.

## Major recommendations[^7]

1.  **Provide a one page summary**. This should be an executive summary
    that includes what was done, what was found, and what the
    implications (recommendations are) in an accessible language,
    understandable by potential donors and policymakers. At present the
    summary section (p. 3-7 but also the webpage link \[Manager: this
    refers to the* HLI webpage
    *[*here*](https://www.happierlivesinstitute.org/report/the-wellbeing-cost-effectiveness-of-strongminds-and-friendship-bench-combining-a-systematic-review-and-meta-analysis-with-charity-related-data-nov-2024-update/ "null")\]
    is too long and overly technical in the methods subsection. The
    report is extremely long itself (over 80 pages) with an appendix of
    over double that length.

    If I were HLI, I would invest in making visually attractive and very
    clear one pagers for all outputs. This is a tremendous amount of
    work they have done but a lot of the 'headline' findings and
    takeaways are not really presented in a particularly clear way. Most
    of the charities and NGOs I have advised and who do one pagers find
    them very useful for dissemination. Most potential donors are also
    quite time-constrained and want to make at least a first line
    decisions relatively quickly. \[For clarity: This is my most
    significant practical recommendation.\][^8]

2.  **\[Writing and organization\] **Relatedly, I will say the report
    can be quite hard to follow -- at times foreshadowing adjustments
    that appear in subsequent sections, at times referring back to other
    sections, mentioning further sensitivities, at times referring to
    the appendix, or a general methodology section found on the website,
    and sometimes even leaving it as a comment that more can be said
    (e.g. for the Baird RCT). There is a lot of information that even
    researchers would find taxing (time demanding) to fully parse, so
    the more that the overall presentation can be made more clear,
    better organized, and logically delineated, the better. I think
    Figure 2 (flow of analysis) is helpful in that regard but imagine
    more can be done. Further, at times I was unsure who HLI's intended
    reader is. The language can be quite informal in places and in some
    basic statistic concepts are explained, whereas in others the reader
    is assumed to grasp a lot of economics and interventional science.\

3.  **\[Communicating 'intangibles', multiple impact outcomes\] **A
    little bit more in regards to strategic communication.[^9] Many
    colleagues and friends I speak to and who regularly donate to
    charities and look at cost-effectiveness often don't look at CEAs
    exclusively (excluding, here moral decisions or "preferred" donation
    categories). Two issues come to mind. First, for instance, many have
    expressed to me they still prefer (when donating to global health
    causes) to support cash transfers because of a variety of
    'intangibles' hard to track in a CEA, for instance, that delivering
    cash is easier and more acceptable than other treatments, such as
    therapy (I put intangibles in ' ' because I somewhat disagree these
    cannot be accounted for but people do seem to have some internal
    sense of 'practical feasibility' that is not always
    operationalized). \
    \
    Second, and perhaps the more important issue I see is that often
    CEAs are calculated based on one primary outcome, but of course
    interventions likely impact multiple outcomes. In the context of
    this example, cash transfers will strongly impact poverty (their
    primary outcome usually) and will likely then affect physical and
    mental health, perhaps sometimes education and job prospects,
    including in the long run. Haushofer and colleagues reported in 2020
    (Haushofer et al. 2020)[@n54h7qvbx3c] that the same could not be
    said for psychotherapy -- psychotherapy had no economic or
    psychological effect one year later, whereas cash transfers did. If
    the CEAs of one intervention type affects many outcomes in notable
    ways and another type of intervention impacts primarily one outcome,
    then there's a distinction. I think these are the real and strong
    practical considerations conscientious (for instance, effective
    altruist aligned) charity donors think about. So if I were HLI, as a
    practical suggestion, I would be thinking about how to better
    position some of my outputs and communication in regards to such
    concerns.\

4.  **Present a specification curve, carry out a multiverse analysis.**
    \[For clarity: A multiverse for the CEA is my most significant
    statistical recommendation.\] A lot of decisions in the CEA (and
    indeed in the meta-analysis, and in the construction of the "initial
    dataset") appear arbitrary and subjective. To HLI's credit, as
    mentioned above, I think they generally do a very solid job \[of\]
    being transparent about both the subjectivity and uncertainty, and
    how they make their decisions. And the issue of researcher degrees
    of freedom is generally shared in other work (i.e. not exclusive to
    HLI, although they do report quite a number of subjective choices).
    Some decisions are a lot more speculative than others (e.g. weights,
    outliers) to my mind and I go in detail further below.

    On the whole, however, my very strong suggestion is that moving
    forward all plausible decisions are systematically presented and
    accounted for in a multiverse analysis. This does not prohibit
    picking a preferred model, as HLI reiterate they need to do.
    Instead, what it allows is the transparent mapping of how different
    plausible, theoretically sound choices affect the final estimate. An
    important part of the issue is that a lot of the choices (e.g. for
    adjustments or discounts) "stack" together (and indeed the very
    lovely Figure 2 highlights this well). Rather than considering
    single values, there should be a range of plausible values that
    better capture uncertainty for each considered factor/ adjustment.
    Indeed, many of the critiques of CEA I see, especially as they
    develop over time, are in the form of "you made this assumption
    here, but if you only change it by 5% \[e.g. due to this or that new
    or unconsidered evidence\] everything crumbles."

    Generally results from multiverse models are much more robust to
    such common critiques. And when you run such a model, you are also
    able to directly, empirically, compare which decisions have the
    highest impact and are most important to get right. This then
    becomes very important as you can better forecast what new evidence
    will be most impactful and will inform you to offer more precise
    estimates. Because HLI's current report includes so very many
    analytical steps, because many of them are subjective, and because
    most are single estimates, I consider the evidence they provide to
    have a very high uncertainty around it. This is in addition to HLI
    themselves suggesting there is uncertainty, for instance due to the
    quality of studies. To offer hopefully some practical use to HLI, I
    highlight some existing work that illustrates the benefits of a
    multiverse, that I consider high quality, and has some useful leads
    for next analytical steps. The examples relate to meta-analyses, and
    a multiverse for the meta-analysis would be great, but to be clear:
    a multiverse for the CEA relevant analytical decisions is key.\

    1.  Crawfurd et al. (2023)[@n9ns360ue6t]

    2.  Ciria et al. (2023)[@nlsz07l19vw]\

    3.  Plessen et al. (2023)[@nemwrv5lg1g]

        Note: this looks at the effects of psychotherapies on
        depression, and so is also topically relevant. Many of the
        considerations they include in their models, for instance around
        risk of bias and publication bias, are relevant to HLI's report.
        I hope Plessen's work illustrates the advantage of being able to
        transparently report and compare such varied analytical choices\

5.  **\[Independent evaluations/red-teaming charities\] **As another
    practical recommendation, if I were HLI, I would be spending some of
    my resources to commission independent evaluations (or a competition
    with prizes, or a red teaming effort &c.) of whichever charities
    they have decided to focus on evaluating next.

    One of my major reservations is that HLI seem more optimistic than
    other evaluators. When looking at Figure 8.2 in their chapter in the
    World Happiness Report[@doi.org/10.18724/whr-e0cy-0r69] there is a
    very notable evaluator effect. I ran a quick regression based on the
    values reported in the figure using evaluator and depth of analysis
    as predictors; evaluator effects were coded as a mean contrast, i.e.
    calculating deviations from the grand mean of all evaluators. This
    revealed a 25.2 point increase in the estimated value, i.e. WELLBYs
    generated per \$1000, if the evaluator is HLI (p = 0.01 even with
    this small sample size of 19 observations). See also Figure 8.4,
    where the average cost-effectiveness for HLI is \$42 per WELLBY,
    whereas the second best one is State of Life, at \$248 per WELLBY.
    How likely is that HLI are finding and evaluating the most
    cost-effective programs vs having a positive bias?

    To their credit, HLI do discuss this on p. 242 of the same chapter.
    Nevertheless, taking this all into account, this stills errs me on
    the side of being more cautious about the current report too, and
    recommending if they have capacity to select charities and
    commission external CEAs.

6.  **\[Feedback on pre-registrations\]** As a further practical
    suggestion, related to the above, if I were HLI, I would show my
    next preregistration (before submissions) to independent colleagues
    for external feedback.

## Further comments

### Suggested considerations

(I start with some questions suggested by The Unjournal in this section,
and move to some of my observations in the next.)

#### Systematic review

> How rigorous is the systematic review and meta-analysis? Are the
> inclusion/exclusion criteria well-justified?

***Response***: I mostly focused on the report, whereas the review
methodology is described in the Appendix. At a high level the search
seems appropriate, different relevant databases were considered, the
flowchart looks sensible in terms of exclusions. I am missing a
descriptive table with summaries on the studies (I discuss this more
when talking about the weights). I can't access the PROSPERO
preregistration link to comment further where definitions of concepts
are concerned.

#### Selective reporting

> Consider issues of selective reporting and what might be called "data
> cherry-picking" -- a general concern

*Response*: I am not concerned about cherry picking in this report. The
search looks thorough and I would be surprised if many key papers were
missed. I consider the issue of how the data is weighted separately.

#### Statistical approach for meta-analysis

> Statistical considerations -- use of random effects etc.

*Response*: I think random effects is certainly the most appropriate
choice and not fixed effects. This is described in the appendix well.
The high heterogeneity they find is supportive of this and a fixed
effect would have been more imprecise. Specifically, in practice from
what I have seen fixed effects lead to overestimation.

#### Use and Assumptions of WELLBYs, conversions

> The authors assume WELLBYs are the best metric for cost-effectiveness.
> Is this justified? Could alternative wellbeing measures alter
> conclusions?

*Response*: I like the use of WELLBYs, although in general I prefer to
also provide direct conversion into other measures (QALY, DALY, (s)HALY)
which aids future research and comparability. Adjusting for
non-linearity could be important in cases where there is a big spread,
as a change from 1/10 to 2/10 on life satisfaction is probably a bigger
change in wellbeing than 9/10 to 10/10, but I am generally not too
concerned for this here.

> Combining SWB & Clinical Measures: The authors merge PHQ-9, GHQ-12
> (mental health) and classical SWB scales into a single measure of
> "WELLBYs." Is this conversion robust and defensible?

*Response***:** In terms of a theoretical standpoint, in my view this is
generally practically okay, and have seen many colleagues do this as
well, particularly for meta-analyses, often referring to the pooled
effects as estimating a "general mental wellbeing" construct (i.e. not a
specific but a broad construct). This view is broadly supported for
instance, by some epidemiologists who argue in favor of a 'p-factor', a
proposed general psychopathology factor (Caspi et al.,
2014)[@nlb4dxydvxe], though it is not universally accepted and there are
opposing views.

It would have been helpful if HLI had offered (in the report text) a
more concrete, ideally preregistered definition, of 'affective mental
health' and wellbeing and what specific measures that means they'd
include and exclude. If I were to infer, they are looking at depression,
anxiety, and then broader (any) wellbeing measures. The appendix is not
very helpful in terms of definition. Table G19 offers a distribution of
included outcomes, but this isn't quite the same as an a priori
definition of a concept. This table is not particularly clear, the
outcomes are labelled somewhat informally and may indicate a conceptual
muddling -- for instance is "general MH" considered an affective mental
health problem or a subjective wellbeing measure?

#### Validity Adjustments (External Relevance & Dosage)

> Friendship Bench Dosage: Attendance is much lower in practice (\~1.1
> sessions) than in the RCTs (up to 6). Is the assumption that few
> sessions can still produce large effects justified?

**Response:** I was very surprised to see an attendance of 1.12 out of 6
possible sessions. I think the explanation they give for still having an
effect is plausible and also in line with other similar reports I have
read in similar contexts (e.g. about stigma, impactful negative
misunderstanding about mental health such as believes about being
cursed). It's still a little surprising given it's a fairly large effect
for one session.

In general, in the clinical literature, we worry about dropouts. A
treatment may appear very effective but it is actually only effective
for those who can stick with it. This is very true for exposure
treatment of phobias for instance, where the therapy is highly effective
but also demanding and non-completion can be high (in such cases
pre-treatment support can be offered, expectations can be addressed,
further accommodations to the sessions can be made &c). A somewhat
similar issue could be at play where there is stigma about mental health
and psychotherapy.

HLI discuss some of the reasons for the low attendance, such as for
instance difficulty of access, but I do also worry about low
acceptability, which would impact scalability. I think this it's
prudent, for clarity, to be explicitly labelling this as psychoeducation
only (as I understand the content of the first session). I wonder if
Friendship Bench have explored scaling down the program to just
psychoeducation given these findings of both low attendance but still
being effective? Or perhaps providing a more limited selt of training,
tailored to psychoeducation (or limiting the number of fully trained
people, with some trained only for psychoeducation) which would drive
down training costs?

One more practical suggestion I would make to HLI, if they haven't
looked into something similar already, is to explore mass communication
programs for psychoeducation -- whether there are any, what they look
like CEA-wise, or if there is a gap. I am thinking of some of the
research by Charity Entrepreneurship, which indicated that public
awareness campaigns, although sometimes hard to implement, can be very
highly cost effective. For instance, one of their top suggestions for
2025 is a [mass communication program for
education](https://www.charityentrepreneurship.com/reports/edu-mass-comm "null")
(Laughlin and Fairless 2025).[@ntbxadnbwwy] Perhaps there is a useful
conversations CE and HLI could have about this.

#### Modeling decay over time

> The effect of psychotherapy is assumed to decay linearly, with some
> partial "long-term" evidence. Are these modeling choices plausible?

**Response:** The effects of psychotherapy throughout treatment and post
(i.e. decay) are generally considered to be non-linear to my knowledge.
The vast majority of psychological phenomena are modelled exponentially
nowadays, though of course there's a long history of linear and
quadratic modelling in the past, particularly for wellbeing, health, and
cognitive outcomes. I am surprised to see a linear choice here.

I followed the link to HLI's general methods page (as given on p. 20).
Their description explains what a linear model of decay is but does not
offer a justification of why they prefer this. So it's hard to
understand what their reasoning is here but I am generally hesitant
about it given what is written in the report. It's always useful to try
to model something even if imperfectly; in this case a linear model is
possibly not the most optimal choice, and likely a liberal one that can
lead to overestimation. Maybe HLI have a sounder reason in choosing a
linear model here but it's simply not written up. I would recommend
either clarifying this, and/or adding an exponential model comparison.

## Other considerations

#### Construction of the initial dataset / removal of high risk studies & outlier

**Removal of high risk of bias studies** from the "initial dataset" (p.
26): The link to the preregistration in the appendix
(<https://www.crd.york.ac.uk/PROSPERO/view/431154>) returns a blank page
for me, so I cannot access this and cannot comment whether this was
preregistered and how it was justified. In general, it is not unsound to
remove high risk of bias studies if this was preregistered, and Cochrane
certainly allows this in their guidelines, but there are differing
opinions in how this should be approached (for instance, if there are
systematic reasons for studies being high risk, such as certain study
designs).

I'm generally in favor of the other option Cochrane suggests, which is
to include high risk studies in a first line analysis, but this should
be stratified by degree of risk and then present a stratified forest
plot. Judgments about risk of bias can carry a degree of subjective
decision making. My general standpoint is that it is more transparent to
present how including and excluding high risk studies affect the
estimate (i.e. one meta-analysis with all studies, one with dropping
high risk studies; this could be done in a multiverse branch or two
separate analyses for simplicity; and there are further sensitivity
approaches like meta-regressions by risk score or Bayesian approaches).

**Outlier removal:** I feel similarly (if not stronger) about the issue
of outlier removal. I cannot tell whether this (specific threshold for
exclusion) was preregistered or not. I would be very positively
surprised if this cutoff for outliers was preregistered and justified in
some fashion. Whether you exclude above \~1.5 SDs or 2, or 3 can be a
bit arbitrary too, and so also illustrates a case where a specification
curve is a sensible approach for transparent reporting. SDs of 2 are
already very high in my opinion. There needs to be a clear reason for
defining what constitutes an outlier. My view is that it's always better
to include the full data, analyze that, and then have a follow-up (or
ideally different branch of multiverse) that drops different thresholds
of outliers based on a theoretical view, previous literature, a
definition of implausible data &c.

For a discussion of more principled approaches to outlier treatment in
meta-analyses, see Viechtbauer, W., & Cheung, M. W. L.
(2010).[@nc74g20cmbv]

#### Adjustments 

The discussion in 5.1.2, the section on adjusting for publication bias,
also indicates wide researcher degrees of freedom, that would similarly
be better tackled through a multiverse analysis.

It seems particularly arbitrary to me to omit adjusting the Baird
results. Whether a study is preregistered is not necessarily always a
protection against potential publication bias -- a preregistration
wouldn't be able to deal with some questionable research practice (QRP)
such as spinning for instance, where the write-up of results and their
interpretation distort them in such a way to highlight the positive
findings and diminish the negative outcomes. Some have argued that
preprints and working papers that show positive findings are also more
likely to get shared, even if it's not in a 'traditional publishing'
manner.

Generally a lot of the write-up for the adjustments has a degree of
subjectivity (e.g. 10% discount for the general psychotherapy evidence
vs lay therapists), and instead of going over each, I'll repeat that a
multiverse approach is appropriate.

Section 5.2.4 -- discusses the issue of "non-compliance \[in Baird et
al.,\] which is unrepresentative of how StrongMinds operates" and also
again on p.69. I think a bit more argumentation as to why this is
unrepresentative is prudent and should be provided directly in the text.

#### Weights 

HLI are transparent that this is speculative, and I agree with that
assessment, this is a bit of a subjective and experimental exercise. I
appreciate figure 11 and tables 15, 16 to this effect. Since each of the
coauthors could express a subjective weighting preference (p. 63), I
would have personally added each one's preference into a multiverse and
made a note about why each was found plausible. The text suggests there
were different weights suggested at least initially, and having them
presented (even without a multiverse exercise, just in a simple table)
would have been great transparency. I generally agree with the authors
that "the charity-related causal evidence and the charity-related
pre-post evidence are more relevant than the general evidence" and one
could suggest a decision branch in a multiverse where you only
incorporate that (direct charity-related) evidence and in another,
incorporate charity-related evidence + the general. This could
articulate well the tradeoffs between, relevance, precision (e.g.
increased sample), quality of evidence etc.

For me, it's hard to advise on the weight structure more concretely in
part because I can't find a summary table with information for the
included studies. Apologies to the authors if I have missed this, but I
cannot seem to locate this in either the report or appendix. For
instance, in a systematic review report, this would be the descriptive
Table 1 (which is a PRISMA requirement and the authors will need to
produce if they haven't already and wish to publish their review in an
academic journal, as they indicate in one of their notes). Such a table
would have information on participant profile, the kind of therapy
offered, the average number of sessions attended, outcome types and
assessment etc. I imagine this as somewhat similar to the current
Appendix tables K1, K2, K3 but for all studies, and with an explicit
naming of the type of therapy, average nr of sessions attended, max nr
of sessions possible, and more on the population (mean age, % gender,
any other demographics of note such as pre-existing health problems &c.)
I generally also include the pre-post change in the outcome in such
tables too.

This information would help \[me\] comment more concretely on \[the\]
overall relevance to the charities whose cost-effectiveness is being
estimated here.

#### Dosage

I may be missing something here, but I am concerned that the effect of
dosage is not statistically significant (p. 52) but is still considered
as warranting a very substantial discount (p. 53). This needs further
explicit justification.

p\. 52: "However, the estimate of the effect of dosage has greatly
varied across different versions of this analysis. Notably, in this
version, it is small, and not statistically significant."

p\. 53: "This attendance of 1.12 sessions is substantially less than the
average 7.18 intended sessions in the general meta-analysis data, so we
adjust for the general meta-analysis for Friendship Bench by an
adjustment of $ln(1.12+1) / ln(7.18+1) = 0.36 $ (i.e., a 64% discount).
This attendance is also less than 6 intended sessions in each of the
Friendship Bench related RCTs, so we also adjust the Friendship Bench
relevant RCTs by $ln(1.12+1) / ln(6+1) = 0.39$ (61% discount). This
seems like very low attendance (and thereby, we assume, very low
dosage)."

I am also surprised by this, Appendix p 61. "The effect of dosage in
this model is so small that taken at face value it would suggest that
receiving 1 session has an initial effect that is 92% the value of
receiving 10 sessions in a log model. This is surprising but there is
research suggesting that single-session interventions can be impactful
(see Appendix H for more discussion of those studies)."

## Minor points

#### Gender

I wonder if it's worthwhile to consider these effects through a gender
lens? [Baird et al.
(2024)](https://documents1.worldbank.org/curated/en/099552207102441963/pdf/IDU1007db5cd16b2f146811a516124d1708f3085.pdf)'s[@ntyoy4mbp2g]
sample is female adolescents only (HLI mention this too, p. 56), and
[Peterson et
al.'s](https://strongminds.org/wp-content/uploads/2024/07/2019-2020-SM-Control-Study-IPT-G-Uganda-.pdf)
(2024)[@n14eywyrmfn] sample is adult women only. Googling around for
Friendship bench, I tend to see a similar trend. Given that the
providers are women as well, would similar results generalize to men?
Would such a treatment be acceptable and effective for men?

#### Early positioning

The first paragraph of the Summary (p. 3) is very ineffective to me. As
the opening paragraph, this is where the reader's attention should be
grabbed, and the problem should be clearly articulated. Instead this
reads as a little too casual in tone and perhaps even naïve. For
instance, psychotherapy itself really isn't that low-cost when delivered
by trained specialists but training lay people to provide forms of
(certain aspects of) psychotherapy can be low cost. In the context of
Friendship Bench, although the approach is CBT inspired, it's not full
CBT -- and given the reality that most attendees participated in only 1
session, which was psychoeducation, it's prudent to be clear and avoid
misleading statements.

There is also a huge omission of complexity between introducing the
problem of mental health in low-income countries and making the sweeping
statement that psychotherapy is a "solution", whereas for many in LMICs
mental health problems are caused by risk factors such as poverty. This
is important, as those arguing cash transfers work for mental health,
see them as treating the core problem, which is poverty.

In any case, my concrete recommendation here is to rewrite this section.
My personal suggestion would be to frame this around the inverse care
problem (similar to what is reported in 1.2.2). For instance, by saying
that in low-income countries, access to mental health care remains very
limited (e.g. underfunding of healthcare institutions, shortage of
specialists, stigma &c.) This treatment gap has profound consequences
for individual and societal wellbeing, but can be resolved by
effectively trained lay counsellors at relatively low cost.

#### Captioning 

A lot of the figures and tables throughout the report can benefit from
better captioning. It's rather minimal in places.

# References

[@n19s8z03qrk] Haushofer, J., Mudida, R., & Shapiro, J. (2020). *The
Comparative Impact of Cash Transfers and a Psychotherapy Program on
Psychological and Economic Well-being*. National Bureau of Economic
Research. <https://doi.org/10.3386/w28106>

[@nf26a8pdd1h] Crawfurd, L., Todd, R., Hares, S., Sandefur, J., &
Bonnifield, R. (2023). *How Much Would Reducing Lead Exposure Improve
Children's Learning in the Developing World?* Retrieved from
<https://www.cgdev.org/publication/how-much-would-reducing-lead-exposure-improve-childrens-learning-developing-world>

[@nowbv4awr1j] Ciria, L. F., Román-Caballero, R., Vadillo, M. A.,
Holgado, D., Luque-Casado, A., Perakakis, P., & Sanabria, D. (2023). An
umbrella review of randomized control trials on the effects of physical
exercise on cognition. *Nature Human Behaviour*, *7*(6), 928--941.
<https://doi.org/10.1038/s41562-023-01554-4>

[@nrdqve0fwm2] Plessen, C. Y., Karyotaki, E., Miguel, C., Ciharova, M.,
& Cuijpers, P. (2023). Exploring the efficacy of psychotherapies for
depression: a multiverse meta-analysis. *BMJ Mental Health*, *26*(1),
e300626. <https://doi.org/10.1136/bmjment-2022-300626>

[@doi.org/10.18724/whr-e0cy-0r69] Plant, M., McGuire, J., Dupret, S.,
Dwyer, R., & Stewart, B. (2025). *Giving to others: How to convert your
money into greater happiness for others* (Version 1). Version 1.
University of Oxford. <https://doi.org/10.18724/WHR-E0CY-0R69>

[@nv0gfv3yzx8] Caspi, A., Houts, R. M., Belsky, D. W., Goldman-Mellor,
S. J., Harrington, H., Israel, S., ... Moffitt, T. E. (2013). The p
Factor. *Clinical Psychological Science*, *2*(2), 119--137.
<https://doi.org/10.1177/2167702613497473>

[@nn3kizhq8qm] Laughlin, & Fairless. (2025). Mass Communication for
Education. Retrieved from
<https://www.charityentrepreneurship.com/reports/edu-mass-comm>

[@n70585m8uh4] Viechtbauer, W., & Cheung, M. W.-L. (2010). Outlier and
influence diagnostics for meta-analysis. *Research Synthesis Methods*,
*1*(2), 112--125. <https://doi.org/10.1002/jrsm.11>

[@nhhpln5uyfq] Baird,Sarah; Berk Ozler; Dell\'Aira,Chiara;
Parisotto,Luca; Us-Salam,Danish. *Therapy, Mental Health, and Human
Capital Accumulation among Adolescents in Uganda (English). *Policy
Research working paper;RRR;PEOPLE;COVID-19 (Coronavirus);Impact
Evaluation series Washington, D.C. : World Bank Group.
<http://documents.worldbank.org/curated/en/099552207102441963>

[@ni0yiyr2tic] Peterson, Ulasevich, Frame, Batrice, Mugoowa, Tanner, ...
Mwanja. (2024). Measuring the impact of IPT-G on adult women with
depression in Uganda in Iganga and Mukono Districts, 2019-2020.
*StrongMinds*.

[@nc4wfic1kun] McGuire, J., Dupret, S., Dwyer, R., Plant, M., & Klapow,
M. (2024). The wellbeing cost-effectiveness of StrongMinds and
Friendship Bench: Combining a systematic review and meta-analysis with
charity-related data (Nov 2024 Update). Happier Lives Institute.
https://www.happierlivesinstitute.org/report/the-wellbeing-cost-effectiveness-of-strongminds-and-friendship-bench/\

# Evaluator details

1.  How long have you been in this field?

    -   \~10 years.

2.  How many proposals and papers have you evaluated?

    -   \~50.

[^1]: Note: if you are reading this before, or soon after this has been
    publicly released, the ratings from this paper may not yet have been
    incorporated into that data presentation.

[^2]: We asked them to rank this paper "heuristically" as a percentile
    "relative to all serious research in the same area that you have
    encountered in the last three years." We requested they "consider
    all aspects of quality, credibility, importance to knowledge
    production, and importance to practice.

[^3]: See ranking tiers discussed
    [here](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#journal-ranking-tiers).

[^4]: The evaluator was given the following instructions: Identify the
    most important and impactful factual claim this research makes --
    e.g., a binary claim or a point estimate or prediction.

    Please state the authors' claim precisely and quantitatively.
    Identify the source of the claim (i.e., cite the paper), and briefly
    mention the evidence underlying this. We encourage you to explain
    why you believe this claim is important, either here, or in the text
    of your report.

[^5]:

[^6]: *We asked:*

    \[Optional\] What additional information, evidence, replication, or
    robustness check would make you substantially more (or less)
    confident in this claim?

    Feel free to refer to the main body of your evaluation here; you
    don\'t need to repeat yourself. Please specify how you would perform
    this robustness check (etc.) as precisely as you are willing. E.g.,
    if you suggest a particular estimation command in a statistical
    package, this could be very helpful for future robustness
    replication work.

[^7]: Manager: bolding/headers and paragraph breaks added here.

[^8]: *Manager:* Evaluator 2 left a further response to a query about
    this.\
    \
    \> I still think \[the web page link\] is quite long and ineffective
    for the purpose of a practical user quickly understanding what was
    done and what the implications are --- for instance, a practical
    user wouldn't care as much about the updates since previous report
    or all the methodological details on the page, but they would care
    about the bottom line which could have been brought up earlier and
    in a clearer way. For instance, HLI do this better in my opinion in
    a different report:
    <https://www.happierlivesinstitute.org/world-happiness-report/>
    where they key insights section is better articulated and
    immediately clear (I would imagine this section alone would be
    sufficient for a practical user to know if they want to continue
    reading or not just from this section alone). And the rest has a
    stronger presentation too (e.g. formatting, sections + visuals).

    If helpful, in my recommendations I'm also thinking about policy
    oriented one pager formats (for which there are many online
    templates that HLI may consult if useful to them), as examples here
    are some from UNICEF:
    <https://www.unicef.org/mena/media/19381/file/One%20pager-Bahrain:%20Design,%20implementation%20and%20child-sensitivity%20of%20social%20protection%20responses%20to%20COVID-19.pdf>
    ; <https://www.unicef.org/mena/media/20381/file> ;
    <https://www.unicef.org.uk/rights-respecting-schools/resources/teaching-resources/outright/one-page-summary-of-survey-findings_outright-201920/>

[^9]: Manager's note re 'strategic: Generally, The Unjournal wouldn't
    ask evaluators to give advice about strategic communication in the
    sense of "how to report your research to (help organizations)
    attract more donations or more support". But this is not what the
    evaluator is getting at here (and they confirmed this). They are
    suggesting something like 'how to report these results in a way that
    will be most helpful to a truth-seeking impact seeking donor would
    find useful'.\
    \
