---
article:
  doi: 10.21428/d28e8e57.86db8633
  elocation-id: evalsumeffectivenudge
author:
- Evaluator 1
- Evaluator 2
- Evaluator 3
- David Reinstein
- Valentin Klotzbucher
bibliography: /tmp/tmp-55GKKMWaM95QdW.json
copyright:
  link: "https://creativecommons.org/licenses/by/4.0/"
  text: Creative Commons Attribution 4.0 International License
  type: CC-BY
csl: /app/dist/server/server/utils/citations/citeStyles/apa-6th-edition.csl
date:
  day: 10
  month: 08
  year: 2024
journal:
  publisher-name: The Unjournal
  title: The Unjournal
link-citations: true
title: "Evaluation Summary and Metrics: \"Selecting the Most Effective
  Nudge: Evidence from a Large-Scale Experiment on Immunization\""
uri: "https://unjournal.pubpub.org/pub/evalsumeffectivenudge"
---

# Abstract 

We organized three evaluations of the paper: \"Selecting the Most
Effective Nudge: Evidence from a Large-Scale Experiment on
Immunization\"[@temp_id_7384950535985287]. The evaluations are generally
extremely positive. However, evaluator 2 expresses some doubts about the
novelty of the authors' ("treatment variation aggregation") approach and
its practical advantages relative to Bayesian estimators that are more
sophisticated than the ones they tested. Evaluator 3 has concerns about
the robustness of the assumptions behind the econometric justifications
for TVA. They also strongly encourage the authors to provide further
software sharing and guidance. To read these evaluations, please see the
links below.

## **Evaluations**

1\. [Anonymous evaluation
1](https://unjournal.pubpub.org/pub/eval1effectivenudge "null")

2\. [Anonymous evaluation
2](https://unjournal.pubpub.org/pub/eval2effectivenudge "null")

3.  [Anonymous evaluation
    3](https://unjournal.pubpub.org/pub/eval3effectivenudge "null")

# **Overall ratings**

We asked evaluators to provide overall assessments as well as ratings
for a range of specific criteria. * *

**I. Overall assessment: **We asked them to rank this paper
"heuristically" as a percentile "relative to all serious research in the
same area that you have encountered in the last three years." We
requested they "consider all aspects of quality, credibility, importance
to knowledge production, and importance to practice."

**II. Journal rank tier, normative rating (0-5):**[^1]** **On a 'scale
of journals', what 'quality of journal' should this be published in?
(See ranking tiers discussed
[here](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#journal-ranking-tiers "null").)
*Note: 0= lowest/none, 5= highest/best.*

+-----------------+-------------------+---+
| * *             | **Overall         | * |
|                 | assessment        | * |
|                 | (0-100)**         | J |
|                 |                   | o |
|                 |                   | u |
|                 |                   | r |
|                 |                   | n |
|                 |                   | a |
|                 |                   | l |
|                 |                   | r |
|                 |                   | a |
|                 |                   | n |
|                 |                   | k |
|                 |                   | t |
|                 |                   | i |
|                 |                   | e |
|                 |                   | r |
|                 |                   | , |
|                 |                   | n |
|                 |                   | o |
|                 |                   | r |
|                 |                   | m |
|                 |                   | a |
|                 |                   | t |
|                 |                   | i |
|                 |                   | v |
|                 |                   | e |
|                 |                   | r |
|                 |                   | a |
|                 |                   | t |
|                 |                   | i |
|                 |                   | n |
|                 |                   | g |
|                 |                   | ( |
|                 |                   | 0 |
|                 |                   | - |
|                 |                   | 5 |
|                 |                   | ) |
|                 |                   | * |
|                 |                   | * |
+=================+===================+===+
| Anon Evaluator  | 98                | 5 |
| 1[^2]           |                   | . |
|                 |                   | 0 |
+-----------------+-------------------+---+
| Anon. evaluator | 95                | 4 |
| 2               |                   | . |
|                 |                   | 7 |
+-----------------+-------------------+---+
| Anon. evaluator | 85                | 4 |
| 3               |                   | . |
|                 |                   | 5 |
+-----------------+-------------------+---+

*See
"*[*Metrics*](https://unjournal.pubpub.org/pub/evalsumeffectivenudge#metrics "null")*"
below for a more detailed breakdown of the evaluators' ratings across
several categories. To see these ratings in the context of all Unjournal
ratings, with some analysis, see our *[*data presentation
here.*](https://unjournal.github.io/unjournaldata/chapters/evaluation_data_analysis.html#basic-presentation "null")[^3]*
*

*See
*[*here*](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#metrics-overall-assessment-categories "null")*
for the current full evaluator guidelines, including further explanation
of the requested ratings.*[^4]

# Evaluation summaries

## Anonymous evaluator 1[^5]

This is an absolutely superb paper tackling a hugely important policy
question. The authors develop a new econometric approach to aggregating
high-dimensional factorial designs in RCTs in order to identify the most
effective policies, and apply it to the question of increasing childhood
immunization rates. I am thoroughly impressed by every aspect of this
paper: the dataset used, the specific treatment arms used, the policy
relevance, and the approach to identifying the best policy.

## Anonymous evaluator 2

Strengths:

-    Introduces a new technique - treatment variant aggregation (TVA) -
    to answer policy-relevant questions

-   Strong theoretical and simulation results grounded in real-world
    data

-   Demonstrates advantages in both selection and estimation of policies
    by comparing to many alternative estimators

-   Application to an urgent, real-world problem

Weaknesses:

-    Simulations rely on parameters from a single dataset, limiting
    generalizability

-    Unclear practical advantage of TVA in selecting better policies
    compared to alternatives

-   Bayesian estimators \[that are more sophisticated than the ones they
    tested\] could rival or surpass TVA

## Anonymous evaluator 3

The method is mostly a clever combination of existing methods; the main
contribution is showing \[the\] consistency and normality of their
estimator. These results need not always hold, as they depend on
assumptions, and so applicability is not universal. The authors provide
guidance by presenting simulations showing that for n \> 3000, normality
seems to hold. The field data reveal no surprise\[s\], and all the
interventions have been tested before. Yet, the paper is strong, and the
method will possibly remain relevant despite the recent advances in
adaptive experimental design, if the authors provide additional guidance
or software.

# Metrics

## Ratings

[See
here](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#quantitative-metrics "null")
for details on the categories below, and the guidance given to
evaluators.

+------------------------------+--------------+----------+--------------+---------+--------------+----------+----------+
|                              | **Evaluator  |          | **Evaluator  |         | **Evaluator  |          |          |
|                              | 1**[^6]      |          | 2**          |         | 3**          |          |          |
|                              |              |          |              |         |              |          |          |
|                              | A            |          | Anonymous    |         | Anonymous    |          |          |
|                              | nonymous\*\* |          |              |         |              |          |          |
+==============================+==============+==========+==============+=========+==============+==========+==========+
| **Rating category**          | **Rating     | **90% CI | **Rating     | **90%   | **Rating     | **90% CI | **       |
|                              | (0-100)**    | **       | (0-100)**    | CI **   | (0-100)**    | **       | Comments |
|                              |              |          |              |         |              |          | **       |
|                              |              | **(      |              | **(0    |              | **(      |          |
|                              |              | 0-100)\* |              | -100)\* |              | 0-100)\* |          |
|                              |              | **       |              | **      |              | **       |          |
+------------------------------+--------------+----------+--------------+---------+--------------+----------+----------+
| Overall assessment[^7]       | 98           | (96,     | 95           | (80,    | 85           | (75, 90) | [^8]     |
|                              |              | 100)     |              | 99)     |              |          |          |
+------------------------------+--------------+----------+--------------+---------+--------------+----------+----------+
| Advancing knowledge and      | 97           | (95,     | 95           | (80,    | 85           | (80, 90) | [^10]    |
| practice[^9]                 |              | 100)     |              | 99)     |              |          |          |
+------------------------------+--------------+----------+--------------+---------+--------------+----------+----------+
| Methods: Justification,      | 97           | (95,     | 90           | (70,    | 80           | (50,     | [^12]    |
| reasonableness, validity,    |              | 100)     |              | 95)     |              | 100)     |          |
| robustness[^11]              |              |          |              |         |              |          |          |
+------------------------------+--------------+----------+--------------+---------+--------------+----------+----------+
| Logic & communication[^13]   | 96           | (93,     | 90           | (70,    | 80           | (50,     | [^14]    |
|                              |              | 100)     |              | 95)     |              | 100)     |          |
+------------------------------+--------------+----------+--------------+---------+--------------+----------+----------+
| Open, collaborative,         | 96           | (92,     | 90           | (70,    | 70           | (25, 80) | [^16]    |
| replicable[^15]              |              | 100)     |              | 95)     |              |          |          |
+------------------------------+--------------+----------+--------------+---------+--------------+----------+----------+
| Real-world relevance [^17]   | 100          | (100,    | 95           | (75,    | 95           | (70,     | [^18]    |
|                              |              | 100)     |              | 99)     |              | 100)     |          |
+------------------------------+--------------+----------+--------------+---------+--------------+----------+----------+
| Relevance to global          | 100          | (100,    | 85           | (70,    | 95           | (90,     | [^20]    |
| priorities[^19]              |              | 100)     |              | 95)     |              | 100)     |          |
+------------------------------+--------------+----------+--------------+---------+--------------+----------+----------+

\*\* Manager's note: We will not incorporate the ratings from Evaluation
1 into our database/dashboard and analysis, as these show signs of being
uncalibrated. In general, we ask our evaluators to take a Bayesian
approach to specifying their credible intervals for percentiles and
predictions, avoiding degenerate intervals.

## Journal ranking tiers

[See
here](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#journal-ranking-tiers "null")
for more details on these tiers.

+-------------------------------+-------------+---------+-----------+--------+-----------+--------+-----------+
|                               | **Evaluator |         | **        |        | **        |        |           |
|                               | 1**[^21]    |         | Evaluator |        | Evaluator |        |           |
|                               |             |         | 2**       |        | 3**       |        |           |
|                               | An          |         |           |        |           |        |           |
|                               | onymous\*\* |         | Anonymous |        | Anonymous |        |           |
+===============================+=============+=========+===========+========+===========+========+===========+
| **Judgment**                  | **Ranking   | **90%   | **Ranking | **90%  | **Ranking | **90%  | **C       |
|                               | tier        | CI **   | tier      | CI **  | tier      | CI **  | omments** |
|                               | (0-5)**     |         | (0-5)**   |        | (0-5)**   |        |           |
+-------------------------------+-------------+---------+-----------+--------+-----------+--------+-----------+
| On a 'scale of journals',     | 5.0         | (5.0,   | 4.7       | (4.0,  | 4.5       | (4.0,  | [^22]     |
| what 'quality of journal'     |             | 5.0)    |           | 5.0)   |           | 5.0)   |           |
| *should* this be published    |             |         |           |        |           |        |           |
| in?                           |             |         |           |        |           |        |           |
+-------------------------------+-------------+---------+-----------+--------+-----------+--------+-----------+
| What 'quality journal' do you | 5.0         | (5.0,   | 4.7       | (4.0,  | 4.7       | (4.0,  | [^23]     |
| expect this work *will* be    |             | 5.0)    |           | 5.0)   |           | 5.0)   |           |
| published in?                 |             |         |           |        |           |        |           |
+-------------------------------+-------------+---------+-----------+--------+-----------+--------+-----------+
| [See                          | *We         |         |           |        |           |        |           |
| here](https://                | summarize   |         |           |        |           |        |           |
| globalimpact.gitbook.io/the-u | these as:*  |         |           |        |           |        |           |
| njournal-project-and-communic |             |         |           |        |           |        |           |
| ation-space/policies-projects | -   0.0:    |         |           |        |           |        |           |
| -evaluation-workflow/evaluati |             |         |           |        |           |        |           |
| on/guidelines-for-evaluators# |  Marginally |         |           |        |           |        |           |
| journal-ranking-tiers "null") |     respect |         |           |        |           |        |           |
| for more details on these     | able/Little |         |           |        |           |        |           |
| tiers.                        |     to no   |         |           |        |           |        |           |
|                               |     value   |         |           |        |           |        |           |
|                               |             |         |           |        |           |        |           |
|                               | -   1.0:    |         |           |        |           |        |           |
|                               |             |         |           |        |           |        |           |
|                               | OK/Somewhat |         |           |        |           |        |           |
|                               |             |         |           |        |           |        |           |
|                               |    valuable |         |           |        |           |        |           |
|                               |             |         |           |        |           |        |           |
|                               | -   2.0:    |         |           |        |           |        |           |
|                               |             |         |           |        |           |        |           |
|                               |    Marginal |         |           |        |           |        |           |
|                               |     B-jou   |         |           |        |           |        |           |
|                               | rnal/Decent |         |           |        |           |        |           |
|                               |     field   |         |           |        |           |        |           |
|                               |     journal |         |           |        |           |        |           |
|                               |             |         |           |        |           |        |           |
|                               | -   3.0:    |         |           |        |           |        |           |
|                               |     Top     |         |           |        |           |        |           |
|                               |     B-jou   |         |           |        |           |        |           |
|                               | rnal/Strong |         |           |        |           |        |           |
|                               |     field   |         |           |        |           |        |           |
|                               |     journal |         |           |        |           |        |           |
|                               |             |         |           |        |           |        |           |
|                               | -   4.0:    |         |           |        |           |        |           |
|                               |             |         |           |        |           |        |           |
|                               |    Marginal |         |           |        |           |        |           |
|                               |     A-      |         |           |        |           |        |           |
|                               | Journal/Top |         |           |        |           |        |           |
|                               |     field   |         |           |        |           |        |           |
|                               |     journal |         |           |        |           |        |           |
|                               |             |         |           |        |           |        |           |
|                               | -   5.0:    |         |           |        |           |        |           |
|                               |     A-      |         |           |        |           |        |           |
|                               | journal/Top |         |           |        |           |        |           |
|                               |     journal |         |           |        |           |        |           |
+-------------------------------+-------------+---------+-----------+--------+-----------+--------+-----------+

#  {#r8953321647}

# Evaluation manager's discussion 

# Unjournal process notes 

**Note on versions:** Evaluator 1 and Evaluator 2 considered an earlier
(Sept. 2022) version of this paper --- the version [on Arxiv
here](https://doi.org/10.48550/arXiv.2104.09645 "null"). Evaluator 3
considered the June 2024 version [linked
here](https://web.stanford.edu/~arungc/TVA.pdf "null"). (Evaluators 1
and 2 were given a chance to adjust their evaluations to the more recent
version but declined to do so.)

We shared a set of "bespoke evaluation notes" with the evaluators; these
are [linked
here](https://docs.google.com/document/d/1g1-c20k3e64roXi6BdTYYYxu6zhsdV2mwgqJu_osvwA/edit?usp=sharing "null"),
and included several specific suggestions for "some key issues and
claims to vet".

# Author engagement

We reached out to the authors on April 9 2024 to let them know we were
commissioning the evaluation of this paper, asking if they had any
particular requests, and asking about any newer or forthcoming
versions.Â  On August 10 we shared these evaluations with the authors,
which they acknowledged, and declined to respond, noting that the paper
had been accepted for publication in the journal Econometrica.[^24]

# Why we chose this paper 

This paper seems to be already having a direct influence on
funding/policy in impactful areas. According to the member of our team
that suggested this this is "already informing a J-PAL project in India,
as well as Suvita." The discussion of incentives vs reminders seems
particularly relevant, it would seem to inform charities like New
Incentives.

The methodological question is how to "select the best policy among
potential bundles that combine several interventions" and report a
credible measure of the impact of this 'best' intervention'. This seems
valuable for a range of high-impact contexts, from global health
interventions to fighting misinformation to promoting effective
charitable giving. I expect approaches like TVA (or more Bayesian
approaches to this question) to be increasingly used in research and
practice.

# Issues meriting further evaluation 

The "issues and claims" we suggested (in the [aforementioned
notes](https://docs.google.com/document/d/1g1-c20k3e64roXi6BdTYYYxu6zhsdV2mwgqJu_osvwA/edit?usp=sharing "null"))
were partially addressed by the evaluators. Some examples:[^25]

1.  "Lasso then prune"/results relying on sparsity assumptions,\
    As one example, we asked them to consider whether their 'lasso then
    prune' approach was justified. Their statistical justification
    relies on a sparsity assumption, i.e., in the real world some
    treatments need to have precisely a zero marginal effect. The third
    evaluator considered this in part, noting

> Treatments that have no effect whatsoever are either \"pooled or
> pruned (pooled with control)\". Scientifically, I am a bit unsure
> whether this should be allowed; a discussion seems warranted. It is
> obviously principled and data-driven, but conceptually, it seems wrong
> to me.
>
> ... Whether or not the strong assumptions are likely to be fulfilled
> in different settings will remain a judgment call.

This issue may merit further, more formal discussion from statisticians
and econometricians who work with machine learning/statistical learning.

2.  We asked about "scalability of the interventions/economic
    feasibility for more widespread adoption: Can these strategies be
    effectively implemented in other regions of India or similar
    contexts or are there aspects of the context in Haryana state that
    might affect the generalizability of the results?..." None of the
    evaluators discussed this issues, nor considered the context (India,
    immunizations) in detail.

3.   The authors report "The most cost-effective policy (information
    hubs, SMS reminders, no incentives) increases the number of
    immunizations per dollar by 9.1%" This is not a value/cost measure
    because it depends on the immunization base rate etc. We'd like to
    encourage the authors to estimate and report even more relevant
    Benefit/Cost measures. Future evaluators could offer specific
    suggestions for this, or estimate these themselves (using the code
    and data that will presumably be provided along with the final
    publication in Econometrica.)

# References

1\. Banerjee, A., Chandrasekhar, A. G., Dalpath, S., Duflo, E.,
Floretta, J., Jackson, M. O., Kannan, H., Loza, F. N., Sankar, A.,
Schrimpf, A., & Shrestha, M. (2025). Selecting the Most Effective Nudge:
Evidence from a Large-Scale Experiment on Immunization. *Econometrica*,
93(4). <https://doi.org/10.3982/ECTA19739.> Working paper: NBER Working
Paper No. 28726. https://doi.org/10.3386/w28726

[^1]: See "1 Mar 2024 note" above.

[^2]: Manager's note: This evaluation did not involve the level of
    substantive depth and careful constructive critique that we request.
    We are posting this with an 'asterisk' and we will not incorporate
    the quantitative ratings into our database.

[^3]: Note: if you are reading this before, or soon after this has been
    publicly released, the ratings from this paper may not yet have been
    incorporated into that data presentation.

[^4]: See "1 Mar 2024 note" above. See
    [here](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators/why-these-guidelines#adjustments-to-metrics-and-guidelines-previous-presentations)
    to learn how this changed, and to see the earlier instructions.

[^5]:

[^6]: Manager's note: This evaluation did not involve the level of
    substantive depth and careful constructive critique that we request.
    We are posting this with an 'asterisk' and we will not incorporate
    the quantitative ratings into our database.

[^7]: Judge the quality of the research heuristically. Consider all
    aspects of quality, credibility, importance to knowledge production,
    and importance to practice.

[^8]: Given the fact that each part of the paper in isolation (method;
    field data) is not in the 15% of best papers in my view (meaning
    with largest impact), the lower limit of the credible interval is
    rather low. As the method might have some impact (e.g., if software
    and more guidance is provided, and if referees are not bothered by
    the fact that normality is at least quite questionable for sample
    sizes below 1000), and as the combination (method + field) makes the
    paper rather unique and quite strong, I have a quite high upper
    limit for the credible interval.

[^9]: To what extent does the project contribute to the field or to
    practice, particularly in ways that are directly or indirectly
    relevant to global priorities and impactful interventions?

[^10]: Even though the method relies on asymptotic and other arguments,
    and hence, applicability will always be a question, I see the
    method\'s relevance as the paper\'s biggest selling point. I am a
    bit less sure, however, how relevant it will become in practice. The
    results of the field data are relevant, but of no particular
    surprise given my reading of recent literature in the field.

[^11]: Are methods clearly justified and explained? Are methods and
    their underlying assumptions reasonable? Are the results likely to
    be robust to changes in the assumptions? Have the authors avoided
    bias and questionable research practices?

[^12]: As far as I can tell, questionable research practices have been
    avoided. The robustness to some assumptions is checked, but not for
    all - in particular - Assumption 5, for example, is quite cryptic.
    In particular for that, no intuition is given, either. Other
    assumptions are cleanly explained and justified, even with
    intuition.

[^13]: Are concepts clearly defined? Is the reasoning transparent? Are
    conclusions consistent with the evidence (or formal proofs)
    presented? Are the data and/or analysis, including tables and
    figures, relevant to the argument?

[^14]: This varies a lot throughout the paper. At places, formulas are
    not explained; the simulation results are not really discussed.
    E.g., the \"r\" in Figure 3, Panel A - if a result of the
    simulations - should be discussed.

[^15]: Would another researcher be able to replicate the analysis? Are
    the method and its details explained sufficiently? Is the source of
    the data clear? Is the data made as widely available as possible,
    with clear labeling and explanation? Do the authors provide
    resources that are likely to enable future research and
    meta-analysis?

[^16]: Data and code for replication is not (yet?) made available; this
    is not unusual until the paper can be accessed via the journal\'s
    homepage, once accepted. A pre-registration is available (but it is
    not prominently mentioned, as this project has not been planned from
    the beginning). Hence, for the time being (paper is a working paper,
    not yet accessible via a journal homepage, but apparently accepted),
    the lower limit of the credible interval must be very low, but I
    absolutely expect code and data to become available in the future.

[^17]: Does the paper consider real-world relevance and deal with policy
    and implementation questions? Are the setup, assumptions, and focus
    realistic and relevant to practitioners?

[^18]: Yes, totally. The paper is very relevant for researchers wishing
    to implement (large-scale as in N \> 1500) RCTs as well as health
    practitioners. Assumptions will not be fulfilled in very many
    settings, though; it is unclear how clustered structures will have
    to be considered. The authors are to congratulate for making their
    exposition very accessible in most parts and providing intuition
    behind their setup for most of the assumptions, yet applicability of
    the method will remain questionable.

[^19]: Are the paper's chosen topic and approach likely to be useful to
    global priorities, cause prioritization, and high-impact
    interventions?

[^20]: The paper deals with finding the most effective policy
    methodically, and applies this to a an applied question in the
    health-context of India.

[^21]: Manager's note: This evaluation did not involve the level of
    substantive depth and careful constructive critique that we request.
    We are posting this with an 'asterisk' and we will not incorporate
    the quantitative ratings into our database.

[^22]: The paper certainly has the scope and potential for a publication
    in a journal ranked with 5; quality-wise, I feel that there is a
    huge overlap of publications in journals ranked with 4 and journals
    ranked with 5. Without checking the proofs thoroughly, I cannot for
    sure assess whether it is really made for the very best journals. As
    assumptions are somewhat strong and not always intuitive, I could
    imagine that a journal ranked with 5 passes. As the field data is
    impressive, and the method potentially useful (even though it is
    unclear exactly how applicable it is because of the asymptotic
    arguments and somewhat cryptic assumptions), I understand that a
    journal ranked with 5 wishes to publish the paper.

[^23]: Publishing in a journal with ranking 5 is random to a high degree
    in my view compared to publishing in a journal with ranking 4; there
    is quite some overlap quality-wise. The paper certainly has the
    scope and potential for a publication in a journal ranked with 5,
    and being from Stanford must help to push the randomness in the
    direction of the 5.

[^24]: We generally aim to to evaluate economics papers earlier in the
    process, before acceptance by a top journal. Nonetheless, our
    evaluations, even 'post-journal-publication' couild provide
    substantial value. What makes a paper \'worthy of Econometrica\'?
    What makes a particular paper innovative, credible, or impactful,
    and who should \'use\' the results and methods? Researchers and
    practitioners (especially those in LMICs and outside elite
    institutions) can gain from seeing the detailed considerations and
    painstaking discussions otherwise hidden in the traditional journal
    review process.

[^25]: As our resources permit, we aim to more concretely follow up this
    and other our evaluation packages with detailed lists of "Issues
    meriting further evaluation". We hope this will help guide
    participants in our nascent "Independent Evaluations" initiative.
