---
affiliation:
- id: 0
  organization: Future of Humanity Institute, Oxford
article:
  elocation-id: e3technologicalrisks
author:
- Gregory J. Lewis
bibliography: /tmp/tmp-46gut0jQ2zoZKo.json
copyright:
  link: "https://creativecommons.org/licenses/by/4.0/"
  text: Creative Commons Attribution 4.0 International License
  type: CC-BY
csl: /app/dist/server/server/utils/citations/citeStyles/apa-6th-edition.csl
date:
  day: 20
  month: 01
  year: 2025
journal:
  publisher-name: The Unjournal
  title: The Unjournal
link-citations: true
title: Evaluation 3 of "The Returns to Science In the Presence of
  Technological Risks", Applied Stream
uri: "https://unjournal.pubpub.org/pub/e3technologicalrisks"
---

# Abstract 

I was principally tasked with kicking the tires on the 'biocatastophe'
parameters as this is my subject of expertise. They check out, and I can
find little to criticise. \
\
Either I'm missing something, or the paper seems to be making a large
mistake by comparing 'all of the benefits' of science on the one hand
versus 'only the technical risks of biocatastrophe' on the other. Is AI
risk the elephant in the room?

# Summary Measures

We asked evaluators to give some overall assessments, in addition to
ratings across a range of criteria. *See the *[*evaluation summary
"metrics"*](https://unjournal.pubpub.org/pub/evalsumtechnologicalrisks#metrics "null")*
for a more detailed breakdown of this. See these ratings in the context
of all Unjournal ratings, with some analysis, in our *[*data
presentation
here.*](https://unjournal.github.io/unjournaldata/chapters/evaluation_data_analysis.html#basic-presentation "null")[^1]*
*

+-------------------+-------------------+---+
|                   | **Rating**        | * |
|                   |                   | * |
|                   |                   | 9 |
|                   |                   | 0 |
|                   |                   | % |
|                   |                   | C |
|                   |                   | r |
|                   |                   | e |
|                   |                   | d |
|                   |                   | i |
|                   |                   | b |
|                   |                   | l |
|                   |                   | e |
|                   |                   | I |
|                   |                   | n |
|                   |                   | t |
|                   |                   | e |
|                   |                   | r |
|                   |                   | v |
|                   |                   | a |
|                   |                   | l |
|                   |                   | * |
|                   |                   | * |
+===================+===================+===+
| **Overall         | 71/100            | 2 |
| assessment **     |                   | 7 |
|                   |                   |   |
|                   |                   | - |
|                   |                   | 8 |
|                   |                   | 7 |
+-------------------+-------------------+---+

**Overall assessment **(See footnote[^2])

# Written report

Thanks for the opportunity to review this intriguing paper. There are
two main parts to my review:

1.  I was principally tasked with kicking the tires on the
    'biocatastrophe' parameters as this is my subject of expertise. They
    check out, and I can find little to criticise.\

2.  Although not my expertise, I read the wider paper, and I struggle to
    make sense of it. Comparing the returns of *all* of science vs.
    *only some* of the technological risks (i.e. biocatastrophies) seems
    to stack the deck against technological pessimism. The 'AI risk'
    numbers for both domain experts and superforecasters alike are much
    higher for 'AI' than 'bio' risks. I think the 'bottom lines' of the
    paper depend on AI risks being excluded from consideration, but this
    exclusion looks very dubious to me.

I am much more sure of myself on the first issue than the second (where
I feel I must be missing something obvious), but the second issue is
much more important in terms of 'what to make of this paper'. I will
discuss both.

# 1) What's the 'right' parameter value for biocatastrophe risk? {#whats-the-right-parameter-value-for-biocatastophe-risk}

The principal inputs to the paper's modelling of technological risks are
forecast estimates of engineered pandemics of particular severity. (I'll
share the paper's convention in labelling these 'biocatastrophes').

## Sourcing from the XPT

The paper sources its estimates for biocatastrophe risk from the
Existential risk Persuasion Tournament (XPT), deriving various rate
parameters from its raw values to plug into the modelling results. The
paper's defence of relying upon this source is straightforward:

> As noted in section 2.3, while forecasting the future is very
> challenging and we do not even know if it can be reliably done at all
> over the long run, I think we have strong reasons to treat the results
> of this forecast as our best guess. I am not a domain expert in the
> relevant fields, but *I think we have strong reasons to see these
> estimates as defaults, and to impose the burden of proof on critics
> who would seek to suggest alternatives***.** \[p 25, my emphasis\]

It is also, in my view, straightforwardly correct: I would also use this
as my default source, and all the alternatives I can suggest are
inferior to the XPT. In essence:

1.  **There is very little 'first principles' modelling of bio risk.**
    The main paper is [Millet and Snyder-Beattie
    2017](https://pmc.ncbi.nlm.nih.gov/articles/PMC5576214/)[@n9t82f301te],
    and their quantitative offers were mainly 'illustrative' rather than
    'best guess'. In any case, their (calculated)[^3] figures for x-risk
    (albeit being offered in a spirit of conservatism) are in a similar
    ballpark to those in the paper (and if anything tending towards the
    lower superforcaster estimates):

+---------------------------------------------------+------------------+
|                                                   | \~Annual x-risk  |
+===================================================+==================+
| Domain expert (per Clancy)                        | 2.3 E-4          |
+---------------------------------------------------+------------------+
| Superforecaster (per Clancy)                      | 1.6 E-6          |
+---------------------------------------------------+------------------+
| Potentially pandemic pathogens (Per M&S-B)        | \~ 1 E-7         |
+---------------------------------------------------+------------------+
| Power law extrapolation \[bioterrorism\] (Per     | 1.4 E-6          |
| M&S-B)                                            |                  |
+---------------------------------------------------+------------------+
| Power law extrapolation \[biowarfare\] (Per       | 5 E-7            |
| M&S-B)                                            |                  |
+---------------------------------------------------+------------------+

> I am not aware of any attempts to provide more rigorous modelling work
> since. Even if such work is out there, I am sure it would be
> sufficiently speculative it would at most *inform* a best guess,
> rather than thinking we should substitute in its results *as* our best
> guess.

2.  **I am not aware of any strongly informative 'proxy indicators' for
    biocatastrophe**: stuff like how many ongoing clandestine BW
    programs, lead indicators like public health emergencies of
    international concern declaration etc. do tell you something about
    the risks you care about, but they're analogous to the weak
    indicators geopolitical forecasters have to resort to. The XPT
    forecasts (from either supers or domain experts) on these proxies
    seem broadly concordant with their forecasts of the key risks, and
    the set of proxies used in the XPT looks pretty good to me.

3.  **Insofar as we are indeed reliant on judgemental forecasting, the
    XPT seems to be the best game in town**, for reasons the paper
    articulates well. Yes, there have been previous opinion polls and
    individuals offering their guesses (e.g. the GCR 2008 conference
    poll, Toby Ord's estimates in The Precipice), which tend to offer
    bigger numbers. Yet the XPT has more people, many of whom selected
    for relevant expertise or ability, working in concert for a longer
    time. We would expect this to arrive at superior forecasts.[^4]

4.  Another key advantage is **most of the relevant 'literature'
    predates the XPT, and much of it was provided in its questions to
    the forecasters.** This poses a greater burden on the critic as it
    is credible their pet information or pet considerations have already
    been 'priced in', and second guessing the judgement does not look
    promising given point 3 above. **I am also not aware of any dramatic
    'post-XPT' revelations which could be expected to dramatically shift
    the aggregate forecasts.**

## Favouring the superforecasters

Despite discussion, the superforecaster and biodomain experts remained
far apart: domain experts gave larger numbers at least a factor of \~3,
and at most a factor of \~100, depending on the exact question. The
paper runs the numbers for both families of estimates. Although the
initial model has the same upshot 'either way', the models from section
8 onwards which incorporates extinction do look qualitatively different
depending on which estimates are plugged in. The paper's discussion
(Section 11.1) argues the superforecaster numbers should be favoured.

Running the numbers for both and highlighting sensitivity is great. I
also agree that the superforecaster numbers should be favoured. My
quibbles are on how strong this lean should be, and how this gap between
estimates could be better presented to the reader.

I think the reasons given in the paper to favour superforecasters count
for something, but do not add up to an impressive value of *pro tanto*.
Briefly:

-   Higher intersubjective accuracy has not (to my knowledge) been
    validated as a good proxy for accuracy, and in principle could be
    confounded by (e.g.) effort and time spent in discussions.[^5] The
    spread in forecasts (more later) demonstrates that being good at
    modelling the views of other participants does not prevent you from
    offering higher forecasts yourself.

-   'Correlated pessimism' for domain experts could be alternatively
    explained by low information: if I focus on AI, I may not know much
    about (e.g.) asteroids, but know enough about forecasting \[to know
    that I should not be giving extremely low probabilities without more
    information. Superforecasters with a general interest may apply
    their efforts more evenly across questions. Although I'd expect an
    optimistic geopolitical disposition to get beaten out of a
    superforecaster with bitter experience, there's very little
    corrective feedback if you're biased to optimistically rate 1%
    long-run risks as 0.1%.

-   Finding a different set of domain experts predicted \~ 3 bio events
    over a period where one happened (and their lower bound estimates
    roughly matched)[^6] is not the most damning indictment of domain
    expert accuracy: superforecasters, if well calibrated, will rack up
    occasional 'big misses' too.

Nonetheless, the best case I can muster for picking the domain experts
over the superforecasts here is even weaker:

-   The standard story for domain experts being better is they have a
    deeper knowledge of what they are talking about, and maybe they have
    developed an intuitive tacit grasp of their field which enhances
    their accuracy but which cannot be communicated to a non-expert
    despite the best intentions of both. Yet the findings from Expert
    Political Judgement suggest domain expertise offers very little
    edge.

-   Your reviewer is both a domain expert *and* a superforecaster, and I
    would take the 'over' on the superforecaster medians - I also
    speculate others I know who 'are both' would likely do the same.
    Presumably this "domain expert superforecaster" estimate should
    'trump' both domain experts and superforecasters in terms of
    checking more of the epistemic virtues. Ignoring the obvious problem
    this supposed aggregate is of "me and my imagination that similar
    folks would agree with me", larger groups of worse forecasters can
    outperform much smaller groups of better ones. Besides, my numbers
    also lean towards the supers, at least if we're taking the mid point
    as the arithmetic average.

### What does the interval between 'superforecaster' and 'domain expert' risk estimates look like in the xrisk modelling?

For the initial modelling of 'just' mortality in the time of perils, you
get the same upshot for the (much lower) superforecaster biocatastrophe
risk estimates or the (much higher) domain expert ones. The picture
looks different when you start considering extinction risks where bio
can stop the future rather than causing greater or lesser bumps along
the way.

If you plug the superforecaster numbers into the model in section 8,
you're indifferent between accelerating science being good or bad if you
start valuing the future at \~\~10\^5 of years of current annual world
value (Lambda). If instead you plug in the domain expert one, the
breakeven value of the future at risk is \~\~10\^2. I'll duck discussion
of whether benchmarking by discount-rates is the most informative (I
found it helpful), but I agree with the author the upshots now look
sensitive to the risk estimate chosen: if the first number, accelerating
science is credibly good, if the second number, it looks dubious.

But I would like to see the values of this modelling for the interval
between 'superforecaster' and 'domain expert' risk estimates. I don't
have a clear intuition[^7] whether the graph of Lambda break-even vs.
risk is basically linear, hyperbolic, log-linear, or something else.
This region seems important as most people's 'best guesses' for
biocatastrophe risk (including the author's, depending on how '75%' the
superforecasters are more correct is interpreted) will be 'somewhere in
between' the superforecaster and domain expert numbers. If the
arithmetic or geometric midpoint would give resulting Lambdas much
closer to one side or the other, I expect this would be handy for most
readers to know.

## Spread, uncertainty, and value of information (the bioxrisk numbers are not that great)

There's also a lot of spread within the groups of superforecasters and
bio experts, which is discussed and handled well by the relevant
appendix. However, although the numbers re. bio xrisks are the best we
have, they are not that great.

-   Only come from 14 bio experts.

-   Participants did one-shot so did not have the same depth of
    discussion amongst and between them.

-   I don't think we have measures of spread for either (although with
    small count the '90% percentile bioexpert estimate' would be noisy),
    and it would be nice to know how much the groups overlap for the
    x-risk questions in particular.

As it turns out these numbers play a starring role in the main
uncertainty the analysis highlights (i.e. maybe science isn't worth it
if there really is a \~1% chance of bio killing everyone by 2100?).
These numbers not being that resilient might be worth highlighting -
and, unlike the XPT generally, it may not be hard to gather more robust
figures on these topics in particular.

Naturally, one cannot fault the paper for playing the imperfect
empirical cards it is dealt. But I wonder whether it would be worth
highlighting this source of uncertainty: if nothing else, the 'murky'
picture for science if we take the domain expert figures for bio
extinction risk could become clearer if larger numbers or further
elicitation lead them to resolve to a higher or lower value.

# 2) Use and interpretation of the parameter

My major worry with the paper is not of getting its biocatastrophe
numbers wrong, but the external validity of its subsequent use of the
parameter in the model, and how those results are being interpreted.

## Comparing all the gains from science vs. a subset of the risks seems illegitimate in principle

The paper seems to compare the benefits of science *as a whole* on the
one hand versus *a subset* of the technical risks of science (i.e.
biocatastrophies) on the other. I struggle to make sense of this.

1.  If the aim is to assess whether accelerating science is worthwhile
    *overall*, one must weigh *all* the benefits versus *all* of the
    risks (AI being the elephant in the room - much more later). If
    accelerating science is indeed overall something that yields a
    negative expected value, finding that it would be worth doing if we
    only had to worry about engineered pandemics is at best irrelevant,
    and at worst misleading.

2.  If the aim is to assess whether *some subset* of science which
    generates technical risk is nonetheless worthwhile to accelerate, it
    should only be getting credit for its 'attributable fraction' of the
    aggregate benefits of science. Commercial fusion power sooner (e.g.)
    may be an expected dividend of accelerated scientific progress, but
    we can be basically sure the breakthroughs required will not be
    found in the life sciences.

In contrast, comparing *all*[^8] of the benefits of science vs. *only*
the biocatastrophe can only adjudicate a much more extreme
tech-pessimism: that the enhanced biocatastrophe risk would be
sufficient *alone* to make accelerating science as a whole undesirable.
That being false is consistent with either (or *both*): "Science in
general is socially undesirable to accelerate", or "Accelerating
\~biology is socially undesirable as its benefits do not outweigh the
increased biocatastophe risk it generates" being true.

It is treacherous for a reviewer to complain about the research question
being 'irrelevant' or 'trivial',[^9] but this seems the wrong question
to be asking - especially as, to the best of my knowledge, the claim
that increasing *bio*risk alone makes scientific progress generally
net-negative is seldom made (although - back to the elephant - I *have*
heard it mooted re. AI). If this is the question the paper wants to
answer, it should make clear its results only rule against an extreme
(bio)tech pessimism, and they provide no answers to steelman (or
realman?) tech pessimism.[^10]

Unfortunately, the paper seems to stray beyond its own modelling
stipulation (primarily - elephant again -- by sweeping AI risk out of
view). Perhaps the best example[^11] (besides the title of the work) can
be found in early in the executive summary:

> On the other hand, once we consider the more remote but much more
> serious possibility that faster science could derail advanced
> civilization, the case for science becomes considerably murkier. In
> this case, the desirability of accelerating science likely depends on
> the expected value of the long-run future, as well as whether we think
> the forecasts of superforecasters or domain experts in the existential
> risk persuasion tournament are preferred. These forecasts differ
> substantially: I estimate domain expert forecasts for annual mortality
> risk are 20x superforecaster estimates, and domain expert forecasts
> for annual extinction risk are 140x superforecaster estimates. The
> domain expert forecasts are high enough, for example, that if we think
> the future is \"worth\" more than 100 years of current social welfare,
> in one version of my model we would not want to accelerate science,
> because the health and income benefits would be outweighed by the
> increases in the remote but extremely bad possibility that new
> technology leads to the end of human civilization. However, if we
> accept the much lower forecasts of extinction risks from the
> superforecasters, then we would need to put very very high value on
> the long-run future of humanity to be averse to risking it. \[p2-3\]

The first sentence is not limiting itself to 'that faster science could
derail advanced civilization *via biocatastrophies alone*'. Yet the
forecast estimates and modeling discussed as (e.g.) 'annual extinction
risk' are only those for biocatastrophe. If we used the domain expert
(or superforecaster) estimates for AI, bio+AI, or 'anthropogenic x-risk'
generally, the future would need to be worth much less than 100 years of
welfare for us to be averse to risking it for faster science.

## Non-biotech risk cannot be reasonably discounted in practice

The paper stipulates in Section 2.2 it is restricting itself to
biological risks: "I call \[biotech enabled pandemics\] the time of
perils. Note my usage here refers specifically to biological perils, not
perils from unaligned AI or other forms of new technology." \[p8\] I
complain this stipulation (and subsequent broader interpretation of
findings) would 'in principle' stack the deck against pessimism.

If it were the case that biocatastrophe appeared to constitute the
lion's share of technical risk, although in principle it will
lower-bound overall peril, in practice it is probably close enough
unless the results proved very finely poised. Not so:

![](https://assets.pubpub.org/onjzrjkh/11732381031143.png){#nwlb39ot0eu}

![](https://assets.pubpub.org/8b8acrmx/41732381031143.png){#ncv02betvqu}

For both 'catastrophe' (\>\~10% dying) and extinction, bio comprises
small fractions of the aggregate risk. If asked 'are these risks
technological?' I aver most would answer 'clearly not' to natural
pathogens and non-anthropogenic risks, 'probably not' to nukes, and
'definitely' to *both* AI and engineered pathogens.

AI, of course, is the elephant here, 'beating' bio across the board. It
is extremely hefty given the paper's preferred weighing to
superforecasters and its finding that the xrisk modelling is the crux of
the matter: these estimates are 38x higher for AI than they are for bio.
So, in practice, 'stipulating out' AI risk from the technical risks
being assessed seems to indeed stack the deck: the returns to science
outweigh its technical risks, but only when the lion's share of
technical risk is ignored. Can this stipulation be justified?

### Science acceleration would not contribute to AI progress/risk? 

One rationale for exclusion would be if we would not expect generally
'boosting science' to contribute to AI risk like it would for engineered
pandemics. The paper moots this mainly in fn1, although it is alluded to
elsewhere:

> One reason this report focuses specifically on biocatastrophes rather
> than risks from advanced AI is that the rate of progress in AI is
> currently driven by major labs that operate outside the traditional
> academic ecosystem where science today is largely performed.
> Fundamental advances in the life sciences, in contrast, continue to be
> predominantly made in the academic ecosystem. \[p8\]

Yet:

-   Even if the picture *currently* is as-described, a lot can change
    between now and 2100. Maybe we have a couple of cycles of tech
    disruption, and the key players in (say) 2060 spawn (Deepmind
    style?) from ongoing academic activity.

-   *Pace* 'scale is all you need', maybe you need more fundamental
    breakthroughs (e.g. that require 'only' someone very smart with pen
    and paper, not 10\^lots FLOPs). It is at most 'murky' whether we
    would expect mega-labs to dominate if the path to AI risk runs
    through more theory, related fields like neuro and linguistics, or
    has a long wilderness without commercial application (etc.)

-   The scoping of meta-science in section 1.0 does not limit itself to
    'fundamental research': major 'applied' lab progress could be
    complemented by (e.g.) 'DARPA-like agencies', public-private
    partnerships, acquiring FROs, continued flows of academia-trained CS
    PhDs, etc.

I grant this reason counts for something, but less than any impressive
value of *pro tanto* (maybe a factor of 1.3x?). On its own, it falls a
long way short of justifying AI risks to be wholly discounted. If there
are other reasons which can successfully carry the rest of the burden, I
did not find them.

### AI only matters insofar *that* (not *how*) it can terminate the time of perils 

In the initial model (e.g section 4.2), AI comes up in terms of its
likelihood of closing the time of peril, where it takes the bulk of the
probability (1.6%/year, vs. 0.08% for non-AI extinction and 0.3% for a
major economic break). In this model, the only relevance to AI is its
contribution to the aggregate chance of exiting the time of peril:
whether the exit is 'good' or 'bad' is out of scope, and so the
propensity of AI to lead to one or the other is irrelevant.

There is a smaller issue here on whether 'non-bio' things can cause
catastrophe without closing the time of perils in ways that accelerated
science could meaningfully contribute to. XPT AI-catastrophe estimates
are greater than their x-risk/general transformation proxies (i.e. 2.1%
vs. 0.4% for the supers), and some of the AI-disaster mechanisms mooted
(e.g. nukes, supply chain disruption, 'AI rebellion') don't guarantee
transformation. A similar story applies to nukes, where most of the risk
is 'merely' catastrophic.

Ultimately, I don't think this adds up to much. Unlike AI, I think we
can roughly discount any contributions of accelerated science to nuke
risk (e.g. my understanding is, short of sci-fi-magic, you still need to
source a lot of fissile material no matter what). For AI, although I
can't give a firm partition, my impression reading through the XPT
report is that the sort of catastrophes folks had in mind were at least
peri-AI-transformation.

The bigger issue is well-described at the start of section 8:

> So far, these models have implicitly adopted the view that
> biocatastrophes reduce utility during the years in which they occur,
> but *do not otherwise impact humanity\'s long-term trajectory*.
> Whether biocatastrophes happen or not, we have maintained the
> assumption that we exit the current epistemic regime with probability
> 1-p in every period.
>
> This assumption is inappropriate for the worst kinds of biological
> peril. To take a simple, if extreme, example, *if an engineered
> pandemic kills every human on the planet, then progress towards
> transformative AI will stop*. This is also an outcome of pandemics
> that do not kill literally everyone, but kill enough to lead to the
> complete collapse of society (as is commonly envisioned in fiction
> like The Stand, The Last of Us, or The Last Man on Earth). \[p50, my
> emphasis\]

Yet (AI-)extinction definitely 'impacts humanity's long-term
trajectory', and would stop progress towards (positive)[^12] AI
transformation too. Now *which way* AI results in the time of perils
closing (i.e. good transformation or extinction/civ collapse) is back in
scope. The possibility scientific acceleration could boost the risk
window closing 'the wrong way' seems important to factor in (although
doing so is fraught - more later).

## Sensitivity?

Only looking at bio (/ignoring AI) seems a big assumption not only in
terms of 'dubious on the merits' but also 'decisive if made'.

Per the summary, the crux of the matter is in science's potential
contribution to 'civilization ending catastrophes'. From section 8 we
get some break-evens, roughly, the NPV of the future of
current-world-value units to compensate for the extinction risk being
run:

![](https://assets.pubpub.org/3buvw8wa/31732381031142.png){#np5lv6dngit}

The (annual rate) dx values are derived from their extinction risk
estimates for bio. And yet from the XPT we see AI risks are judged to be
substantial multiples higher than bio ones:

+---------------------------+------------------------------------------+
|                           | P                                        |
+===========================+==========================================+
| Bioxrisk                  | 0.01%                                    |
| (superforecaster)         |                                          |
+---------------------------+------------------------------------------+
| AIxrisk (superforecaster) | 0.38%                                    |
+---------------------------+------------------------------------------+
| Bioxrisk (domain expert)  | 1%                                       |
+---------------------------+------------------------------------------+
| AIxrisk (domain expert)   | 3%                                       |
+---------------------------+------------------------------------------+

I beg forgiveness for not properly running the numbers myself to derive
figures in terms of Lambda breakevens etc., but qualitatively I'd expect
the superforecaster aixrisks to net out as somewhat more 'optimistic'
than the biorisk estimates from domain experts, and the ai biorisk ones
to be more 'pessimistic'.

Qualitatively, moving from 'bio' numbers to 'ai' numbers should make the
results look a lot worse for accelerating science:[^13] the 'worst case'
bio numbers in the paper are now in a similar ballpark to the 'best
case' AI numbers (maybe Lambda of ??\~1000), and the new worst case
should be substantially worse (??\~50). Or, another way of looking at
it: the rosier picture of accelerating science now relies on both a
heavy lean to the superforecasters *and* a heavy discount for ai risks,
if only one or the other, the value of accelerating science looks
tenuous.

# Coda - maybe try and restrict to life science benefits?

One motivation for trying to keep AI out of the analysis that I am very
sympathetic to (and that I suspect the author shares) is trying to
'factor it in' is a complete nightmare. In the same way I don't want all
my \>10 year forecasts on \~every subject to have huge and uninformative
'but what about AI?' error bars, the paper is targeting an interesting
topic which we should prefer not degenerate into yet another rehearsal
of well-worn highly-uncertain AI-related considerations. Can't we just
bracket this out somehow?

My naive treatment of the AI risk estimates just before certainly
wouldn't do the trick: both the 'good' and 'bad' AI exits from the time
of perils should require further scientific activity, so generalised
scientific progress increases the rate of both. As \~everything depends
on which hazard happens first, we likely return to standard AI
considerations around which of the 'good' or 'bad' elements of AI
development have greater marginal response to generalized 'boosts'
applicable to both. The main policy upshot would be a pretty sterile
version of differential technology development ("Try to prod AI
development in the better-than direction.")

Yet the paper's set-up does make something like this the (\~intractable)
crux of the matter. Increments of existential risk are going to weigh
heavily no matter how one prices the future. If we take the XPT
seriously, the great bulk of 'addressable' existential risk relates to
AI. It is not that surprising small nudges to this parameter could
dominate the analysis if they were accounted for. "If we ignore AI risk,
the returns of accelerating science likely outweigh the risks which
could emerge from its advancement" ignores most of the issue.

I wonder whether partitioning the *upside* has more promise as a way of
"bracketing out" AI. Near the start of this section I mooted an
alternative 'like for like' comparison besides 'all of science's returns
vs. all of its risks': just take 'biocatastrophies' as the downside, but
just take 'bioscience returns' as the upside to compare it against.

By the standards of what the paper is already trying to tackle, getting
a handle on the 'attributable fraction' of bioscience benefits looks
feasible: R&D is 10-50% life sciences (depending on how and what you
count), and I expect funding share undercounts life sciences' expected
future contribution to aggregate benefits which heavily weigh life
expectancy. On the back of the envelope, reducing the returns of science
by a factor of (say) 3 doesn't change the qualitative upshots: speeding
up bio generally looks 'worth it' unless you hew heavily towards the
domain experts. Although an imperfect dissection, it does largely
'bracket out' AI to indirect/second-order-y corrections which are more
'reasonably neglectable'.[^14] This doesn't let you ignore AI when
assessing and deciding "all things considered", but it might allow a
fair look at the 'non-AI' parts of the problem in their own right.

## References

[@njzlltcf0v6]Millett, P., & Snyder-Beattie, A. (2017). Existential Risk
and Cost-Effective Biosecurity. *Health Security*, *15*(4), 373--383.
<https://doi.org/10.1089/hs.2017.0028>

# Evaluator details

1.  How long have you been in this field?

    -   9 years

2.  How many proposals and papers have you evaluated?

    -   \~50

[^1]: Note: if you are reading this before, or soon after this has been
    publicly released, the ratings from this paper may not yet have been
    incorporated into that data presentation.

[^2]: We asked them to rank this paper "heuristically" as a percentile
    "relative to all serious research in the same area that you have
    encountered in the last three years." We requested they "consider
    all aspects of quality, credibility, importance to knowledge
    production, and importance to practice.

[^3]: They also use the 2008 GCR poll for risk levels, which I'll cover
    later.

[^4]: A minor caveat is the GCR/xrisk bio estimates (which turn out to
    be more important for the report's bottom-line) were generated in a
    'one shot' after the XPT, so you don't get the added benefits of
    discussion (see fn 31
    [[here]{.underline}](https://static1.squarespace.com/static/635693acf15a3e2a14a56a4a/t/64f0a7838ccbf43b6b5ee40c/1693493128111/XPT.pdf)).
    But this is still superior to any other attempt to get these numbers
    I know of.

[^5]: I grant *this* is a reasonable (and somewhat more validated) proxy
    of accuracy itself, but I expect proxies-of-proxies to be not all
    that great.

[^6]: Although it depends on the aggregation (I beg forgiveness for not
    carefully checking). Adding up (e.g.) the lower 95% confidence
    interval for a bunch of estimates would give you a much lower
    percentile for the aggregate (e.g. 99%+,
    [[cf.]{.underline}](https://forum.effectivealtruism.org/posts/zHFBQ23o4DKjsoXcC/incorporating-and-visualizing-uncertainty-in-cost)),
    so getting outside these bounds could be a very big miss.

[^7]: I expect if I was more mathematically able or less lazy I could
    roughly figure it out from the equations, but I hope I am not the
    least able and most idle member of the paper's intended audience.

[^8]: In fairness, not really *all*: (e.g.) something like, "intrinsic
    value for us better understanding the world we live in" might have
    value not captured by GDP or life expectancy, and if I thought
    harder I could probably think of something less twee/small print.
    But even if (borrowing the framing section 3) I would want more for
    my children than them living long and being rich, living longer and
    being richer does comprise the 'lion's share' of what I could
    reasonably expect technological progress to do for them.

[^9]: E.g. Perhaps this *was* a question exercising OP internally, and
    it clearly would have stark implications on immediate funding
    decisions, worldview diversification, etc.

[^10]: In fairness, such a result tells you *something* about the
    aggregate: if you take the second biggest technical risk and find it
    doesn't outweigh the benefits of science as a whole, this rules
    against worlds where aggregate technical risk is extremely high (+/-
    and somewhat distributed between a few sources of danger). But 'not
    extremely high' can still be 'too high'.

[^11]: I think there are others, e.g.:

    > "The core dilemma is that we have good reason to think science
    > will soon enable a wider range of state and non-state actors to
    > genetically engineer new viruses, leading to more frequent and
    > deadly pandemic events" \[p1\]. Most (including me, despite being
    > a biorisk SME) would say Bio is not even the most important
    > dilemma: it's AI.

    > "Synthetic biology is not the only technology with the potential
    > to destroy humanity - a short list could also include nuclear
    > weapons, nanotechnology, and geoengineering." \[p. 9\] A short
    > list *should* include AI, and unless we are treating everything
    > outside of bio as exogenous, the report leaves open the question
    > about whether bio can claim the lion's share of the aggregate
    > danger.

    > This problem comes up elsewhere - bio is often mooted discursively
    > as 'particularly salient', or introduced in section 2.2 as a
    > 'special case' of the broader econ modelling. Yet it needs to get
    > to something like 'lion's share of the issue' for results
    > investigating it alone to plausibly generalise.

    > Section 8 is titled 'modeling extinction risks', yet only
    > considers bioxrisk both in the text and the parameter values (much
    > more later)

    > "I think the superforecaster estimates are more likely to be
    > correct. If this is so, the risks of extinction are so remote that
    > I do not think they should drive our decision-making". \[p68\]
    > Although superforecaster bioxrisk is "reasonably negligible" at
    > 0.01% by 2100, the superforecaster AI xrisk of 0.38% (\~1/300) is
    > not.

[^12]: I relegate the verbal disagreement of whether AI extinction
    counts as 'transformative' to this footnote, although the paper
    itself seems to only have the 'good' transformations in mind when it
    mentions transformative AI.

[^13]: Using AI risk alone is still a lower bound vs. AI + bio, but
    adding the risks may have double-counting issues.

[^14]: E.g. "What is bioscience advances are largely driven by AI?", or
    "Could advances in bioscience indirectly enhance advanced AI
    capabilities/risk" look like small print to me.
