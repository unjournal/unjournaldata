---
article:
  doi: 10.21428/d28e8e57.1809195c/a20707c8
  elocation-id: e1strongminds
author:
- Evaluator 1
bibliography: /tmp/tmp-43Y25ODRu5Rk55.json
copyright:
  link: "https://creativecommons.org/licenses/by/4.0/"
  text: Creative Commons Attribution 4.0 International License
  type: CC-BY
csl: /app/dist/server/server/utils/citations/citeStyles/apa-6th-edition.csl
date:
  day: 20
  month: 06
  year: 2025
journal:
  publisher-name: The Unjournal
  title: The Unjournal
link-citations: true
title: "Evaluation 1 of \"The wellbeing cost effectiveness of StrongMinds
  and Friendship Bench: review and meta-analysis with charity-related
  data\""
uri: "https://unjournal.pubpub.org/pub/e1strongminds"
---

# Abstract 

The paper evaluates the wellbeing cost-effectiveness of two
psychotherapy interventions in Sub-Saharan Africa, concluding they are
substantially more cost-effective, from a wellbeing perspective, than
cash transfers. The methodology is comprehensive and transparent. It
combines multiple data sources and conservative modelling assumptions.
While robust and transparent, the evaluation relies on project-specific
choices that may affect programme rankings. While the conclusions are
largely convincing, the paper would benefit from clearer discussion of
the the sensitivity of the findings to underlying assumptions and
standardised adjustment rules. Overall, this is a rigorous and valuable
contribution.

# Summary Measures

We asked evaluators to give some overall assessments, in addition to
ratings across a range of criteria. *See the *[*evaluation summary
"metrics"*](https://unjournal.pubpub.org/pub/evalsumstrongminds#metrics "null")*
for a more detailed breakdown of this. See these ratings in the context
of all Unjournal ratings, with some analysis, in our *[*data
presentation
here.*](https://unjournal.github.io/unjournaldata/chapters/evaluation_data_analysis.html#basic-presentation "null")[^1]*
*

+-------------------+-------------------+---+
|                   | **Rating**        | * |
|                   |                   | * |
|                   |                   | 9 |
|                   |                   | 0 |
|                   |                   | % |
|                   |                   | C |
|                   |                   | r |
|                   |                   | e |
|                   |                   | d |
|                   |                   | i |
|                   |                   | b |
|                   |                   | l |
|                   |                   | e |
|                   |                   | I |
|                   |                   | n |
|                   |                   | t |
|                   |                   | e |
|                   |                   | r |
|                   |                   | v |
|                   |                   | a |
|                   |                   | l |
|                   |                   | * |
|                   |                   | * |
+===================+===================+===+
| **Overall         | 85/100            | 7 |
| assessment **     |                   | 5 |
|                   |                   |   |
|                   |                   | - |
|                   |                   | 9 |
|                   |                   | 5 |
+-------------------+-------------------+---+
| **Journal rank    | 4.2/5             | N |
| tier, normative   |                   | / |
| rating**          |                   | A |
+-------------------+-------------------+---+

**Overall assessment **(See footnote[^2])

**Journal rank tier, normative rating (0-5): ** On a 'scale of
journals', what 'quality of journal' should this be published in?[^3]
*Note: 0= lowest/none, 5= highest/best. *

# Claim identification and assessment

## I. Identify the most important and impactful factual claim this research makes[^4] {#i-identify-the-most-important-and-impactful-factual-claim-this-research-makes}

**Claim 1:** The paper evaluates cost-effectiveness, in terms of
wellbeing, of two psychotherapy interventions in Sub-Saharan Africa.
These are Friendship Bench and StrongMinds. The paper concludes that
both interventions are cost-effective, in particular, 5-6 times more
cost-effective than cash transfers, as evaluated by a meta-analysis by a
group with the same first author:

-   Friendship Bench has a cost-effectiveness of 49 WBp1k, or \$21 per
    WELLBY.

-   StrongMinds has a cost-effectiveness of 40 WBp1k, or \$25 per
    WELLBY. \*

-   GiveDirectly has a cost-effectiveness of 7.55 WBp1k (i.e., \$132 per
    WELLBY) using a meta-analysis (McGuire et al., 2022a).

**Claim 2:** The authors also provide estimates of WELLBYs per \$1,000
donated to each of the organisations running the interventions and find
Friendship Bench to be almost 25% more effective than StrongMinds.

These claims are important because they address the question of how best
to allocate resources to improve wellbeing in LMICs. By providing
quantitative evidence suggesting that these psychotherapy interventions
are \[substantially\] more cost-effective \[from a wellbeing
perspective\] than interventions like cash transfers, the research makes
a case for increased funding and prioritisation of mental health
programmes in these settings.

## II. To what extent do you \*believe\* the claim you stated above?[^5] {#ii-to-what-extent-do-you-believe-the-claim-you-stated-above}

As discussed in my report, I find the first claim very convincing. I am
less convinced by the second, as it could be reversed by using a
different set of justifiable assumptions.

## III. Suggested robustness checks[^6] {#iii-suggested-robustness-checks}

My detailed suggestions are provided in the main body of the review. My
main concern is the number of arguably ad hoc adjustments based on
different sources. I would be substantially more confident in the
comparative cost-effectiveness estimates if the authors provided a clear
set of rules for the adjustments they are making, along with a list of
these adjustments to be applied consistently across all evaluated
projects. Given the number of choices the authors need to make, it is
unlikely that every reader will be convinced by all of them --- though
they are well justified. I believe the most effective course of action
would be to ensure maximum transparency around how these choices impact
the final estimates.

# Written report

The paper evaluates the wellbeing cost-effectiveness of two
psychotherapy interventions in Sub-Saharan Africa, concluding they are
cost-effective and 5 to 6 times more cost-effective than cash transfers.

For each programme, Friendship Bench and StrongMinds, the authors
combine evidence from three main sources: a general meta-analysis of
psychotherapy in LMICs, charity-related causal evidence (RCTs), and
charity-related pre-post monitoring and evaluation data. They estimate
the total effect \[on\] recipients over time, apply a series of internal
and external validity adjustments, estimate household spillovers, and
then aggregate the findings using informed subjective weights. The
cost-effectiveness is calculated by dividing the estimated overall
wellbeing effect by the cost per person treated.

Friendship Bench is estimated to be almost 25% more cost-effective than
StrongMinds. StrongMinds is estimated to have over twice the effect of
Friendship Bench on wellbeing, but it is also more than 2.5 times more
expensive per recipient.

The paper answers an important question about \[the\] comparative
effectiveness of the work of different charities in improving wellbeing.
The adoption of WELLBYs as a common outcome measure allows for a direct
comparison between different types of interventions, e.g. comparing
therapy to cash transfers \[as well as to\] other interventions. The
authors combine the evidence from a meta-analysis of therapy
effectiveness in LMICs with programme-specific data, \[and this
combination\] allows balancing better-quality evidence and more relevant
evidence. The main report and the extensive appendix are transparent
regarding the methodology, data sources, and analytical choices. The
authors acknowledge the limitations of the work, and the subjectivity
involved in certain parts of their methodology, particularly the
weighting of evidence sources.

**My main concern is that there are many project-specific choices that
the authors make for each evaluation.** Some of them affect the
coefficients significantly. Because it is a lengthy and detailed report,
it is not clear how much each of these choices affects the final result.
These are all perfectly justifiable modelling choices. However, given
the complexity of the task and the level of uncertainty involved, one
might imagine a different set of perfectly justifiable choices (more on
this below), that might lead to a different ranking. To be clear, the
authors are largely[^7] choosing more conservative estimates across
available options. If anything, the effects of the interventions are
more likely to be underestimated rather than overestimated. However, I
would treat the comparative ranking with caution, because of the level
of uncertainty involved. *\[Manager: see *[*discussion between
evaluators*](https://unjournal.pubpub.org/pub/e1strongminds/draft?access=hf7ej4oi "null")*
at bottom\]*

## My comments: [^8]

### \[1. Clarify evidence hierarchy\]

As a general point, it would have been helpful if the authors had
clarified what type of evidence they would consider 'ideal' for the
purposes of their comparative evaluation -- e.g. a quasi-experimental
evaluation conducted in the field, an RCT of this programme or a similar
one, or a combination of both. First, this could help motivate charities
to design evaluations accordingly. Second, it might guide the authors in
systematically weighting other sources based on their proximity to the
preferred evidence.

### \[2. Systematize adjustments\]

It would also make the claims more convincing if the authors provided a
set of rules for the adjustments they are making and the list of these
adjustments, which would be applied consistently across all evaluated
projects. This seems particularly relevant given the group's ambition
extends beyond evaluating just these two programmes. For example, the
authors go through a series of external validity adjustments using
various sources. To make these adjustments more systematic, they could
include characteristics -- such as number of sessions (possibly as
indicators for different durations), therapy type, whether the target
group has a clinically diagnosable condition or is from the general
population, and whether instructors are lay or professional -- as
moderators in the meta-analysis. This would allow them to generate
estimates that are best suited to each programme as a combination of
coefficients.[^9] I appreciate that some of these coefficients may not
be statistically significant; however, I see merit in relying on a
single source, rather than selecting different sources for each
adjustment.

### \[3. Sensitivity analysis\]

** **Given that readers may be more or less convinced by certain
adjustments, or may find them more or less relevant for their purposes,
it would be helpful to provide a concise visual summary of how these
adjustments affect the final estimates. I would suggest including an
analogue of Figure 2 -- with numerical results for both interventions --
early in the paper. This could draw on a combination of Tables 4, 8, 11,
13, and the cost estimates. If possible, incorporating the uncertainty
reported in Tables 20 and 21 would also be valuable. This would help
readers understand what drives the differences in results. If a reader
is unconvinced by a particular step, they could clearly see how
excluding it would alter the findings.

Two examples of alternative choices that would significantly affect the
results include:

-   The authors include Baird et al. as an RCT related to the programme,
    yet they explain in detail that the implementation, target group,
    and other key characteristics differ. Given all the differences, it
    is not clear why this paper should not be included in the
    meta-analysis instead.

```{=html}
<!-- -->
```
-   The authors inflate the cost estimate for StrongMinds from \$31
    (based on the 2024 report) to \$44.60. Without this adjustment -- or
    if it were only partially applied -- StrongMinds would appear more
    cost-effective than Friendship Bench.

### \[4. Meta-analysis issues\]

The meta-analysis carries the largest (though not equal) weight in both
evaluations. I have the following comments and questions regarding the
meta-analysis:

\[A. Weighting outcomes\] You include several outcomes, when available,
from each study. How are these weighted? If they are not, it would mean
that studies reporting more outcomes contribute disproportionately to
the analysis. Is your result sensitive to this?

\[B. Mental health vs. wellbeing --- clarify distinction\] You use
mental health and wellbeing measures interchangeably. While this is
acknowledged on page 57, it would be helpful to make this distinction
earlier. Figures such as B3 and those that follow would be more
informative if they clearly distinguished between mental health and
wellbeing outcomes in colour coding. Although they are colour-coded, it
is not clear what the colours represent. Alternatively, you could
present summary statistics or meta-regression results both with and
without the inclusion of mental health measures.

\[C. Which therapies?\] Do you apply any exclusion criteria regarding
the type of therapy? While it may be too restrictive to include only the
therapies used by the charities in question, could you consider relying
on studies that differentiate between interventions with higher or lower
levels of evidence-based support?

## Other comments: 

1\. The authors consider spillover effects for household members. As an
observation, under the current modelling, all else being equal, the
programme will be evaluated as more effective in countries with larger
households.[^10]

2\. Because there is no PPP adjustment for costs, all else being equal,
the programme will appear more effective in countries with lower
purchasing power.

3\. I would expect a linear decay to zero to be a conservative
assumption.

4\. For Table 3, could you include a histogram of effect sizes for each
of the three groups? This would illustrate how the estimates shift when
certain studies are excluded.

5\. On p.27, you state "See Appendix P for how much it influences the
analysis (not much)." It would be helpful to quantify this more
precisely \-- for example, by indicating the percentage change.

6\. How do you estimate confidence intervals for the figures that are
based on combinations of regression estimates (e.g. the final three rows
of Table 4)?

7\. On page 58, you write: "Moreover, spillovers can be greater or
lesser for one intervention: our previous working has found that cash
transfers have a relatively bigger spillover effect than cash." Could
you please clarify what this means?

8\. Regarding variance adjustments: I understand that initial response
variance is lower among respondents above the clinical threshold
compared to the general population. However, we would also expect the
treatment effect to be larger in that group. How does this affect your
analysis?

# Evaluator details

1.  How long have you been in this field?

    -   Over 5 years.

2.  How many proposals and papers have you evaluated?

    -   Over 20.

# Discussion amongst evaluators

> To be clear, the authors are consistently \[replaced with 'largely'\]
> choosing more conservative estimates across available options.

***E2:***

> This is a really important statement in my view. It's expressed very
> strongly currently and I disagree with it in this form --- I would
> update my position if the evaluator could break down a little bit more
> explicitly why the choices are conservative. For instance as one
> example, the evaluator goes \[on\] to suggest later on the linear
> decay is a conservative choice. I disagree with this, as I think an
> exponential choice is more appropriate and under such a view the
> linear model is actually more liberal. I generally understand decay
> effects of therapy to be seen as nonlinear in the literature, and here
> specifically, I further hesitate in thinking a linear model as
> appropriate given the small average number of sessions attended in FB
> particularly.
>
> Or as another example, whether or not the combination of the three
> streams of evidence and the subjectively informed weighing exercise
> are conservative or liberal seems to me a much more open-ended
> empirical question.

***E1:***

> I agree that exponential decay would be a more appropriate choice, and
> it would be interesting to see whether, in this particular case,
> exponential decay produces a more conservative estimate. In general, I
> consider linear decay to be conservative because it reduces the effect
> of therapy to zero. Given that most papers in the meta-analysis have
> follow-ups within one year, the linear decay in this case will reflect
> the rate during the first year. Take, for example, Figure 1 in Fava et
> al. (2004). They find that the decay in the first year is around 15%,
> but around 40% after six years. If modelled linearly based on the
> first year, the effect would be reduced by 90% after six years ---
> hence, the estimate would be more conservative.
>
> That said, I agree that the choice of weights is not necessarily
> conservative. In the comments below, I suggest using weighting and
> adjustments more systematically. My concern there is not whether they
> are conservative or not, but rather that they seem ad hoc at times.
>
> I am happy to replace *consistently* with *largely* in the sentence to
> reflect that weights are not covered. However, I do believe that the
> authors make a considerable effort to err on the side of caution.\
> \
> Fava, G. A., Ruini, C., Rafanelli, C., Finos, L., Conti, S., & Grandi,
> S. (2004). Six- Year Outcome of Cognitive Behavior Therapy for
> Prevention of Recurrent Depression. *American Journal of Psychiatry*,
> *161*(10), 1872--1876.

Manager: we replaced '*consistently'* with '*largely'* in the relevant
sentence.

[^1]: Note: if you are reading this before, or soon after this has been
    publicly released, the ratings from this paper may not yet have been
    incorporated into that data presentation.

[^2]: We asked them to rank this paper "heuristically" as a percentile
    "relative to all serious research in the same area that you have
    encountered in the last three years." We requested they "consider
    all aspects of quality, credibility, importance to knowledge
    production, and importance to practice.

[^3]: See ranking tiers discussed
    [here](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#journal-ranking-tiers).

[^4]: The evaluator was given the following instructions: Identify the
    most important and impactful factual claim this research makes --
    e.g., a binary claim or a point estimate or prediction.

    Please state the authors' claim precisely and quantitatively.
    Identify the source of the claim (i.e., cite the paper), and briefly
    mention the evidence underlying this. We encourage you to explain
    why you believe this claim is important, either here, or in the text
    of your report.

[^5]:

[^6]: *We asked:*

    \[Optional\] What additional information, evidence, replication, or
    robustness check would make you substantially more (or less)
    confident in this claim?

    Feel free to refer to the main body of your evaluation here; you
    don\'t need to repeat yourself. Please specify how you would perform
    this robustness check (etc.) as precisely as you are willing. E.g.,
    if you suggest a particular estimation command in a statistical
    package, this could be very helpful for future robustness
    replication work.

[^7]: Manager --- this was changed from 'consistently' to 'largely'
    after a text discussion with the other anonymous evaluator. We share
    this discussion at the bottom.

[^8]: Manager: We added the bracketed headers for clarity.

[^9]: Manager: Which coefficients?\
    \
    E1: I mean the coefficients of the model suggested above as a way to
    make external validity adjustments more systematic. That could be a
    model similar to the last specification in Table 10, but controlling
    for more treatment-specific characteristics. These estimates can be
    used for external validity adjustments. The authors currently do it
    for some characteristics but not others.

[^10]: Manager: "But isn't that part of what you would want to capture?"
    (and same for the PPP issue)?\
    \
    E1: "I didn't mean it as a critique --- more as an observation that
    the results are driven not only by the programme performance, but
    also by characteristics of the area. If so, it would indeed make
    sense to target low-GDP states with large household. I am convinced
    about the former. The latter, however, assumes that the size of
    spillover effects is independent of the household size --- which I'm
    not entirely certain about."
