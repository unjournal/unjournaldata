---
article:
  doi: 10.21428/d28e8e57.1809195c
  elocation-id: evalsumstrongminds
author:
- Evaluator 1
- Evaluator 2
- David Reinstein
- Davit Jintcharadze
- Valentin Klotzbücher
bibliography: /tmp/tmp-60ltpb1ectb4su.json
copyright:
  link: "https://creativecommons.org/licenses/by/4.0/"
  text: Creative Commons Attribution 4.0 International License
  type: CC-BY
csl: /app/dist/server/server/utils/citations/citeStyles/apa-6th-edition.csl
date:
  day: 20
  month: 06
  year: 2025
journal:
  publisher-name: The Unjournal
  title: The Unjournal
link-citations: true
title: "Evaluation Summary and Metrics: \"The wellbeing cost
  effectiveness of StrongMinds and Friendship Bench: review and
  meta-analysis with charity-related data\", applied stream"
uri: "https://unjournal.pubpub.org/pub/evalsumstrongminds"
---

# Abstract 

We organized two evaluations of the paper: "The wellbeing cost
effectiveness of StrongMinds and Friendship Bench: review and
meta-analysis with charity-related data". This paper was suggested by
*Happier Lives Institute* as part of [our
initiative](https://forum.effectivealtruism.org/posts/GxDjyEXCZc3pthkBb/unjournal-evaluating-1-research-output-per-relevant-ea "null")
to evaluate research from impact-focused/effective-altruism-aligned (or
funded) organizations; we're grateful for their engagement and for their
detailed and careful response. Both evaluators commend the thoroughness,
comprehensiveness, and transparency of this work overall, and gave this
paper high ratings. Both offer critiques and substantive suggestions,
particularly more systemization and explanation of design choices, and a
clearer depiction of the sensitivity analysis ("transparency around how
these choices impact the final estimates"). Both have some confidence in
the underlying results on both charities effectiveness relative to cash
transfers. Evaluator 2 is more critical of design choices that "appear
arbitrary". The authors dispute some of E2's specifics, arguing they
*already* present sensitivity checks similar to those requested; that
the case that "HLI more optimistic than other evaluators" is misleading;
and that most of their design choices are *conservative*. They also
disagree about the feasibility of systematization in this CEA context,
and about the usefulness of the formal "multiverse analysis" that E2
strongly advises. We discuss "issues meriting further evaluation and
analysis", and some approaches that might yield further confidence in
the results in the "evaluation manager's discussion" below.

## **Evaluations and author response **

1\. [Evaluator
1](https://unjournal.pubpub.org/pub/e1strongminds/draft?access=hf7ej4oi "null")

2\. [Evaluator
2](https://unjournal.pubpub.org/pub/e2strongminds/draft?access=2vxk271r "null")

-   [Authors'
    response](https://unjournal.pubpub.org/pub/authorsresponsestrongminds "null")

# **Overall ratings**

We asked evaluators to provide overall assessments as well as ratings
for a range of specific criteria. * *

**I. Overall assessment **(See footnote[^1])

**II. Journal rank tier, normative rating (0-5): **On a 'scale of
journals', what 'quality of journal' should this be published in?[^2]
*Note: 0= lowest/none, 5= highest/best. *

+---+-------------------+---+
|   | **Overall         | * |
|   | assessment        | * |
|   | (0-100)**         | J |
|   |                   | o |
|   |                   | u |
|   |                   | r |
|   |                   | n |
|   |                   | a |
|   |                   | l |
|   |                   | r |
|   |                   | a |
|   |                   | n |
|   |                   | k |
|   |                   | t |
|   |                   | i |
|   |                   | e |
|   |                   | r |
|   |                   | , |
|   |                   | n |
|   |                   | o |
|   |                   | r |
|   |                   | m |
|   |                   | a |
|   |                   | t |
|   |                   | i |
|   |                   | v |
|   |                   | e |
|   |                   | r |
|   |                   | a |
|   |                   | t |
|   |                   | i |
|   |                   | n |
|   |                   | g |
|   |                   | ( |
|   |                   | 0 |
|   |                   | - |
|   |                   | 5 |
|   |                   | ) |
|   |                   | * |
|   |                   | * |
+===+===================+===+
| E | 85                | 4 |
| v |                   | . |
| a |                   | 2 |
| l |                   |   |
| u |                   |   |
| a |                   |   |
| t |                   |   |
| o |                   |   |
| r |                   |   |
| 1 |                   |   |
+---+-------------------+---+
| E | 87                | 4 |
| v |                   | . |
| a |                   | 2 |
| l |                   |   |
| u |                   |   |
| a |                   |   |
| t |                   |   |
| o |                   |   |
| r |                   |   |
| 2 |                   |   |
+---+-------------------+---+

*See
"*[*Metrics*](https://unjournal.pubpub.org/pub/evalsumstrongminds#metrics "null")*"
below for a more detailed breakdown of the evaluators' ratings across
several categories. To see these ratings in the context of all Unjournal
ratings, with some analysis, see our *[*data presentation
here.*](https://unjournal.github.io/unjournaldata/chapters/evaluation_data_analysis.html#basic-presentation "null")[^3]*
*

*See
*[*here*](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#metrics-overall-assessment-categories "null")*
for the current full evaluator guidelines, including further explanation
of the requested ratings.*

# Evaluation summaries

## Anonymous evaluator 1

The paper evaluates the wellbeing cost-effectiveness of two
psychotherapy interventions in Sub-Saharan Africa, concluding they are
substantially more cost-effective, from a wellbeing perspective, than
cash transfers. The methodology is comprehensive and transparent. It
combines multiple data sources and conservative modelling assumptions.
While robust and transparent, the evaluation relies on project-specific
choices that may affect programme rankings. While the conclusions are
largely convincing, the paper would benefit from clearer discussion of
the sensitivity of the findings to underlying assumptions and
standardised adjustment rules. Overall, this is a rigorous and valuable
contribution.

## Anonymous evaluator 2

This is a thorough report on the cost-effectiveness of two charities
offering psychotherapy/ psychoeducation, which are found to be very cost
effective. The evidence, however, is highly uncertain, both due to the
quality of the studies and many of the somewhat arbitrary analytical
choices. I discuss some of these subjective decisions and chiefly
recommend a multiverse approach that transparently maps and compares
such decisions.

# Metrics

## Ratings

[See
here](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#quantitative-metrics "null")
for details on the categories below, and the guidance given to
evaluators.

+----------------------------------+-----------------+----------+---------------+-------------------+------------+
|                                  | **Evaluator 1** |          | **Evaluator   |                   |            |
|                                  |                 |          | 2**           |                   |            |
|                                  | Anonymous       |          |               |                   |            |
|                                  |                 |          | Anonymous     |                   |            |
+==================================+=================+==========+===============+===================+============+
| **Rating category**              | **Rating        | **90% CI | **Rating      | **90% CI **       | **         |
|                                  | (0-100)**       | **       | (0-100)**     |                   | Comments** |
|                                  |                 |          |               | **(0-100)\* **    |            |
|                                  |                 | **(      |               |                   |            |
|                                  |                 | 0-100)\* |               |                   |            |
|                                  |                 | **       |               |                   |            |
+----------------------------------+-----------------+----------+---------------+-------------------+------------+
| Overall assessment[^4]           | 85              | (75, 95) | 87            | (84, 90)          |            |
+----------------------------------+-----------------+----------+---------------+-------------------+------------+
| Claims, strength,                | 85              | (75, 95) | 93            | (90, 96)          |            |
| characterization of evidence[^5] |                 |          |               |                   |            |
+----------------------------------+-----------------+----------+---------------+-------------------+------------+
| Advancing knowledge and          | 90              | (85, 95) | 85            | (75, 90)          |            |
| practice[^6]                     |                 |          |               |                   |            |
+----------------------------------+-----------------+----------+---------------+-------------------+------------+
| Methods: Justification,          | 90              | (80, 95) | 85            | (80, 90)          |            |
| reasonableness, validity,        |                 |          |               |                   |            |
| robustness[^7]                   |                 |          |               |                   |            |
+----------------------------------+-----------------+----------+---------------+-------------------+------------+
| Logic & communication[^8]        | 85              | (80, 90) | 74            | (70, 85)          | [^9]       |
+----------------------------------+-----------------+----------+---------------+-------------------+------------+
| Open, collaborative,             | 90              | (85, 95) | 76            | (66, 80)          | [^11]      |
| replicable[^10]                  |                 |          |               |                   |            |
+----------------------------------+-----------------+----------+---------------+-------------------+------------+
| Real-world relevance [^12],[^13] | 95              | (90, 97) | 91            | (89, 93)          |            |
+----------------------------------+-----------------+----------+---------------+-------------------+------------+
| Relevance to global              | 95              | (90, 97) | 91            | (89, 93)          |            |
| priorities[^14], [^15]           |                 |          |               |                   |            |
+----------------------------------+-----------------+----------+---------------+-------------------+------------+

## Journal ranking tiers

[See
here](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#journal-ranking-tiers "null")
for more details on these tiers.

+-------------------------------+-------------+-----------+-----------+---------------+
|                               | **Evaluator | **        |           |               |
|                               | 1**         | Evaluator |           |               |
|                               |             | 2**       |           |               |
|                               | Anonymous   |           |           |               |
|                               |             | Anonymous |           |               |
+===============================+=============+===========+===========+===============+
| **Judgment**                  | **Ranking   | **Ranking | **90% CI  | **Comments**  |
|                               | tier        | tier      | **        |               |
|                               | (0-5)**     | (0-5)**   |           |               |
+-------------------------------+-------------+-----------+-----------+---------------+
| On a 'scale of journals',     | 4.2         | 4.2       | (3.5,     | [^16]         |
| what 'quality of journal'     |             |           | 4.4)      |               |
| *should* this be published    |             |           |           |               |
| in?                           |             |           |           |               |
+-------------------------------+-------------+-----------+-----------+---------------+
| What 'quality journal' do you | 3.5         | N/A       | N/A       |               |
| expect this work *will* be    |             |           |           |               |
| published in?                 |             |           |           |               |
+-------------------------------+-------------+-----------+-----------+---------------+
| [See                          | *We         |           |           |               |
| here](https://                | summarize   |           |           |               |
| globalimpact.gitbook.io/the-u | these as:*  |           |           |               |
| njournal-project-and-communic |             |           |           |               |
| ation-space/policies-projects | -   0.0:    |           |           |               |
| -evaluation-workflow/evaluati |             |           |           |               |
| on/guidelines-for-evaluators# |  Marginally |           |           |               |
| journal-ranking-tiers "null") |     respect |           |           |               |
| for more details on these     | able/Little |           |           |               |
| tiers.                        |     to no   |           |           |               |
|                               |     value   |           |           |               |
|                               |             |           |           |               |
|                               | -   1.0:    |           |           |               |
|                               |             |           |           |               |
|                               | OK/Somewhat |           |           |               |
|                               |             |           |           |               |
|                               |    valuable |           |           |               |
|                               |             |           |           |               |
|                               | -   2.0:    |           |           |               |
|                               |             |           |           |               |
|                               |    Marginal |           |           |               |
|                               |     B-jou   |           |           |               |
|                               | rnal/Decent |           |           |               |
|                               |     field   |           |           |               |
|                               |     journal |           |           |               |
|                               |             |           |           |               |
|                               | -   3.0:    |           |           |               |
|                               |     Top     |           |           |               |
|                               |     B-jou   |           |           |               |
|                               | rnal/Strong |           |           |               |
|                               |     field   |           |           |               |
|                               |     journal |           |           |               |
|                               |             |           |           |               |
|                               | -   4.0:    |           |           |               |
|                               |             |           |           |               |
|                               |    Marginal |           |           |               |
|                               |     A-      |           |           |               |
|                               | Journal/Top |           |           |               |
|                               |     field   |           |           |               |
|                               |     journal |           |           |               |
|                               |             |           |           |               |
|                               | -   5.0:    |           |           |               |
|                               |     A-      |           |           |               |
|                               | journal/Top |           |           |               |
|                               |     journal |           |           |               |
+-------------------------------+-------------+-----------+-----------+---------------+

# Claim identification and assessment (summary)

For *the full discussions, see the* *corresponding sections in each
linked evaluation.*

+------------+------------------------------------+------------------------------+--------------------------------------+
|            | **Main research claim**[^17]       | **Belief in claim**[^18]     | **Suggested robustness checks**[^19] |
+============+====================================+==============================+======================================+
| *          | **Claim 1:** Friendship Bench and  | I find the first claim very  | Main concern is \[... ad hoc         |
| *Evaluator | StrongMinds \[5--6x more           | convincing. I am less        | adjustments\]; if the authors        |
| 1          | cost-effective than cash           | convinced by the second, as  | provided \[clear, consistent rules\] |
| *          | transfers\]\                       | it could be reversed by      | and \[maximum transparency on their  |
| *Anonymous | **Claim 2:** Friendship Bench      | using a different set of     | impact\] I would be substantially    |
|            | gives \[\~25% more WELLBYs per     | justifiable assumptions.     | more confident.                      |
|            | \$1k than StrongMinds\].           |                              |                                      |
+------------+------------------------------------+------------------------------+--------------------------------------+
| *          | Both charities evaluated at 5-6    | I believe the general claim  | More direct data (RCTs) independent  |
| *Evaluator | times more cost-effective than     | \... in a moderate to strong | evaluating these charities. For      |
| 2          | cash transfers at improving        | manner, but \[uncertain      | robustness checks, see my discussion |
| *          | subjective wellbeing (as reported  | about specific estimates\]   | of multiverse.                       |
| *Anonymous | in abstract and in report).        | due to \[subjective          |                                      |
|            |                                    | decisions and literature     |                                      |
|            |                                    | quality\].                   |                                      |
+------------+------------------------------------+------------------------------+--------------------------------------+

#  {#r3808192343}

# Evaluation manager's discussion[^20]

Evaluators 1 and 2, along with the Happier Lives Institute (HLI)
authors, largely agree on the overall positive assessment of the report.

Both evaluators provided similar high overall assessment scores for the
report and generally recognized the report\'s methodology as
comprehensive, thorough, and transparent. Evaluator 1 described it as
\"rigorous and valuable,\" while Evaluator 2 appreciated HLI\'s
adherence to rigorous practices, high transparency, and clear reporting
of changes over time. The use of WELLBYs as a common outcome measure for
comparing interventions was implicitly or explicitly supported by both.
Evaluator 2 explicitly confirmed that using random effects in the
meta-analysis was the \"most appropriate choice,\" aligning with the
authors\' approach. There was also agreement, including from the
authors, that the report could benefit from clearer write-up and
additional visualizations, particularly on how analytical choices impact
results. Finally, both evaluators acknowledged that HLI\'s site visits
to both charities provided reassurance about their professional and
effective operations.

Their criticisms focus on specific methodological decisions. Evaluator 1
highlights the need for \"standardised adjustment rules\" and \"clearer
discussion of assumption impacts\" to address \"ad hoc adjustments\"
that might influence program rankings. Evaluator 2 emphasized that \"a
lot of decisions in the CEA\... appear arbitrary and subjective\" and
primarily recommended a \"multiverse approach\" to transparently map and
compare how different plausible choices affect the final estimates. The
HLI authors, in their response, acknowledged that subjectivity is
\"unavoidable\" in CEAs due to a lack of pre-existing academic consensus
and the need to make \"judgement calls\" for recommendations, defending
their choices as \"principled decisions\" with empirical bases rather
than \"pure subjective guesses\".

A main concern for Evaluator 1 was Friendship Bench\'s "very low
attendance (dosage)," which could significantly affect results if
different assumptions were made. Evaluator 2 was \"very surprised to see
an attendance of 1.12 out of 6 possible sessions,\" questioning the
assumption of large effects from few sessions, though acknowledging the
plausibility of psychoeducation\'s impact. HLI agreed it was a "major
source of uncertainty" but maintained this was plausible due to the
therapeutic nature of the first session and psychoeducation\'s role in
LMICs, defending their dosage adjustment as "conservative".

Evaluator 2 critiqued the removal of outliers (e.g., effect sizes \>2
SDs) and high-risk bias studies, arguing it seems "arbitrary" and that
it\'s "always better to include the full data" to transparently show the
impact of such exclusions. HLI defended these decisions, stating they do
provide detailed sensitivity analyses for these exclusions in the
appendix, and that removing them is a "conservative option" consistent
with other meta-analyses.

On the linear decay model used for effects over time, Evaluator 1
considered linear decay to zero a "conservative assumption". Evaluator 2
expressed surprise at the linear choice, suggesting exponential models
are more common in psychological phenomena and that linear decay might
lead to \"overestimation\". HLI explained the linear model was chosen
for simplicity and a clear stopping point, noting that quick tests with
exponential decay showed "little difference".

Evaluator 2 raised reservations about HLI appearing "more optimistic
than other evaluators," citing regression analysis from HLI\'s World
Happiness Report chapter that showed higher estimated WELLBYs by HLI.
HLI strongly disagreed, attributing differences to their deliberate
search for the most cost-effective charities, focus on low-income
settings, and downward adjustments that other evaluators do not use.

We recommend that readers consult the full evaluation reports and the
authors' detailed response before forming a final judgment.

## Issues meriting further evaluation and analysis[^21]

*​​We shared the relevant content with NotebookLM and asked it "Do the
evaluations address the questions considered in the \'bespoke notes\'
file? (under \"Why does it need (more) review? What are some key issues
and claims to vet?\") What issues are not considered in the
evaluations?" See Footnote for its answer to the latter question, along
with the author's response.*[^22]

In consideration of the evaluator's discussion and authors' responses we
suggest further evaluation of the following:

-   Are the choices made (outlier removal, linear decay, publication
    bias adjustments, etc.) indeed conservative in terms of
    *understating* the impact of the mental health interventions?

-   Did the authors' approaches substantially and faithfully follow the
    methods from their previous analysis?

*And some future steps for research and analysis:*

-   Mapping the full range of plausible analytic choices (using a
    multiverse or specification-curve analysis) could reveal how
    sensitive the headline cost-effectiveness ratios are to each
    assumption (further evaluators and replicators might also be able to
    do this, to some extent).

-   Interactive dashboards/a transparent interface would let readers
    adjust key parameters and observe the resulting WELLBY estimates in
    real time

-   Building on approaches for systematizing ad-hoc adjustments (see,
    e.g., Bettle 2023 [@nlsldieqw5t]), develop clearer guidelines for
    dosage discounts, lay-therapist penalties, and other contextual
    factors.

-   As Evaluator 2 notes, additional RCTs of both charities would
    naturally provide the strongest validation.

# Unjournal process notes[^23] 

## Why we chose this paper 

We chose this paper as part of [our
plan](https://forum.effectivealtruism.org/posts/GxDjyEXCZc3pthkBb/unjournal-evaluating-1-research-output-per-relevant-ea "null")
to evaluate at least one piece of research per relevant Effective
Altruism-funded or impact-aligned organization. (Note: HLI has not
received substantial funding from large EA-aligned organizations, but it
is impact-focused and highly active in the EA ecosystem). This paper was
also suggested (among other work) by HLI's founder Michael Plant in
response to our email. They note this work is a fourth iteration
refining this analysis. It makes direct actionable claims that are
directly relevant to effective philanthropy.

Footnote: chatGPT summary of our broader notes:[^24]

*Note that this discussion generally takes the appropriateness of the
WELLBY metric for granted; this is something we aim to explore in
further evaluations and our *[*Pivotal Questions
project.*](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/pivotal-questions "null")\

## Evaluation process

This was an involved process, with several iterations. As noted above,
we first contacted HLI, who suggested this work, after some discussion.

We sought evaluators with both methodological and contextual expertise
(see footnote), also trying to avoid those that seemed to have strong
preconceptions or takes about this issue.[^25]

The evaluators conferred amongst themselves before we shared this with
the authors. The authors prepared a private response, which we shared
with the evaluators. E1 responded (we have incorporated one response as
a footnote) and adjusted their report slightly; E2 chose not to respond
further. David Reinstein has also shared some sidebar notes and comments
on the pubs in this package in the PubPub interface. These notes should
not be interpreted as official Unjournal responses.

*This was evaluated as part ofd our Applied/Policy stream.*

#  References

1\. McGuire, J., Dupret, S., Dwyer, R., Plant, M., & Klapow, M. (2024).
The wellbeing cost-effectiveness of StrongMinds and Friendship Bench:
Combining a systematic review and meta-analysis with charity-related
data (Nov 2024 Update). Happier Lives Institute.
https://www.happierlivesinstitute.org/report/the-wellbeing-cost-effectiveness-of-strongminds-and-friendship-bench/

[^1]: We asked them to rank this paper "heuristically" as a percentile
    "relative to all serious research in the same area that you have
    encountered in the last three years." We requested they "consider
    all aspects of quality, credibility, importance to knowledge
    production, and importance to practice.

[^2]: See ranking tiers discussed
    [here](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#journal-ranking-tiers).

[^3]: Note: if you are reading this before, or soon after this has been
    publicly released, the ratings from this paper may not yet have been
    incorporated into that data presentation.

[^4]: Judge the quality of the research heuristically. Consider all
    aspects of quality, credibility, importance to knowledge production,
    and importance to practice.

[^5]: "Do the authors do a good job of (i) stating their main questions
    and claims, (ii) providing strong evidence and powerful approaches
    to inform these, and (iii) correctly characterizing the nature of
    their evidence?"\
    \
    This was on the newer form only

[^6]: To what extent does the project contribute to the field or to
    practice, particularly in ways that are directly or indirectly
    relevant to global priorities and impactful interventions?

[^7]: Are methods clearly justified and explained? Are methods and their
    underlying assumptions reasonable? Are the results likely to be
    robust to changes in the assumptions? Have the authors avoided bias
    and questionable research practices?

[^8]: Are concepts clearly defined? Is the reasoning transparent? Are
    conclusions consistent with the evidence (or formal proofs)
    presented? Are the data and/or analysis, including tables and
    figures, relevant to the argument?

[^9]: Transparency and logic are of high level but the communication and
    structure are at times disjointed.

[^10]: Would another researcher be able to replicate the analysis? Are
    the method and its details explained sufficiently? Is the source of
    the data clear? Is the data made as widely available as possible,
    with clear labeling and explanation? Do the authors provide
    resources that are likely to enable future research and
    meta-analysis?

[^11]: I didn't not see a way to examine the data or code. Many of the
    procedures are well described but some are at a very broad level.

[^12]: Does the paper consider real-world relevance and deal with policy
    and implementation questions? Are the setup, assumptions, and focus
    realistic and relevant to practitioners?

[^13]: The latter ratings were merged in the newer form\
    \
    "Are the paper's chosen topic and approach likely to be useful to
    global priorities, cause prioritization, and high-impact
    interventions?"

    "Does the paper consider real-world relevance and deal with policy
    and implementation questions? Are the setup, assumptions, and focus
    realistic? Do the authors report results that are relevant to
    practitioners? Do they provide useful quantified estimates (costs,
    benefits, etc.)?"

[^14]: Are the paper's chosen topic and approach likely to be useful to
    global priorities, cause prioritization, and high-impact
    interventions?

[^15]: The latter ratings were merged in the newer form\
    \
    "Are the paper's chosen topic and approach likely to be useful to
    global priorities, cause prioritization, and high-impact
    interventions?"

    "Does the paper consider real-world relevance and deal with policy
    and implementation questions? Are the setup, assumptions, and focus
    realistic? Do the authors report results that are relevant to
    practitioners? Do they provide useful quantified estimates (costs,
    benefits, etc.)?"

[^16]: I think the systematic review and meta-analysis (assuming the
    details in the prereg I could not verify are OK) can be published in
    a very good journal and my ratings are based on this. I am more
    hesitant and doubtful about the CEA component so do not mark this.

[^17]: The evaluator was given the following instructions:\
    \
    Identify the most important and impactful factual claim this
    research makes -- e.g., a binary claim or a point estimate or
    prediction.

    Please state the authors' claim precisely and quantitatively.
    Identify the source of the claim (i.e., cite the paper), and briefly
    mention the evidence underlying this. We encourage you to explain
    why you believe this claim is important, either here, or in the text
    of your report.

[^18]: Evaluators were asked: To what extent do you \*believe\* the
    claim you stated above? Feel free to express this either a. in terms
    of the probability of the claim being true, b. as a credible
    interval for the parameter being estimated, or c. however you feel
    comfortable.

[^19]: *We asked:*

    \[Optional\] What additional information, evidence, replication, or
    robustness check would make you substantially more (or less)
    confident in this claim?

    Feel free to refer to the main body of your evaluation here; you
    don\'t need to repeat yourself. Please specify how you would perform
    this robustness check (etc.) as precisely as you are willing. E.g.,
    if you suggest a particular estimation command in a statistical
    package, this could be very helpful for future robustness
    replication work.

[^20]: Evaluation managers: After the evaluations and author responses
    are in, you may want to give a brief synthesis and reflection on the
    research, the evaluations, and the response, considering the
    implications of these, future directions, etc. You can insert your
    own judgment here, if you like.

[^21]: Evaluation managers: Use this section to list or explain issues
    that need further scrutiny, potentially by ["independent Unjournal
    evaluators"](https://coda.io/d/_d0KBG3dSZCs/Crowdsourcing-independent-evaluations_sufF1).
    These may be issues you suggested in your "manager's notes"/"bespoke
    evaluation notes" or issues evaluators identified that they
    acknowledged they were not fully able to address.

[^22]: *NotebookLM on Issues not explicitly considered in the
    evaluations:*

    -   Does the methodology properly account for challenges in
        measuring subjective wellbeing across different populations and
        contexts? While the evaluations and HLI discuss external
        validity and adjust for certain contextual factors, a deep dive
        into the fundamental challenges of cross-cultural measurement of
        subjective wellbeing is not a primary focus.

    -   Are the WELLBY estimates robust to assumptions about the
        persistence of effects? While sensitivity analyses address the
        inclusion/exclusion of long-term follow-ups, the specific
        robustness of WELLBY estimates to different *models* of effect
        decay (beyond linear decay to zero) is not explicitly detailed
        in the evaluation excerpts.

    -   Time Horizon & Baselines (in the comparison to cash transfers):
        While the evaluations address the overall cost-effectiveness
        claim, a detailed examination of the consistency of assumed time
        horizons for benefits and the control of baseline conditions in
        the comparison with cash transfers is not a central point in
        these excerpts.

    *Author's response:*

    -   The 1st one is outside of the scope of the report.

    -   The 2nd one is a very good point. The modelling of longterm
        effects could be a whole paper. It is rather novel for
        meta-analyses to include this quantitative modelling of longterm
        effects. We do not mention it in our report, but we have briefly
        looked into exponential decay and that did not seem to change
        the results much.

    -   The 3rd one is difficult to read and understand. As mentioned
        above, the comparison to cash transfers isn't the core of our
        evaluation here. Our cash transfers have been evaluated in other
        reports in detail. We are more conservative (and sophisticated)
        for the analysis of psychotherapy. On the other hand, the data
        quality for cash transfers is higher.

[^23]: Evaluation managers: We usually put a brief discussion of why we
    prioritized this work and the evaluation process here. Please try to
    keep this concise: avoid boilerplate, profuse gratitude or flattery.
    There is no need to report on the parts of this process that worked
    normally.

[^24]: This paper presents a cost-effectiveness analysis of two major
    mental health interventions---Friendship Bench and
    StrongMinds---through the lens of *Wellbeing-Adjusted Life Years*
    (WELLBYs). The study combines a systematic review, meta-analysis,
    and charity-related data to estimate intervention effectiveness and
    cost per WELLBY.

    This work is particularly relevant as:

    -   It contributes to the emerging field of *wellbeing economics*,
        directly linking mental health interventions to a standardized
        measure of impact.

    -   It provides **comparative cost-effectiveness** estimates,
        including a comparison to cash transfers, an established
        benchmark in development economics.

    -   The **Happier Lives Institute (HLI)** is positioning this as its
        most refined iteration of their wellbeing-based
        cost-effectiveness approach, which may influence funding and
        policy decisions within Effective Altruism (EA) and global
        philanthropy.

    Given the increasing emphasis on subjective wellbeing (SWB) as an
    outcome measure in impact evaluation, this paper could have
    substantial implications for funding decisions in mental health and
    development economics.

[^25]: We looked for expertise in:

    -   Cost-effectiveness analysis & impact evaluation (to assess
        whether the conclusions follow from the methodology) 

    -   Wellbeing & subjective measures of welfare (to critically
        examine the use of WELLBYs and their validity)

    -   Meta-analysis & systematic reviews (to assess robustness of
        effect size estimation)

    -   Global health & mental health interventions (to ensure
        contextual accuracy of intervention effectiveness claims)

    -   Development economics & philanthropy strategy (to evaluate
        broader implications for intervention prioritization)

    -   RCT implementation and reporting
