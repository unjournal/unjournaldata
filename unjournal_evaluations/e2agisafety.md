---
article:
  doi: 10.21428/d28e8e57.9b0d8cda/cb512252
  elocation-id: e2agisafety
author:
- Evaluator 2
bibliography: /tmp/tmp-59kFNeSa74rS7g.json
copyright:
  link: "https://creativecommons.org/licenses/by/4.0/"
  text: Creative Commons Attribution 4.0 International License
  type: CC-BY
csl: /app/dist/server/server/utils/citations/citeStyles/apa-6th-edition.csl
date:
  day: 03
  month: 06
  year: 2025
journal:
  publisher-name: The Unjournal
  title: The Unjournal
link-citations: true
title: "Evaluation 2 of \"Towards best practices in AGI safety and
  governance: A survey of expert opinion\" for The Unjournal, Applied
  Stream"
uri: "https://unjournal.pubpub.org/pub/e2agisafety"
---

# Abstract 

This paper makes a valuable contribution to AGI safety by identifying
areas of consensus in best practices, with implications for
standards-setting. While the methodology is rigorous, more justification
is needed for the selection and classification of the 50 practices.
Potential biases---particularly AGI labs endorsing their own
practices---should be better addressed. Organizing and contextualizing
practices more clearly would aid practitioners. The interpretation of
results should in some occasions be caveated with what the agreement
gathered itself affords, and avoid going beyond that. Overall, this
paper makes a great first step toward best practices for AGI and sets
the grounds for important future work on the topic.

# Note: Applied and Policy Stream

This evaluation was done through our "applied and policy stream",
described
[here](https://docs.google.com/document/d/1RwkmGJtaOcryK-tJs3mrs6DIAj411YRnsauaj6EE6WY/edit#heading=h.o7xzpvh0h0b2 "null").
The ratings should not be directly compared to those in our main
academic stream.

# Summary Measures

We asked evaluators to give some overall assessments, in addition to
ratings across a range of criteria. *See the *[*evaluation summary
"metrics"*](https://unjournal.pubpub.org/pub/evalsumagisafety#metrics "null")*
for a more detailed breakdown of this. See these ratings in the context
of all Unjournal ratings, with some analysis, in our *[*data
presentation
here.*](https://unjournal.github.io/unjournaldata/chapters/evaluation_data_analysis.html#basic-presentation "null")[^1]*
*

+-------------------+-------------------+---+
|                   | **Rating**        | * |
|                   |                   | * |
|                   |                   | 9 |
|                   |                   | 0 |
|                   |                   | % |
|                   |                   | C |
|                   |                   | r |
|                   |                   | e |
|                   |                   | d |
|                   |                   | i |
|                   |                   | b |
|                   |                   | l |
|                   |                   | e |
|                   |                   | I |
|                   |                   | n |
|                   |                   | t |
|                   |                   | e |
|                   |                   | r |
|                   |                   | v |
|                   |                   | a |
|                   |                   | l |
|                   |                   | * |
|                   |                   | * |
+===================+===================+===+
| **Overall         | 75/100            | 6 |
| assessment **     |                   | 5 |
|                   |                   |   |
|                   |                   | - |
|                   |                   | 8 |
|                   |                   | 5 |
+-------------------+-------------------+---+

**Overall assessment: **We asked evaluators to rank this paper
"heuristically" as a percentile "relative to applied and policy research
you have read aiming at a similar audience, and with similar goals." We
requested they "consider all aspects of quality, credibility, importance
to future impactful applied research, and practical relevance and
usefulness."

# Written report

**Contribution**: This paper constitutes an important contribution to
the development of best practices in AGI safety and governance.
Importantly, it does so by identifying which practices already have
broad support and where more work is needed. As the authors suggest, the
findings also have important implications for standards-setting, by
highlighting emerging areas of agreement in best practices that can
inform standards processes.

## Remarks for further improvement

### \[1. Criteria for selection of statements\][^2]

The paper sets out to select a range of practices extracted from (1)
current practices at individual AGI labs, (2) planned practices at
individual labs, (3) proposals in the literature, and (4) discussion
with experts and colleagues. While the paper makes clear how these
practices were selected, more effort could go into explaining why they
were selected and why they are considered "best" practices. The paper
selects 50 "statements", which is quite a high number. It could be
useful, especially from a practitioner's perspective, to introduce an
inclusion/exclusion criterion to provide a better justification as to
why some statements are selected. Such criterion does not have to be
necessarily narrow, it can also just take into consideration excessive
overlap between statements. Alternatively, the selection of such a
number of best practices could be justified by wanting to ensure a
representative set, and thus wanting to include more rather than less
given their fast-changing and emerging nature. \

Additionally, the wealth of statements/practices presented may relate to
each other. For example, there might be trade-offs between the choice of
different practices (e.g., choosing between increasing levels of
external scrutiny and industry sharing of security information) or some
practices might be more specific instantiations of some more general
statements (e.g., notify affected parties could be a specific
instantiation of a more general practice like report safety incidents).
While the paper makes a first important step in identifying a broad set
of practices, it might be useful for practitioners to have a paragraph
where they are contextualised or elaborated upon a bit more.\

### \[2. Selection biases for "current practices"; categorization\]

The paper thoroughly presents its methods in section 2, showing great
rigour. While the section is very clear, some methodological choices
might introduce some important biases that are currently not fully
reflected in limitations. Importantly, the paper states that the
selected practices are extracted from (1) current practices at
individual AGI labs and (2) planned practices at individual labs, among
other sources. At the same time, results suggest that AGI labs show
general agreement on practices (even that "respondents from AGI labs had
significantly higher overall mean agreement ratings than respondents
from academia or civil society"), and even that on some items they
(somewhat surprisingly) showed more agreement on some individual
practices than other actors (e.g. the statement that AGI labs should
grant independent researchers access to deployed models was more
supported by AGI labs than academia and civil society, even though the
result was not statistically significant). \
These results might suggest a selection bias where statements selected
from labs practices are agreed on by labs themselves, and thus decrease
the value of what that agreement really means for practitioners who read
this research.[^3]

Another choice which needs further backing is the decision to cluster
"government" into the category "other", together with actors such as
consulting firms. While this is not an insensible choice overall, it
constrains the ability of the paper to elaborate on implications for
regulators (section 4.3) and thus, it misses an important opportunity.\

### \[3. Characterization of results\] 

The paper does a great job in presenting the results of its analysis in
the discussion. This section is very clear and well-written. As authors
recognize in their limitations, the number of statements as well as the
"high-level" at which they were presented to participants may suggest
that agreement on practices could indicate a host of different things.
While this is recognized in the paper as a limitation, it is important
that it is considered in the way that results are interpreted too. For
example, in the policy implications (section 4.3), the paper states that
its "findings suggest that AGI labs need to improve their risk
management practices. In particular, there seems to be room for
improvement when it comes to their risk governance."

\
While one can agree with such claim, it is difficult to see how this
conclusion can be reached from the paper's results. For example,
agreement (or lack thereof) on a set of practices by labs might mean
many more different things than a lack of work on those practices (e.g.
lack of agreement on a set of practices might mean that labs prefer an
alternative set of practices to achieve the same goal). In order for the
paper's results not to be misleading for practitioners, it is important
that implications do not go beyond the scope of the paper or that this
nuance is made clear.\

### \[Potential follow-up\]

In terms of future directions, the paper states that the paper was
accompanied by a workshop (Section 4.5). In the workshop, participants
were asked about identifying the main blockers for the creation of best
practices as well as open questions. One thing a follow-up workshop
could do is to invite some (if not all) of the participants to the
survey to delve deeper into some of the findings (e.g. look into the
potential "why" of some of the replies or try to prioritize between the
different elements that were voted on). Still, this is just a suggestion
and I believe that the workshop which took place is a great first step
into sketching future directions for the paper.

Overall, I think this paper is a great first step into developing best
practices for AGI and I wish the best to the authors, looking forward to
their future work.

# Evaluator details

1.  How long have you been in this field?

    -   For 8 years.

2.  How many proposals and papers have you evaluated?

    -   It is a bit hard to count, but perhaps an average of 5 per year.

[^1]: Note: if you are reading this before, or soon after this has been
    publicly released, the ratings from this paper may not yet have been
    incorporated into that data presentation.

[^2]: Manager: We added these bracketed subheaders

[^3]: Manager: Paragraph break added here\
