---
article:
  doi: 10.21428/d28e8e57.36ba6211
  elocation-id: evalsumscientificincentives
author:
- Daniel Lee
- Gary McDowell
- David Reinstein
bibliography: /tmp/tmp-60MSKqzSsdKt3r.json
copyright:
  link: "https://creativecommons.org/licenses/by/4.0/"
  text: Creative Commons Attribution 4.0 International License
  type: CC-BY
csl: /app/dist/server/server/utils/citations/citeStyles/apa-6th-edition.csl
date:
  day: 16
  month: 04
  year: 2025
journal:
  publisher-name: The Unjournal
  title: The Unjournal
link-citations: true
title: "Evaluation Summary and Metrics: \"Stagnation and Scientific
  Incentives\""
uri: "https://unjournal.pubpub.org/pub/evalsumscientificincentives"
---

# Abstract 

We organized one evaluation of the paper: \"Stagnation and Scientific
Incentives\"[@n4yi9p24k63]. The paper implies that this "stagnation" is
attributable to the rise of citation-based bibliometrics. To read this
evaluation, please see the link below. The evaluator argues that this
paper "lacks sufficient support \[for\] its claims", and "struggles in
terms of accuracy, novelty, and generalizability". He recommends "a
discussion of the difference between citation indices and journal lists
\[and\] ... how they impact scientist evaluations", "a comparison to
non-scientific fields", some "regression model\[s\] or statistical
tests" and "formal definitions of the terms at hand (e.g. scientific
play, exploration, breakthrough, giant, etc.)"

## **Evaluations**

1\. [Daniel
Lee](https://unjournal.pubpub.org/pub/e1scientificincentives/draft?access=igjun2uu "null")

# **Overall ratings**

We asked evaluators to provide overall assessments as well as ratings
for a range of specific criteria. * *

**I. Overall assessment **(See footnote[^1])

**II. Journal rank tier, normative rating (0-5): **On a 'scale of
journals', what 'quality of journal' should this be published in?[^2]
*Note: 0= lowest/none, 5= highest/best. *

+---+-------------------+---+
|   | **Overall         | * |
|   | assessment        | * |
|   | (0-100)**         | J |
|   |                   | o |
|   |                   | u |
|   |                   | r |
|   |                   | n |
|   |                   | a |
|   |                   | l |
|   |                   | r |
|   |                   | a |
|   |                   | n |
|   |                   | k |
|   |                   | t |
|   |                   | i |
|   |                   | e |
|   |                   | r |
|   |                   | , |
|   |                   | n |
|   |                   | o |
|   |                   | r |
|   |                   | m |
|   |                   | a |
|   |                   | t |
|   |                   | i |
|   |                   | v |
|   |                   | e |
|   |                   | r |
|   |                   | a |
|   |                   | t |
|   |                   | i |
|   |                   | n |
|   |                   | g |
|   |                   | ( |
|   |                   | 0 |
|   |                   | - |
|   |                   | 5 |
|   |                   | ) |
|   |                   | * |
|   |                   | * |
+===+===================+===+
| D | 70                | 3 |
| a |                   | . |
| n |                   | 0 |
| i |                   |   |
| e |                   |   |
| l |                   |   |
| L |                   |   |
| e |                   |   |
| e |                   |   |
+---+-------------------+---+

*See
"*[*Metrics*](https://unjournal.pubpub.org/pub/alatasevalsum/release/9#metrics-all-evaluators "null")*"
below for a more detailed breakdown of the evaluators' ratings across
several categories. To see these ratings in the context of all Unjournal
ratings, with some analysis, see our *[*data presentation
here.*](https://unjournal.github.io/unjournaldata/chapters/evaluation_data_analysis.html#basic-presentation "null")[^3]*
*

*See
*[*here*](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#metrics-overall-assessment-categories "null")*
for the current full evaluator guidelines, including further explanation
of the requested ratings.*

# Evaluation summaries

## Daniel Lee

I think this paper raises an excellent point, but lacks sufficient
support of its claims. I think the most important aspect of this paper
is its overall ethos to do right by science, but struggles in terms of
accuracy, novelty, and generalizability---all of which are discussed
further in my report below.

# Metrics

## Ratings

[See
here](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#quantitative-metrics "null")
for details on the categories below, and the guidance given to
evaluators.

+----------------------------------+-----------------+-----------------------+
|                                  | **Evaluator 1** |                       |
|                                  |                 |                       |
|                                  | Daniel Lee      |                       |
+==================================+=================+=======================+
| **Rating category**              | **Rating        | **90% CI **           |
|                                  | (0-100)**       |                       |
|                                  |                 | **(0-100)\* **        |
+----------------------------------+-----------------+-----------------------+
| Overall assessment[^4]           | 70              | (55, 85)              |
+----------------------------------+-----------------+-----------------------+
| Advancing knowledge and          | N/A[^6]         | (67, N/A)             |
| practice[^5]                     |                 |                       |
+----------------------------------+-----------------+-----------------------+
| Methods: Justification,          | N/A[^8]         | (25, N/A)             |
| reasonableness, validity,        |                 |                       |
| robustness[^7]                   |                 |                       |
+----------------------------------+-----------------+-----------------------+
| Logic & communication[^9]        | 80              | (70, 90)              |
+----------------------------------+-----------------+-----------------------+
| Open, collaborative,             | 50              | (40, 60)              |
| replicable[^10]                  |                 |                       |
+----------------------------------+-----------------+-----------------------+
| Real-world relevance [^11],[^12] | 50              | (40, 60)              |
+----------------------------------+-----------------+-----------------------+
| Relevance to global              | 50              | (40, 60)              |
| priorities[^13], [^14]           |                 |                       |
+----------------------------------+-----------------+-----------------------+

## Journal ranking tiers

[See
here](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#journal-ranking-tiers "null")
for more details on these tiers.

+-------------------------------+--------------+-----------------------------+
|                               | **Evaluator  |                             |
|                               | 1**          |                             |
|                               |              |                             |
|                               | Daniel Lee   |                             |
+===============================+==============+=============================+
| **Judgment**                  | **Ranking    | **90% CI **                 |
|                               | tier (0-5)** |                             |
+-------------------------------+--------------+-----------------------------+
| On a 'scale of journals',     | 3.0          | (2.3, 3.7)                  |
| what 'quality of journal'     |              |                             |
| *should* this be published    |              |                             |
| in?                           |              |                             |
+-------------------------------+--------------+-----------------------------+
| What 'quality journal' do you | 3.5          | (3.0, 4.0)                  |
| expect this work *will* be    |              |                             |
| published in?                 |              |                             |
+-------------------------------+--------------+-----------------------------+
| [See                          | *We          |                             |
| here](https://                | summarize    |                             |
| globalimpact.gitbook.io/the-u | these as:*   |                             |
| njournal-project-and-communic |              |                             |
| ation-space/policies-projects | -   0.0:     |                             |
| -evaluation-workflow/evaluati |              |                             |
| on/guidelines-for-evaluators# |   Marginally |                             |
| journal-ranking-tiers "null") |     respec   |                             |
| for more details on these     | table/Little |                             |
| tiers.                        |     to no    |                             |
|                               |     value    |                             |
|                               |              |                             |
|                               | -   1.0:     |                             |
|                               |              |                             |
|                               |  OK/Somewhat |                             |
|                               |     valuable |                             |
|                               |              |                             |
|                               | -   2.0:     |                             |
|                               |     Marginal |                             |
|                               |     B-jo     |                             |
|                               | urnal/Decent |                             |
|                               |     field    |                             |
|                               |     journal  |                             |
|                               |              |                             |
|                               | -   3.0: Top |                             |
|                               |     B-jo     |                             |
|                               | urnal/Strong |                             |
|                               |     field    |                             |
|                               |     journal  |                             |
|                               |              |                             |
|                               | -   4.0:     |                             |
|                               |     Marginal |                             |
|                               |     A        |                             |
|                               | -Journal/Top |                             |
|                               |     field    |                             |
|                               |     journal  |                             |
|                               |              |                             |
|                               | -   5.0:     |                             |
|                               |     A        |                             |
|                               | -journal/Top |                             |
|                               |     journal  |                             |
+-------------------------------+--------------+-----------------------------+

# Claim identification and assessment (summary)

For *the full discussions, see the* *corresponding sections in each
linked evaluation.*

+-------------------+------------------------+----------------------------+--------------------------------------------+
|                   | **Main research        | **Belief in claim**[^16]   | **Suggested robustness checks**[^17]       |
|                   | claim**[^15]           |                            |                                            |
+===================+========================+============================+============================================+
| **Evaluator 1 **  | The stagnation in      | I believe this is a        | 1a) a discussion of the difference between |
|                   | science is correlated  | credible claim, but        | citation indices and journal lists and 1b) |
| Daniel Lee        | with the rise of       | requires more to be a      | a discussion of how they impact scientist  |
|                   | bibliometrics and      | believable claim (see      | evaluations \                              |
|                   | other citation         | next)                      | 2) a comparison to non-scientific fields \ |
|                   | measurements           |                            | 3) any sort of regression model or         |
|                   |                        |                            | statistical test \                         |
|                   |                        |                            | 4) formal definitions of the terms at hand |
|                   |                        |                            | (e.g. scientific play, exploration,        |
|                   |                        |                            | breakthrough, giant, etc.)                 |
+-------------------+------------------------+----------------------------+--------------------------------------------+

# Evaluation manager's discussion[^18]

# Unjournal process notes (\~optional)[^19] 

## Why we chose this paper 

*OpenAI summary, leveraging our notes*

> This paper presents a novel critique of how current scientific
> incentives---specifically the emphasis on citation counts---have
> contributed to scientific stagnation by discouraging exploration in
> favor of incremental research. The authors argue against the
> \"vanishing secrets\" theory, instead positing that stagnation results
> from structural incentive changes due to the \"citation revolution.\"

*From our internal notes*\

> The paper was discussed in an "EA Forum post'[
> here](https://forum.effectivealtruism.org/posts/e5crqEmkaSf6hB5TR/improving-incentives-in-research-could-we-topple-the-h-index "null"),
> citing the global priorities relevance of novel research.
>
> I think it has interesting ramifications on long-run scientific
> progress, and I wonder if there's a way of framing AI alignment in the
> early stages of this, which may have spillovers.

> \[This is relevant\] "in relation to the increased focus of NIH's
> iCite, based on the Relative Citation Ration (a metric that puts your
> citation numbers in context with your field)"

> This paper is itself fairly heavily cited.

## Evaluation process

We were looking for evaluators with a combination of expertise (see
footnote).[^20] We have had some difficulty finding evaluators in these
areas, particularly innovation economics. After finding a qualified
evaluator, we shared [the linked "bespoke evaluation
notes"](https://docs.google.com/document/d/1vO8JijAEgEtjD4a70iK2vs28XvXqjuD15jqtYaHYr4g/edit?tab=t.0#heading=h.7havg98i8l3f "null")
with him, offering context and some suggestions for aspects to evaluate.
Including:

> Much of the argument relies on historical cases and theoretical
> models, Do their sources/analogies actually support their model? Does
> it provide sufficient justification for their proposals?
>
> The claim they make is important, but is their approach rigorous?
>
> What is their definition of 'scientific impact' -- is it precise and
> meaningful? Are the measures of novelty and impact sufficiently
> distinct and valid?

These questions reflected some doubts about how serious and rigorous
this work was intended to be. The evaluation reinforced some of these
concerns (e.g., "many of the claims it makes (both trivial and
significant) feel asserted rather than proven or demonstrated"). Given
this, we decided not to seek a second evaluation.

# References

1\. Bhattacharya, J., & Packalen, M. (2020). Stagnation and Scientific
Incentives. NBER Working Paper 26865. https://doi.org/10.3386/w26865

[^1]: We asked them to rank this paper "heuristically" as a percentile
    "relative to all serious research in the same area that you have
    encountered in the last three years." We requested they "consider
    all aspects of quality, credibility, importance to knowledge
    production, and importance to practice.

[^2]: See ranking tiers discussed
    [here](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#journal-ranking-tiers).

[^3]: Note: if you are reading this before, or soon after this has been
    publicly released, the ratings from this paper may not yet have been
    incorporated into that data presentation.

[^4]: Judge the quality of the research heuristically. Consider all
    aspects of quality, credibility, importance to knowledge production,
    and importance to practice.

[^5]: To what extent does the project contribute to the field or to
    practice, particularly in ways that are directly or indirectly
    relevant to global priorities and impactful interventions?

[^6]: Manager: We're clarifying with the evaluator as to whether the
    figure entered was meant as a midpoint or a lower bound

[^7]: Are methods clearly justified and explained? Are methods and their
    underlying assumptions reasonable? Are the results likely to be
    robust to changes in the assumptions? Have the authors avoided bias
    and questionable research practices?

[^8]: Manager: We're clarifying with the evaluator as to whether the
    figure entered was meant as a midpoint or a lower bound

[^9]: Are concepts clearly defined? Is the reasoning transparent? Are
    conclusions consistent with the evidence (or formal proofs)
    presented? Are the data and/or analysis, including tables and
    figures, relevant to the argument?

[^10]: Would another researcher be able to replicate the analysis? Are
    the method and its details explained sufficiently? Is the source of
    the data clear? Is the data made as widely available as possible,
    with clear labeling and explanation? Do the authors provide
    resources that are likely to enable future research and
    meta-analysis?

[^11]: Does the paper consider real-world relevance and deal with policy
    and implementation questions? Are the setup, assumptions, and focus
    realistic and relevant to practitioners?

[^12]: The latter ratings were merged in the newer form\
    \
    "Are the paper's chosen topic and approach likely to be useful to
    global priorities, cause prioritization, and high-impact
    interventions?"

    "Does the paper consider real-world relevance and deal with policy
    and implementation questions? Are the setup, assumptions, and focus
    realistic? Do the authors report results that are relevant to
    practitioners? Do they provide useful quantified estimates (costs,
    benefits, etc.)?"

[^13]: Are the paper's chosen topic and approach likely to be useful to
    global priorities, cause prioritization, and high-impact
    interventions?

[^14]: The latter ratings were merged in the newer form\
    \
    "Are the paper's chosen topic and approach likely to be useful to
    global priorities, cause prioritization, and high-impact
    interventions?"

    "Does the paper consider real-world relevance and deal with policy
    and implementation questions? Are the setup, assumptions, and focus
    realistic? Do the authors report results that are relevant to
    practitioners? Do they provide useful quantified estimates (costs,
    benefits, etc.)?"

[^15]: The evaluator was given the following instructions:\
    \
    Identify the most important and impactful factual claim this
    research makes -- e.g., a binary claim or a point estimate or
    prediction.

    Please state the authors' claim precisely and quantitatively.
    Identify the source of the claim (i.e., cite the paper), and briefly
    mention the evidence underlying this. We encourage you to explain
    why you believe this claim is important, either here, or in the text
    of your report.

[^16]: Evaluators were asked: To what extent do you \*believe\* the
    claim you stated above? Feel free to express this either a. in terms
    of the probability of the claim being true, b. as a credible
    interval for the parameter being estimated, or c. however you feel
    comfortable.

[^17]: *We asked:*

    \[Optional\] What additional information, evidence, replication, or
    robustness check would make you substantially more (or less)
    confident in this claim?

    Feel free to refer to the main body of your evaluation here; you
    don\'t need to repeat yourself. Please specify how you would perform
    this robustness check (etc.) as precisely as you are willing. E.g.,
    if you suggest a particular estimation command in a statistical
    package, this could be very helpful for future robustness
    replication work.

[^18]: Evaluation managers: After the evaluations and author responses
    are in, you may want to give a brief synthesis and reflection on the
    research, the evaluations, and the response, considering the
    implications of these, future directions, etc. You can insert your
    own judgment here, if you like.

[^19]: Evaluation managers: We usually put a brief discussion of why we
    prioritized this work and the evaluation process here. Please try to
    keep this concise: avoid boilerplate, profuse gratitude or flattery.
    There is no need to report on the parts of this process that worked
    normally.

[^20]:
    -   Textual analysis of research publications to measure scientific
        novelty,

    ```{=html}
    <!-- -->
    ```
    -   Empirical analysis of biomedical research papers to demonstrate
        the relationship between novelty and scientific influence

    -   \"Novelty Measurement"

    -   A simple model of the lifecycle of a scientific idea is used to
        explain the relationship between scientific effort, impact, and
        the different phases of exploration, breakthrough, and
        incremental advance

    -   Bibliometrics/Scientometrics

    -   Economics of Science/Innovation

    -   History of Science

    -   Biomedical Research (familiarity)
