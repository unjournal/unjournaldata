---
article:
  doi: 10.21428/d28e8e57.b3e3867e
  elocation-id: evalsumforecastingexistentialrisk
author:
- Evaluator 1
- Evaluator 2
- David Reinstein
- Anca Hanea
bibliography: /tmp/tmp-55dIqbE4jyoPyf.json
copyright:
  link: "https://creativecommons.org/licenses/by/4.0/"
  text: Creative Commons Attribution 4.0 International License
  type: CC-BY
csl: /app/dist/server/server/utils/citations/citeStyles/apa-6th-edition.csl
date:
  day: 23
  month: 08
  year: 2024
journal:
  publisher-name: The Unjournal
  title: The Unjournal
link-citations: true
title: "Evaluation Summary and Metrics: \"Forecasting Existential Risks:
  Evidence from a Long Run Forecasting Tournament\", Applied Stream"
uri: "https://unjournal.pubpub.org/pub/evalsumforecastingexistentialrisk"
---

# Abstract 

We organized two evaluations of the paper "Forecasting Existential
Risks: Evidence from a Long Run Forecasting
Tournament"[@https://static1.squarespace.com/static/635693acf15a3e2a14a56a4a/t/64f0a7838ccbf43b6b5ee40c/1693493128111/XPT.pdf].
This paper was evaluated as part of our "applied and policy stream",
described
[here](https://docs.google.com/document/d/1RwkmGJtaOcryK-tJs3mrs6DIAj411YRnsauaj6EE6WY/edit#heading=h.o7xzpvh0h0b2 "null").
To read these evaluations, please see the links below.

> *23 Sep 2024 note: *The above link to the paper is not currently
> working. As a backup, see
> [here](https://forecastingresearch.org/s/XPT.pdf "null") or the
> ('Wayback machine') Web Archive
> [here](https://web.archive.org/web/20240330092020/https://static1.squarespace.com/static/635693acf15a3e2a14a56a4a/t/64f0a7838ccbf43b6b5ee40c/1693493128111/XPT.pdf "null").

## **Evaluations**

1\. [Anonymous evaluation
1](https://unjournal.pubpub.org/pub/e1forecastingexistentialrisk "null")

2\. [Anonymous evaluation
2](https://unjournal.pubpub.org/pub/e2forecastingexistentialrisk "null")

# **Overall ratings**

### Note: Applied and Policy Stream

This paper was evaluated as part of our "applied and policy stream",
described
[here](https://docs.google.com/document/d/1RwkmGJtaOcryK-tJs3mrs6DIAj411YRnsauaj6EE6WY/edit#heading=h.o7xzpvh0h0b2 "null").
The ratings should not be directly compared to those in our main
academic stream.

We asked evaluators to provide overall assessments as well as ratings
for a range of specific criteria. * Note: *The second evaluator was
given an updated form geared specifically for our [Applied and Policy
stream](https://docs.google.com/document/d/1RwkmGJtaOcryK-tJs3mrs6DIAj411YRnsauaj6EE6WY/edit?usp=sharing "null"),
while the first evaluator used the (very similar) interface intended for
the academic stream.

**I. Overall assessment: **We asked them to rank this paper
"heuristically" as a percentile "relative to all serious research in the
same area that you have encountered in the last three years." We
requested they "consider all aspects of quality, credibility, importance
to knowledge production, and importance to practice."

**II. Journal rank tier, normative rating (0-5):**[^1]** **On a 'scale
of journals', what 'quality of journal' should this be published in?
(See ranking tiers discussed
[here](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#journal-ranking-tiers "null").)
*Note: 0= lowest/none, 5= highest/best.*

+---+-------------------+---+
|   | **Overall         | * |
|   | assessment        | * |
|   | (0-100)**         | J |
|   |                   | o |
|   |                   | u |
|   |                   | r |
|   |                   | n |
|   |                   | a |
|   |                   | l |
|   |                   | r |
|   |                   | a |
|   |                   | n |
|   |                   | k |
|   |                   | t |
|   |                   | i |
|   |                   | e |
|   |                   | r |
|   |                   | , |
|   |                   | n |
|   |                   | o |
|   |                   | r |
|   |                   | m |
|   |                   | a |
|   |                   | t |
|   |                   | i |
|   |                   | v |
|   |                   | e |
|   |                   | r |
|   |                   | a |
|   |                   | t |
|   |                   | i |
|   |                   | n |
|   |                   | g |
|   |                   | ( |
|   |                   | 0 |
|   |                   | - |
|   |                   | 5 |
|   |                   | ) |
|   |                   | * |
|   |                   | * |
+===+===================+===+
| A | 75                | 4 |
| n |                   | . |
| o |                   | 0 |
| n |                   |   |
| y |                   |   |
| m |                   |   |
| o |                   |   |
| u |                   |   |
| s |                   |   |
| e |                   |   |
| v |                   |   |
| a |                   |   |
| l |                   |   |
| u |                   |   |
| a |                   |   |
| t |                   |   |
| i |                   |   |
| o |                   |   |
| n |                   |   |
| 1 |                   |   |
+---+-------------------+---+
| A | 80                | N |
| n |                   | / |
| o |                   | A |
| n |                   |   |
| y |                   |   |
| m |                   |   |
| o |                   |   |
| u |                   |   |
| s |                   |   |
| e |                   |   |
| v |                   |   |
| a |                   |   |
| l |                   |   |
| u |                   |   |
| a |                   |   |
| t |                   |   |
| i |                   |   |
| o |                   |   |
| n |                   |   |
| 2 |                   |   |
+---+-------------------+---+

*See
"*[*Metrics*](https://unjournal.pubpub.org/pub/evalsumforecastingexistentialrisk#metrics "null")*"
below for a more detailed breakdown of the evaluators' ratings across
several categories. To see these ratings in the context of all Unjournal
ratings, with some analysis, see our *[*data presentation
here.*](https://unjournal.github.io/unjournaldata/chapters/evaluation_data_analysis.html#basic-presentation "null")[^2]*
*

*See
*[*here*](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#metrics-overall-assessment-categories "null")*
for the current full evaluator guidelines, including further explanation
of the requested ratings.*[^3]

# Evaluation summaries

## Anonymous evaluator 1

I really appreciate the team's ambitions for tackling difficult
long-term forecasts of important existential risks. Just as importantly,
they are exploring how experts might update their forecasts when
confronted with arguments from peers. Of course there are many
methodological challenges to credibly eliciting forecasts for events far
into the future, and the researchers do their best to meet them, but it
might still fall short. The policy implications of the results so far
could also be further clarified.

## Anonymous evaluator 2

This report considers a forecasting tournament in which experts in
relevant domains and super-forecasters from previous tournaments are
asked to predict a large number of quantities of interest related to
existential threats to human existence on Earth. A third group, taken
from the public more widely, is also consulted. The tournament structure
involves several rounds with interaction within and between groups in
online forums, sharing anonymised predictions and rationales, rewards
for active individuals and assessment of the quality of predictions in
terms of how well individuals could predict the values given by others
and accuracy on quantities to be revealed over short time horizons.\
\
As an exercise it is very interesting and could potentially be valuable,
both to society in general and perhaps decision makers at various
levels. It is written appropriately for a relatively non-technical
audience, with most of the detail contained in the appendices. The
methodology used is consistent with best practice in the literature for
this type of tournament, although as the report points out there are
question areas where they had to make decisions based on inconclusive
research such as predictions for low probability events. Overall, I
enjoyed reading the document and found that the summaries do for the
most part match the data collected. I do think the report would benefit
from revision, based on the comments in my full report.

# Metrics

## Ratings

+----------------------------------+-------------+----------+--------------+
|                                  | **Evaluator |          | **Evaluator  |
|                                  | 1\***       |          | 2**          |
|                                  |             |          |              |
|                                  | Anonymous   |          | Anonymous    |
+==================================+=============+==========+==============+
| **Rating category\***            | **Rating    | **90% CI | **Rating     |
|                                  | (0-100)**   | **       | (0-100)**    |
|                                  |             |          |              |
|                                  |             | **(      |              |
|                                  |             | 0-100)\* |              |
|                                  |             | **       |              |
+----------------------------------+-------------+----------+--------------+
| Overall assessment[^4]           | 75          | (60, 90) | 80           |
+----------------------------------+-------------+----------+--------------+
| Advancing knowledge and          | 75          | (60, 90) | 85           |
| practice[^5]                     |             |          |              |
+----------------------------------+-------------+----------+--------------+
| Methods: Justification,          | 70          | (60, 80) | 75           |
| reasonableness, validity,        |             |          |              |
| robustness[^6]                   |             |          |              |
+----------------------------------+-------------+----------+--------------+
| Logic & communication[^7]        | 80          | (70, 90) | 75           |
+----------------------------------+-------------+----------+--------------+
| Open, collaborative,             | 70          | (60, 80) | 85           |
| replicable[^8]                   |             |          |              |
+----------------------------------+-------------+----------+--------------+
| Real-world relevance             | 90          | (85, 95) | 95           |
| [^9]/Relevance to global         |             |          |              |
| priorities[^10] \*\*             |             |          |              |
+----------------------------------+-------------+----------+--------------+

-   Evaluator 1 was given a form following the categories described
    below, and explained in detail [
    here](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#quantitative-metrics "null"),
    with guidance. Evaluator 2 was given the updated form
    [here](https://coda.io/form/Unjournal-evaluation-form-applied-stream_dkjUPyzvHoH "null"),
    with slightly different categories and guidelines, aimed at our
    [Applied
    stream](https://docs.google.com/document/d/1RwkmGJtaOcryK-tJs3mrs6DIAj411YRnsauaj6EE6WY/edit?usp=sharing "null").
    Evaluator 2 opted to give midpoint ratings only, not CIs.

\*\* The final categories (Real-world.../Relevance) were separate for
Evaluator 1 (although they gave the same ratings for both). These were
combined for Evaluator 2, as per our new evaluation template.

## Journal ranking tiers

[See
here](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#journal-ranking-tiers "null")
for more details on these tiers.[^11]

+-------------------------------+-------------+-------------------------------+
|                               | **Evaluator |                               |
|                               | 1**         |                               |
|                               |             |                               |
|                               | Anonymous   |                               |
+===============================+=============+===============================+
| **Judgment**                  | **Ranking   | **90% CI **                   |
|                               | tier        |                               |
|                               | (0-5)**     |                               |
+-------------------------------+-------------+-------------------------------+
| On a 'scale of journals',     | 4.0         | (3.5, 5.0)                    |
| what 'quality of journal'     |             |                               |
| *should* this be published    |             |                               |
| in?                           |             |                               |
+-------------------------------+-------------+-------------------------------+
| What 'quality journal' do you | 4.0         | (3.5, 5.0)                    |
| expect this work *will* be    |             |                               |
| published in?                 |             |                               |
+-------------------------------+-------------+-------------------------------+
| [See                          | *We         |                               |
| here](https://                | summarize   |                               |
| globalimpact.gitbook.io/the-u | these as:*  |                               |
| njournal-project-and-communic |             |                               |
| ation-space/policies-projects | -   0.0:    |                               |
| -evaluation-workflow/evaluati |             |                               |
| on/guidelines-for-evaluators# |  Marginally |                               |
| journal-ranking-tiers "null") |     respect |                               |
| for more details on these     | able/Little |                               |
| tiers.                        |     to no   |                               |
|                               |     value   |                               |
|                               |             |                               |
|                               | -   1.0:    |                               |
|                               |             |                               |
|                               | OK/Somewhat |                               |
|                               |             |                               |
|                               |    valuable |                               |
|                               |             |                               |
|                               | -   2.0:    |                               |
|                               |             |                               |
|                               |    Marginal |                               |
|                               |     B-jou   |                               |
|                               | rnal/Decent |                               |
|                               |     field   |                               |
|                               |     journal |                               |
|                               |             |                               |
|                               | -   3.0:    |                               |
|                               |     Top     |                               |
|                               |     B-jou   |                               |
|                               | rnal/Strong |                               |
|                               |     field   |                               |
|                               |     journal |                               |
|                               |             |                               |
|                               | -   4.0:    |                               |
|                               |             |                               |
|                               |    Marginal |                               |
|                               |     A-      |                               |
|                               | Journal/Top |                               |
|                               |     field   |                               |
|                               |     journal |                               |
|                               |             |                               |
|                               | -   5.0:    |                               |
|                               |     A-      |                               |
|                               | journal/Top |                               |
|                               |     journal |                               |
+-------------------------------+-------------+-------------------------------+

# Evaluation manager's discussion 

## Anca Hanea's comments and summary 

### The Research

-   What will the next century bring in terms of grave risks humanity
    faces and how \[do\] "we"\* evaluate and perceive such risks
    (nuclear weapons, pandemics, AI)?

    -   \*("We" = ) college graduates, specialists, and superforecasters
        (people who are known for accurate predictions of future events)
        were involved in a forecasting tournament in a few experimental
        conditions

-   The tournament followed state of the art guidance on interaction
    between and within groups, anonymized predictions and rationales,
    incentives

-   Summary measures were considered when aggregating the different
    groups predictions:, e.g., the median expert/superforecaster
    predicted a 6%/1% chance of human extinction by 2100

-   Correlations between answers across topics, within groups  were
    considered (but not properly measured)

-   The benefits of rationales and discussion were considered

### The Evaluations

-   Both evaluators applauded the initiative and outlined the difficulty
    of the task at hand.

-   Data sharing was suggested

-   Criticism of the design and implementation in terms of questions
    framing, definition of expertise, lack of training, rare events
    description

-   Criticism of the statistical/quant analysis: CI not capturing the
    uncertainty properly, dependence measures not estimated, statistical
    inference not performed, aggregation methods not appropriate

### Impact

-   Interesting and potentially valuable as an exercise and experimental
    design

-   \[Hanea perceives the\] Most impact in \[terms of the insights
    into\] designing and identifying challenges and complications with
    such predictions

-   Impact through the lessons learned in analysing such data, framing
    long term forecasts in existential risks

-   Policy implications may be huge if such forecasts can prove reliable
    (and we can find proxies for reliability)

-   Further analysis, clarification and exploration needed before impact
    can be claimed

## Why we prioritized this paper (in brief)

1.  Potential to provide credible *measures* of existential and
    catastrophic risk, and insight into key debates (e.g. over AI risk)

2.  Already seems to be influencing other analyses and decisions (such
    as Clancy 2023 [@https://doi.org/10.48550/arxiv.2312.14289])

3.  The XPT data and results seem likely to be used widely in modeling
    for global prioritization

4.  Understanding and harnessing expert judgment is key to other

5.  Understanding persuasion and consensus-building may help foster
    cooperation on global issues

## Author engagement, suggestions for evaluators

Ezra Karger supported our efforts, and offered some suggestions for
particular areas they would like feedback. We shared these suggestions
for feedback, along with a detailed list of our own suggestions, in the
[Bespoke Evaluation Notes linked
here](https://docs.google.com/document/d/1AZCj7Sp4PZbxmmhvapWUgCiSUd5QD0V997B7QRwknD8/edit#heading=h.7havg98i8l3f "null").
The author team was particularly concerned with the importance of this
work, the usefulness of the data, and the characterization of the
relevant debates. Also see the authors' [ EA Forum
post.](https://forum.effectivealtruism.org/s/b3AtJeBggrjfDKCyj/p/76r25fSByRiNa7Wos "null")

The authors have not provided a written response to these evaluations.
They are invited to do so, and if they do, we will integrate it here.

## How we chose the evaluators

We sought evaluators with expertise and experience in

1.  Expert elicitation and aggregation

2.  Rare events elicitation methods

3.  Existential and catastrophic risks in particular areas, and the
    quantification of this (Note: neither of the evaluators we recruited
    had particular expertise in the specific risk areas).

## Issues meriting further evaluation

The evaluators (especially Evaluator 2) addressed many of the suggested
issues
([here](https://docs.google.com/document/d/1AZCj7Sp4PZbxmmhvapWUgCiSUd5QD0V997B7QRwknD8/edit#heading=h.7havg98i8l3f "null")).
However, this was a long paper, and an extensive list. We suspect some
issues merit further detailed evaluation
[@{E.g., as part of the Unjournal’s “Independent Evaluations” initiative.}],
including...

**Data sharing: **Is the data shared in a clear and useful way? How
could it be made more useful?

**Design and implementation, context**

-   Are they correctly describing the debates in these spaces? Did they
    ask the right questions?  Were the prediction questions well-framed?

-   Was the choice of 'experts' reasonable?  Were these and other groups
    recruited in ways that would tilt them towards one side or the
    other, or towards being intransigent?

-   Could there be substantial anchoring behavior as a result of their
    displaying the "Prior Forecasts"?

-   Did they do a good job of framing of 'questions about how they would
    allocate resources to mitigate potential risks' (Note some prior
    literature on 'eliciting policy choices' from the public and
    policymaker participants.)

-   What specifically are the most appropriate methods for eliciting
    these rare event forecasts?

**Statistical/quantitative analysis**

-   Are their aggregation methods appropriate? If not, what approach
    would be more approproate.?

-   Lack of statistical modeling inference (this was *noted* by
    evaluators; further evaluators could propose or implement specific
    approaches)

-   Did they adequately consider the potential for attrition bias?

# References

1\. Karger, E., Rosenberg, J., Jacobs, Z., Hickman, M., Hadshar, R.,
Gamin, K., Smith, T., Williams, B., McCaslin, T., & Tetlock, P. (2023).
Forecasting Existential Risks: Evidence from a Long-Run Forecasting
Tournament. Forecasting Research Institute.
https://forecastingresearch.org/publications (PDF:
https://static1.squarespace.com/static/635693acf15a3e2a14a56a4a/t/64f0a7838ccbf43b6b5ee40c/1693493128111/XPT.pdf)

\
\

[^1]: See "1 Mar 2024 note" above.

[^2]: Note: if you are reading this before, or soon after this has been
    publicly released, the ratings from this paper may not yet have been
    incorporated into that data presentation.

[^3]: See "1 Mar 2024 note" above. See
    [here](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators/why-these-guidelines#adjustments-to-metrics-and-guidelines-previous-presentations)
    to learn how this changed, and to see the earlier instructions.

[^4]: Judge the quality of the research heuristically. Consider all
    aspects of quality, credibility, importance to knowledge production,
    and importance to practice.

[^5]: To what extent does the project contribute to the field or to
    practice, particularly in ways that are directly or indirectly
    relevant to global priorities and impactful interventions?

[^6]: Are methods clearly justified and explained? Are methods and their
    underlying assumptions reasonable? Are the results likely to be
    robust to changes in the assumptions? Have the authors avoided bias
    and questionable research practices?

[^7]: Are concepts clearly defined? Is the reasoning transparent? Are
    conclusions consistent with the evidence (or formal proofs)
    presented? Are the data and/or analysis, including tables and
    figures, relevant to the argument?

[^8]: Would another researcher be able to replicate the analysis? Are
    the method and its details explained sufficiently? Is the source of
    the data clear? Is the data made as widely available as possible,
    with clear labeling and explanation? Do the authors provide
    resources that are likely to enable future research and
    meta-analysis?

[^9]: Does the paper consider real-world relevance and deal with policy
    and implementation questions? Are the setup, assumptions, and focus
    realistic and relevant to practitioners?

[^10]: Are the paper's chosen topic and approach likely to be useful to
    global priorities, cause prioritization, and high-impact
    interventions?

[^11]: This ranking tier rating is de-prioritized in our applied stream
    (thus Evaluator 2, using the new form, chose not to provide it).
