---
affiliation:
- id: 0
  organization: PhD Candidate at the University of Michigan School of
    Information
- id: 1
  organization: University of Wisconsin-Madison
- id: 2
  organization: University of California San Diego
article:
  doi: 10.21428/d28e8e57.df86328a
  elocation-id: evalsumchooseyourmoments
author:
- James M. Zumel Dumlao
- B. Ian Hutchins
- David Reinstein
- Joshua Graff Zivin
bibliography: /tmp/tmp-55r53E7NSvVBdl.json
copyright:
  link: "https://creativecommons.org/licenses/by/4.0/"
  text: Creative Commons Attribution 4.0 International License
  type: CC-BY
csl: /app/dist/server/server/utils/citations/citeStyles/apa-6th-edition.csl
date:
  day: 01
  month: 06
  year: 2025
journal:
  publisher-name: The Unjournal
  title: The Unjournal
link-citations: true
title: "Evaluation Summary and Metrics: \"Choose Your Moments: NIH Peer
  Review and Scientific Risk Taking\""
uri: "https://unjournal.pubpub.org/pub/evalsumchooseyourmoments"
---

# Abstract 

We organized two evaluations of the paper: \"Choose Your Moments: NIH
Peer Review and Scientific Risk Taking\"[@n5te1sjtu5f]. The evaluators
identify the paper's claim that "scientists have a preference for
'dissensus' among reviewers' scores" and are largely convinced by this.
However, the first evaluator (Dumlao) is less confident in the claim
that "higher score variance is correlated with higher-risk research with
greater potential to have positive impacts for the public." The
evaluations are positive overall, praising many aspects of the paper's
design ("the balanced incomplete block design approach is well-suited to
the ... internal NIH data"). Their ratings suggest the paper merits
publication in a "top B-journal", a "marginal A-Journal" and possibly a
"top journal". Both evaluators suggest ways to make the paper more
credible and useful (e.g., inter-rater reliability as a robustness
check). The second evaluator (Hutchins) provides particular contextual
insights ("the assertion that grants can be resubmitted twice is out of
date") and suggestions to improve communication and impact ("asterisks
denoting ... p \< 0.10 will ... reduce confidence by biomedical
audiences, including at NIH").

## **Evaluations**

1\. [James
Dumlao](https://unjournal.pubpub.org/pub/e1chooseyourmoments/draft?access=2hw54etx "null")

2.  [B. Ian
    Hutchins](https://unjournal.pubpub.org/pub/e2chooseyourmoments/draft?access=dhpb1o9e "null")

# **Overall ratings**

We asked evaluators to provide overall assessments as well as ratings
for a range of specific criteria. * *

**I. Overall assessment **(See footnote[^1])

**II. Journal rank tier, normative rating (0-5): **On a 'scale of
journals', what 'quality of journal' should this be published in?[^2]
*Note: 0= lowest/none, 5= highest/best. *

+---+-------------------+---+
|   | **Overall         | * |
|   | assessment        | * |
|   | (0-100)**         | J |
|   |                   | o |
|   |                   | u |
|   |                   | r |
|   |                   | n |
|   |                   | a |
|   |                   | l |
|   |                   | r |
|   |                   | a |
|   |                   | n |
|   |                   | k |
|   |                   | t |
|   |                   | i |
|   |                   | e |
|   |                   | r |
|   |                   | , |
|   |                   | n |
|   |                   | o |
|   |                   | r |
|   |                   | m |
|   |                   | a |
|   |                   | t |
|   |                   | i |
|   |                   | v |
|   |                   | e |
|   |                   | r |
|   |                   | a |
|   |                   | t |
|   |                   | i |
|   |                   | n |
|   |                   | g |
|   |                   | ( |
|   |                   | 0 |
|   |                   | - |
|   |                   | 5 |
|   |                   | ) |
|   |                   | * |
|   |                   | * |
+===+===================+===+
| J | 70                | 3 |
| a |                   | . |
| m |                   | 8 |
| e |                   |   |
| s |                   |   |
| D |                   |   |
| u |                   |   |
| m |                   |   |
| l |                   |   |
| a |                   |   |
| o |                   |   |
+---+-------------------+---+
| B | 75                |   |
| . |                   | 4 |
| I |                   | . |
| a |                   | 0 |
| n |                   |   |
| H |                   |   |
| u |                   |   |
| t |                   |   |
| c |                   |   |
| h |                   |   |
| i |                   |   |
| n |                   |   |
| s |                   |   |
+---+-------------------+---+

*See
"*[*Metrics*](https://unjournal.pubpub.org/pub/evalsumchooseyourmoments#metrics "null")*"
below for a more detailed breakdown of the evaluators' ratings across
several categories. To see these ratings in the context of all Unjournal
ratings, with some analysis, see our *[*data presentation
here.*](https://unjournal.github.io/unjournaldata/chapters/evaluation_data_analysis.html#basic-presentation "null")[^3]*
*

*See
*[*here*](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#metrics-overall-assessment-categories "null")*
for the current full evaluator guidelines, including further explanation
of the requested ratings.*

# Evaluation summaries

## James Dumlao

This paper includes two experiments aimed at informing reform in the NIH
grant peer review process. The first experiment finds that biomedical
scientists have a preference for studies with higher score variance
("dissensus"), interpreted as a desire for risk-taking. The second
experiment finds that tightening budgets reduces scientists' dissensus
tolerance. Authors conclude that relaxed budgets and rewarding dissensus
would encourage the production of more high-impact science.

## B. Ian Hutchins {#b-ian-hutchins}

"Choose Your Moments: NIH Peer Review and Scientific Risk Taking" is an
interesting study on review decision-making. Authors examine the effect
of NIH peer review score distribution on decision-making about funding
particular grants, using a portfolio from NIH 2016 RePORTER data, using
a BIBD approach. Their approach is sound and I am persuaded by their
conclusion that reviewers have a preference for grants with increased
variance in review scores, and that there is an asymmetry in preference
for high-variance applications in response to funding shocks.

# Metrics

## Ratings

[See
here](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#quantitative-metrics "null")
for details on the categories below, and the guidance given to
evaluators.

+-------------------------------+--------------+--------+----------+--------------+--------+----------+
|                               | **Evaluator  |        |          | **Evaluator  |        |          |
|                               | 1**          |        |          | 2**          |        |          |
|                               |              |        |          |              |        |          |
|                               | James Dumlao |        |          | B. Ian       |        |          |
|                               |              |        |          | Hutchins     |        |          |
+===============================+==============+========+==========+==============+========+==========+
| **Rating category**           | **Rating     | **90%  | **Co     | **Rating     | **90%  | **Co     |
|                               | (0-100)**    | CI **  | mments** | (0-100)**    | CI **  | mments** |
|                               |              |        |          |              |        |          |
|                               |              | **(0-  |          |              | **(0-  |          |
|                               |              | 100)\* |          |              | 100)\* |          |
|                               |              | **     |          |              | **     |          |
+-------------------------------+--------------+--------+----------+--------------+--------+----------+
| Overall assessment[^4]        | 70           | (50,   |          | 75           | (60,   |          |
|                               |              | 90)    |          |              | 80)    |          |
+-------------------------------+--------------+--------+----------+--------------+--------+----------+
| Claims, strength,             | 56           | (45,   | [^6]     | 75           | (60,   |          |
| characterization of           |              | 61)    |          |              | 80)    |          |
| evidence[^5]                  |              |        |          |              |        |          |
+-------------------------------+--------------+--------+----------+--------------+--------+----------+
| Advancing knowledge and       | 72           | (66,   | [^8]     | 80           | (75,   |          |
| practice[^7]                  |              | 84)    |          |              | 90)    |          |
+-------------------------------+--------------+--------+----------+--------------+--------+----------+
| Methods: Justification,       | 76           | (68,   | [^10]    | 60           | (50,   |          |
| reasonableness, validity,     |              | 85)    |          |              | 70)    |          |
| robustness[^9]                |              |        |          |              |        |          |
+-------------------------------+--------------+--------+----------+--------------+--------+----------+
| Logic & communication[^11]    | 55           | (45,   | [^12]    | 45           | (35,   | [^13]    |
|                               |              | 70)    |          |              | 55)    |          |
+-------------------------------+--------------+--------+----------+--------------+--------+----------+
| Open, collaborative,          | 65           | (50,   | [^15]    | 30           | (15,   | [^16]    |
| replicable[^14]               |              | 79)    |          |              | 45)    |          |
+-------------------------------+--------------+--------+----------+--------------+--------+----------+
| Real-world relevance          | 50           | (40,   | [^19]    | 80           | (75,   |          |
| [^17],[^18]                   |              | 60)    |          |              | 89)    |          |
+-------------------------------+--------------+--------+----------+--------------+--------+----------+
| Relevance to global           | 50           | (40,   |          | 80           | (75,   |          |
| priorities[^20], [^21]        |              | 60)    |          |              | 89)    |          |
+-------------------------------+--------------+--------+----------+--------------+--------+----------+

## Journal ranking tiers

[See
here](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#journal-ranking-tiers "null")
for more details on these tiers.

+-------------------------------+-------------+----------------+--------------+----------------+
|                               | **Evaluator |                | **Evaluator  |                |
|                               | 1**         |                | 2**          |                |
|                               |             |                |              |                |
|                               | James       |                | B. Ian       |                |
|                               | Dumlao      |                | Hutchins     |                |
+===============================+=============+================+==============+================+
| **Judgment**                  | **Ranking   | **90% CI **    | **Ranking    | **90% CI **    |
|                               | tier        |                | tier (0-5)** |                |
|                               | (0-5)**     |                |              |                |
+-------------------------------+-------------+----------------+--------------+----------------+
| On a 'scale of journals',     | 3.8         | (3.0, 4.1)     | 4.0          | (2.5, 4.5)     |
| what 'quality of journal'     |             |                |              |                |
| *should* this be published    |             |                |              |                |
| in?                           |             |                |              |                |
+-------------------------------+-------------+----------------+--------------+----------------+
| What 'quality journal' do you | 3.9         | (3.5, 4.3)     | 3.5          | (2.5, 4.0)     |
| expect this work *will* be    |             |                |              |                |
| published in?                 |             |                |              |                |
+-------------------------------+-------------+----------------+--------------+----------------+
| [See                          | *We         |                |              |                |
| here](https://                | summarize   |                |              |                |
| globalimpact.gitbook.io/the-u | these as:*  |                |              |                |
| njournal-project-and-communic |             |                |              |                |
| ation-space/policies-projects | -   0.0:    |                |              |                |
| -evaluation-workflow/evaluati |             |                |              |                |
| on/guidelines-for-evaluators# |  Marginally |                |              |                |
| journal-ranking-tiers "null") |     respect |                |              |                |
| for more details on these     | able/Little |                |              |                |
| tiers.                        |     to no   |                |              |                |
|                               |     value   |                |              |                |
|                               |             |                |              |                |
|                               | -   1.0:    |                |              |                |
|                               |             |                |              |                |
|                               | OK/Somewhat |                |              |                |
|                               |             |                |              |                |
|                               |    valuable |                |              |                |
|                               |             |                |              |                |
|                               | -   2.0:    |                |              |                |
|                               |             |                |              |                |
|                               |    Marginal |                |              |                |
|                               |     B-jou   |                |              |                |
|                               | rnal/Decent |                |              |                |
|                               |     field   |                |              |                |
|                               |     journal |                |              |                |
|                               |             |                |              |                |
|                               | -   3.0:    |                |              |                |
|                               |     Top     |                |              |                |
|                               |     B-jou   |                |              |                |
|                               | rnal/Strong |                |              |                |
|                               |     field   |                |              |                |
|                               |     journal |                |              |                |
|                               |             |                |              |                |
|                               | -   4.0:    |                |              |                |
|                               |             |                |              |                |
|                               |    Marginal |                |              |                |
|                               |     A-      |                |              |                |
|                               | Journal/Top |                |              |                |
|                               |     field   |                |              |                |
|                               |     journal |                |              |                |
|                               |             |                |              |                |
|                               | -   5.0:    |                |              |                |
|                               |     A-      |                |              |                |
|                               | journal/Top |                |              |                |
|                               |     journal |                |              |                |
+-------------------------------+-------------+----------------+--------------+----------------+

# Claim identification and assessment (summary)

For *the full discussions, see the* *corresponding sections in each
linked evaluation.*

+-------------------+--------------------------------------------+----------------------------------+------------------------+-------------------------+
|                   | **Main research claim**[^22]               | **Belief in claim**[^23]         | **Suggested robustness | **Important             |
|                   |                                            |                                  | checks**[^24]          | 'implication', policy,  |
|                   |                                            |                                  |                        | credibility**[^25]      |
+===================+============================================+==================================+========================+=========================+
| **Evaluator 1     | Claim 1: Scientists have a preference for  | Claim 1: 95% true\               | N/A                    | N/A                     |
| **James Dumlao    | "dissensus" among reviewers' scores.\      | \                                |                        |                         |
|                   | \                                          | Claim 2: Much less tenable, 65%  |                        |                         |
|                   | Claim 2: Higher score variance is          | true (lacks direct evidence from |                        |                         |
|                   | correlated with higher risk research with  | the studies conducted)           |                        |                         |
|                   | greater potential to have positive impacts |                                  |                        |                         |
|                   | for the public.                            |                                  |                        |                         |
+-------------------+--------------------------------------------+----------------------------------+------------------------+-------------------------+
| **Evaluator 2     | Reviewers have a preference for grants     | I am persuaded unless I see      | Inter-rater            | Program Officers should |
| **B. Ian Hutchins | with increased variance in review scores,  | contrary evidence. I have seen   | reliability            | be provided with        |
|                   | and ... there is an asymmetry in           | primary internal NIH data, and   | information, if        | summary statistics      |
|                   | preference for high-variance applications  | this comports with that          | available.             | about review variance.  |
|                   | in response to funding shocks in the       | experience.                      |                        |                         |
|                   | positive vs. negative direction.           |                                  |                        |                         |
+-------------------+--------------------------------------------+----------------------------------+------------------------+-------------------------+

# Authors' brief response 

Joshua Graff Zivin provided the following response:

> We are grateful for the thoughtful and constructive feedback provided
> by both reviewers. In response, we will strengthen our discussion of
> higher moments by engaging more deeply with the existing literature,
> while acknowledging that our current hypothetical setting does not
> allow us to empirically evaluate these effects. As noted in the
> manuscript, the true relationship between higher moments and grant
> outcomes can only be estimated with historical data from the NIH or
> other grant-making agencies. We remain hopeful that this will someday
> become available to researchers. We also appreciate Reviewer 1's
> suggestion to explore heterogeneity across dimensions of the
> score---such as team quality or innovativeness---which we view as a
> compelling direction for future experimental work, though not one we
> can pursue within the current design.[^26]
>
> Reviewer 2's comments were particularly helpful in encouraging us to
> clarify the policy relevance of our findings and better frame the
> study for a broader, interdisciplinary audience. We also agree with
> Reviewer 2 that further exploration of inter-rater reliability would
> be valuable, and we will examine whether our data are sufficiently
> powered to support such an analysis, despite the limitations of our
> current design.

# Unjournal process notes

## Why we chose this paper 

From our internal discussion:

> It's one of the few papers trying to understand grant design and how
> that impacts science.

> Seems to be targeting a high-value question: using grant design
> (amount, timing) to motivate risk-taking in scientific research. It's
> based on a hypothetical survey/thought experiment, but this is
> probably the best approach to a challenging question...

##  {#r6083976122}

# References

1\. Carson, R. T., Graff Zivin, J. S., & Shrader, J. G. (2023). Choose
Your Moments: Peer Review and Scientific Risk Taking. NBER Working Paper
No. 31409. https://doi.org/10.3386/w31409

[^1]: We asked them to rank this paper "heuristically" as a percentile
    "relative to all serious research in the same area that you have
    encountered in the last three years." We requested they "consider
    all aspects of quality, credibility, importance to knowledge
    production, and importance to practice.

[^2]: See ranking tiers discussed
    [here](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#journal-ranking-tiers).

[^3]: Note: if you are reading this before, or soon after this has been
    publicly released, the ratings from this paper may not yet have been
    incorporated into that data presentation.

[^4]: Judge the quality of the research heuristically. Consider all
    aspects of quality, credibility, importance to knowledge production,
    and importance to practice.

[^5]: "Do the authors do a good job of (i) stating their main questions
    and claims, (ii) providing strong evidence and powerful approaches
    to inform these, and (iii) correctly characterizing the nature of
    their evidence?"\
    \
    This was on the newer form only

[^6]: Some overclaiming in connecting findings to policy
    recommendations.

[^7]: To what extent does the project contribute to the field or to
    practice, particularly in ways that are directly or indirectly
    relevant to global priorities and impactful interventions?

[^8]: Studies advance knowledge well, but findings lack direct
    connection to policy.

[^9]: Are methods clearly justified and explained? Are methods and their
    underlying assumptions reasonable? Are the results likely to be
    robust to changes in the assumptions? Have the authors avoided bias
    and questionable research practices?

[^10]: Experimental methods and robustness checks make for high internal
    validity.

[^11]: Are concepts clearly defined? Is the reasoning transparent? Are
    conclusions consistent with the evidence (or formal proofs)
    presented? Are the data and/or analysis, including tables and
    figures, relevant to the argument?

[^12]: Major logical leap from score variance to risk tolerance to
    high-impact science not well-motivated. Otherwise, writing style is
    very accessible and clear.

[^13]: More details on randomization would be much appreciated.

[^14]: Would another researcher be able to replicate the analysis? Are
    the method and its details explained sufficiently? Is the source of
    the data clear? Is the data made as widely available as possible,
    with clear labeling and explanation? Do the authors provide
    resources that are likely to enable future research and
    meta-analysis?

[^15]: Procedures clearly states, experiment conditions/instruments
    provided, no data shared.

[^16]: As far as I can tell, there is no open data or code, which is
    becoming the norm.

[^17]: Does the paper consider real-world relevance and deal with policy
    and implementation questions? Are the setup, assumptions, and focus
    realistic and relevant to practitioners?

[^18]: The latter ratings were merged in the newer form\
    \
    "Are the paper's chosen topic and approach likely to be useful to
    global priorities, cause prioritization, and high-impact
    interventions?"

    "Does the paper consider real-world relevance and deal with policy
    and implementation questions? Are the setup, assumptions, and focus
    realistic? Do the authors report results that are relevant to
    practitioners? Do they provide useful quantified estimates (costs,
    benefits, etc.)?"

[^19]: Variance could be useful for breaking ties, but not clear that it
    is *the* information to incorporate into funding decisions over
    other forms of information. More information is likely always
    beneficial, some comparison to other possible policy interventions
    would make relevance more apparent.

[^20]: Are the paper's chosen topic and approach likely to be useful to
    global priorities, cause prioritization, and high-impact
    interventions?

[^21]: The latter ratings were merged in the newer form\
    \
    "Are the paper's chosen topic and approach likely to be useful to
    global priorities, cause prioritization, and high-impact
    interventions?"

    "Does the paper consider real-world relevance and deal with policy
    and implementation questions? Are the setup, assumptions, and focus
    realistic? Do the authors report results that are relevant to
    practitioners? Do they provide useful quantified estimates (costs,
    benefits, etc.)?"

[^22]: The evaluator was given the following instructions:\
    \
    Identify the most important and impactful factual claim this
    research makes -- e.g., a binary claim or a point estimate or
    prediction.

    Please state the authors' claim precisely and quantitatively.
    Identify the source of the claim (i.e., cite the paper), and briefly
    mention the evidence underlying this. We encourage you to explain
    why you believe this claim is important, either here, or in the text
    of your report.

[^23]: Evaluators were asked: To what extent do you \*believe\* the
    claim you stated above? Feel free to express this either a. in terms
    of the probability of the claim being true, b. as a credible
    interval for the parameter being estimated, or c. however you feel
    comfortable.

[^24]: *We asked:*

    \[Optional\] What additional information, evidence, replication, or
    robustness check would make you substantially more (or less)
    confident in this claim?

    Feel free to refer to the main body of your evaluation here; you
    don\'t need to repeat yourself. Please specify how you would perform
    this robustness check (etc.) as precisely as you are willing. E.g.,
    if you suggest a particular estimation command in a statistical
    package, this could be very helpful for future robustness
    replication work.

[^25]: *We asked:*\
    \
    \[Optional\] Identify the important \*implication\* of the above
    claim for funding and policy choices? To what extent do you
    \*believe\* this implication? How should it inform policy choices?\
    \
    Note: this 'implication' could be suggested by the evaluation
    manager in some cases. As an example of an \'implication\' \... in a
    global health context, the \'main claim\' might suggest that a
    vitamin supplement intervention, if scaled up, would save lives at a
    \$XXXX per life saved.

    We did not ask this in the 'applied stream' as it is most likely
    redundant.\
    \

[^26]: Manager: paragraph break added\
