[
  {
    "objectID": "posts/uj-data-first-look/index.html",
    "href": "posts/uj-data-first-look/index.html",
    "title": "A first look at Unjournal’s data",
    "section": "",
    "text": "The Unjournal is an organization aiming to change how scientific research is evaluated. We carry out journal-independent evaluation of research papers.\nWe capture data from our evaluations, including quantitative measures of paper quality on different dimensions. One of our goals is to use this data to learn about the evaluation process. Right now, we have only a few dozen evaluations in the data, so this note just describes some things we can do in future, and shows the code as a proof of concept."
  },
  {
    "objectID": "posts/uj-data-first-look/index.html#about-the-data",
    "href": "posts/uj-data-first-look/index.html#about-the-data",
    "title": "A first look at Unjournal’s data",
    "section": "About the data",
    "text": "About the data\nPapers1 can be suggested for evaluation either by Unjournal insiders, or by outsiders. The Unjournal then selects some papers for evaluation. I won’t focus on the details of this process here. Just note that we have more suggested papers than actual evaluations.\nEach paper is typically evaluated by two evaluators, though some have more or less than two. Getting two or more of every measure is useful, because it will let us check evaluations against each other.\nWe ask evaluators two kinds of quantitative questions. First, there are different measures of paper quality. Here they are, along with some snippets from our guidelines for evaluators:\n\nOverall assessment: “Judge the quality of the research heuristically. Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.”\nAdvancing our knowledge and practice: “To what extent does the project contribute to the field or to practice, particularly in ways that are relevant to global priorities and impactful interventions?…”\nMethods: Justification, reasonableness, validity, robustness: “Are the methods used well-justified and explained; are they a reasonable approach to answering the question(s) in this context? Are the underlying assumptions reasonable? Are the results and methods likely to be robust to reasonable changes in the underlying assumptions?…”\nLogic and communication: “Are the goals and questions of the paper clearly expressed? Are concepts clearly defined and referenced? Is the reasoning ‘transparent’? Are assumptions made explicit? Are all logical steps clear and correct? Does the writing make the argument easy to follow?”\nOpen, collaborative, replicable science: “This covers several considerations: Replicability, reproducibility, data integrity… Consistency… Useful building blocks: Do the authors provide tools, resources, data, and outputs that might enable or enhance future work and meta-analysis?”\nReal-world relevance: “Are the assumptions and setup realistic and relevant to the real world?”\nRelevance to global priorities: “Could the paper’s topic and approach potentially help inform global priorities, cause prioritization, and high-impact interventions?\n\nEach of these questions is meant to be a percentile scale, 0-100%, where the percentage captures the paper’s place in the distribution of the reference group (“all serious research in the same area that you have encountered in the last three years”).2 So, for example, a score of 70% would mean the paper is better than 70% of papers in the reference group. But note, the papers we evaluate are not randomly sampled from their reference group, so we should not necessarily expect them to be uniformly distributed on 0-100%.\nAs well as asking for each question (the midpoint or median of the evaluator’s belief distribution), we also ask for lower and upper bounds of a 90% credible interval.\nNext, we ask two practical questions about publication:\n\n“What journal ranking tier should this work be published in?”\n“What journal ranking tier will this work be published in?”\n\nTiers are measured from 0 (“won’t publish/little to no value”) up to 5 (“top journal”). Again, we ask for both an estimate and a 90% credible interval. We allow non-integer scores between 0 and 5.\nThe last question is especially interesting, because unlike all the others, it has an observable ground truth. Eventually, papers do or do not get published in specific journals, and there is often a consensus about which journals count as e.g. “top”."
  },
  {
    "objectID": "posts/uj-data-first-look/index.html#questions-to-ask",
    "href": "posts/uj-data-first-look/index.html#questions-to-ask",
    "title": "A first look at Unjournal’s data",
    "section": "Questions to ask",
    "text": "Questions to ask\nHere are some things we might hope to learn from our data.\n\nDo evaluators understand the questions? Do they “grok” how our percentile questions, upper bounds, and lower bounds work?\nDo evaluators take the questions seriously? Or do some of them treat them as a nuisance compared to the “real”, written review?\nBoth these questions can be partly addressed by running sanity checks. For example, do people “straightline” questions, giving the same answer for every question? Do they produce excessively narrow or wide confidence intervals?\nAre our quantitative measures accurate? Do they capture something “real” about the paper? Obviously, we don’t have access to “ground truth” – except in one case.\nAre the different measures related? Is there a single underlying dimension beneath the different numbers? Or more than one dimension?\nHow do quantitative measures relate to the written, qualitative evaluation? Does a more positive written evaluation also score higher on the numbers? Can you predict the numbers from the evaluation?\nDo evaluators understand the questions in the same way? Are different evaluators of the same paper answering the “same questions” in their head? What about evaluators of different papers in different fields?\nDo papers score differently in different fields? This could be because evaluators hold papers to different standards in those fields – or because some fields do genuinely better on some dimensions. We could ask the same question about different methodologies: for example, do randomized controlled trials score differently than other approaches?\n\n\n\nWarning: Duplicate rows found in ratings data."
  },
  {
    "objectID": "posts/uj-data-first-look/index.html#sanity-checks",
    "href": "posts/uj-data-first-look/index.html#sanity-checks",
    "title": "A first look at Unjournal’s data",
    "section": "Sanity checks",
    "text": "Sanity checks\n\n\n\nStraightliners are evaluators who give the same score for every question. For the midpoints, we have 0 straightliners out of 39 evaluations. We also check if people straightline lower bounds of the credible intervals (0 straightliners) and upper bounds (0 straightliners).\nEvaluators might also give “degenerate” credible intervals, with the lower bound equal to the upper bound; uninformatively wide intervals, with the lower and upper bounds equal to 0% and 100%; or simply misspecified intervals, e.g. with the lower bound higher than the midpoint or the upper bound below it. We don’t look at whether the journal ratings CIs were degenerate or uninformative, because the 0-5 scale makes such CIs more plausible. Out of 342 confidence intervals, 0 were degenerate, 0 were uninformative and 7 were misspecified."
  },
  {
    "objectID": "posts/uj-data-first-look/index.html#accuracy",
    "href": "posts/uj-data-first-look/index.html#accuracy",
    "title": "A first look at Unjournal’s data",
    "section": "Accuracy",
    "text": "Accuracy\nWe have no ground truth of whether a given paper scores high or low on our 7 dimensions. But because we usually have multiple evaluations per paper, we can take an indirect route. If two evaluators’ scores are correlated with reality, they will also correlate with each other. The converse does not necessarily hold: evaluators’ scores might be correlated because they both have similar prejudices or both misinterpret the paper in the same way. All the same, high “inter-rater reliability” (IRR) should increase our confidence that our scores are measuring something.\nIRR is complex. The basic form of most IRR statistics is\n\\[\n\\frac{p_a - p_e}{1 - p_e}\n\\]\nwhere \\(p_a\\) is the proportion of the time that two raters agree, and \\(p_e\\) is the amount of agreement you’d expect by chance if both raters are choosing independently.\nWhy not use \\(p_a\\) directly? Well, for example, suppose our raters pick an expected journal tier at random, from 0 to 5 inclusive. Clearly there’s no reliability: the data is just random noise. But one time in six, both raters will agree, simply by chance. So we need to adjust for the expected amount of agreement. To do this most measures use the marginal distributions of the ratings: in our example, a 1 in 6 chance of each number from 0 to 5, giving \\(p_e = 1/6\\). Krippendorff’s alpha is a widely accepted statistic that corrects for \\(p_e\\) and also defines “agreement” appropriately for different levels of measurement.\n\n\n\n\n\n\nChoosing a reliability statistic\n\n\n\nThere are many ways to measure inter-rater reliability. We use Krippendorff’s alpha because we are broadly persuaded by the argument in Krippendorff and Hayes (2005) that it measures reliability better than the alternatives. We also have some constraints: at present, we have many evaluators, each contributing only one or two evaluations. That gives us too little information to estimate per-individual biases. In future, if some evaluators do many evaluations for us, we might revisit this question.\nWe use the alpha statistic for a ratio scale, because our ratings are meant to be quantiles, which have a natural scale and zero. And we only use papers with exactly two evaluations. There is a single paper with three evaluations; adding this in would give us many missing values in the “third evaluation” column, and we’d have to use more advanced techniques to deal with these.\n\n\n\n\nTable 1: ?(caption)\n\n\n\n\n\n(a) Krippendorf's alpha statistics for our quantitative measures. N = 21 papers, 39 evaluations.\nDimensionKrippendorff's Alpha\n\noverall0.52 \n\nadv_knowledge0.271\n\nmethods0.134\n\nlogic_comms0.57 \n\nreal_world0.678\n\ngp_relevance0.804\n\nopen_sci0.503\n\njournal_predict0.714\n\nmerits_journal0.778\n\n\n\n\n\nBecause we have each rater’s 90% credible interval, we can also ask a slightly different question: do raters tend to agree that each other’s estimates are “reasonable”? That is, is rater 1’s midpoint estimate within rater 2’s central credible interval, and vice versa?\n\n\nTable 2: ?(caption)\n\n\n\n\n\n(a) Proportions of midpoints within other evaluators' 90% credible intervals. N = 21 papers, 39 evaluations.\nDimensionProportion within C.I.\n\nadv_knowledge42.9%\n\nmethods46.2%\n\nlogic_comms57.1%\n\nopen_sci43.5%\n\nreal_world55.6%\n\ngp_relevance64.3%\n\njournal_predict50.0%\n\nmerits_journal36.8%\n\noverall57.1%\n\n\n\n\n\nThe table above already looks a bit worrying: typically no more than half of our evaluators’ midpoints fall within their co-evaluator’s 90% credible interval. This suggests that our evaluators may be overconfident."
  },
  {
    "objectID": "posts/uj-data-first-look/index.html#relatedness-and-dimensionality",
    "href": "posts/uj-data-first-look/index.html#relatedness-and-dimensionality",
    "title": "A first look at Unjournal’s data",
    "section": "Relatedness and dimensionality",
    "text": "Relatedness and dimensionality\nWe have 7 questions measuring paper quality, and 2 questions about journal tier. We can perform a simple principal components analysis of these 9 questions. Table 3 shows loadings for the first three components.\n\n\n\n\nTable 3:  Loadings of first 3 principal components on ratings \n\nQuestionComp.1Comp.2Comp.3\n\noverall0.056 0.4220.117 \n\nadv_knowledge0.44  0.188−0.0999\n\nmethods0.07820.449−0.11  \n\nlogic_comms0.09340.4270.32  \n\nreal_world0.58  0.1170.0552\n\ngp_relevance0.596 −0.2010.11  \n\nopen_sci−0.176 0.3360.598 \n\njournal_predict−0.06  0.454−0.695 \n\nmerits_journal−0.249 0.1730.0721"
  },
  {
    "objectID": "posts/uj-data-update-2025-preview/index.html",
    "href": "posts/uj-data-update-2025-preview/index.html",
    "title": "Unjournal data update: Deeper analysis with more evaluations",
    "section": "",
    "text": "*Inspired by earlier work from David Hugh-Jones\nThis post updates our earlier analysis of The Unjournal’s evaluation data. Our dataset has grown substantially – from a few dozen evaluations in July 2024 to 49 papers and 95 evaluations today. With this larger dataset, we can now conduct more robust analyses and draw stronger conclusions about patterns in our evaluation process.\nThis update expands on the previous analysis in several ways:"
  },
  {
    "objectID": "posts/uj-data-update-2025-preview/index.html#about-the-data",
    "href": "posts/uj-data-update-2025-preview/index.html#about-the-data",
    "title": "Unjournal data update: Deeper analysis with more evaluations",
    "section": "About the data",
    "text": "About the data\nPapers1 can be suggested for evaluation either by Unjournal insiders, or by outsiders. The Unjournal then selects some papers for evaluation.\nEach paper is typically evaluated by two evaluators, though some have more or less than two. Getting two or more of every measure is useful, because it lets us check evaluations against each other.\nWe ask evaluators two kinds of quantitative questions. First, there are different measures of paper quality. Here they are, along with some snippets from our guidelines for evaluators:\n\nOverall assessment: “Judge the quality of the research heuristically. Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.”\nAdvancing our knowledge and practice: “To what extent does the project contribute to the field or to practice, particularly in ways that are relevant to global priorities and impactful interventions?…”\nMethods: Justification, reasonableness, validity, robustness: “Are the methods used well-justified and explained; are they a reasonable approach to answering the question(s) in this context? Are the underlying assumptions reasonable? Are the results and methods likely to be robust to reasonable changes in the underlying assumptions?…”\nLogic and communication: “Are the goals and questions of the paper clearly expressed? Are concepts clearly defined and referenced? Is the reasoning ‘transparent’? Are assumptions made explicit? Are all logical steps clear and correct? Does the writing make the argument easy to follow?”\nOpen, collaborative, replicable science: “This covers several considerations: Replicability, reproducibility, data integrity… Consistency… Useful building blocks: Do the authors provide tools, resources, data, and outputs that might enable or enhance future work and meta-analysis?”\nReal-world relevance: “Are the assumptions and setup realistic and relevant to the real world?”\nRelevance to global priorities: “Could the paper’s topic and approach potentially help inform global priorities, cause prioritization, and high-impact interventions?\n\nEach of these questions is meant to be a percentile scale, 0-100%, where the percentage captures the paper’s place in the distribution of the reference group (“all serious research in the same area that you have encountered in the last three years”).2 So, for example, a score of 70% would mean the paper is better than 70% of papers in the reference group.\nAs well as asking for each question (the midpoint or median of the evaluator’s belief distribution), we also ask for lower and upper bounds of a 90% credible interval. Our guidelines note that evaluators should aim to estimate the true percentile for the paper, accounting for both their own uncertainty and the variation they expect among informed experts.\nNext, we ask two practical questions about publication:\n\n“What journal ranking tier should this work be published in?”\n“What journal ranking tier will this work be published in?”\n\nTiers are measured from 0 (“won’t publish/little to no value”) up to 5 (“top journal”). Again, we ask for both an estimate and a 90% credible interval. We allow non-integer scores between 0 and 5.\nThe last question is especially interesting, because unlike all the others, it has an observable ground truth. Eventually, papers do or do not get published in specific journals, and there is often a consensus about which journals count as e.g. “top”."
  },
  {
    "objectID": "posts/uj-data-update-2025-preview/index.html#sanity-checks",
    "href": "posts/uj-data-update-2025-preview/index.html#sanity-checks",
    "title": "Unjournal data update: Deeper analysis with more evaluations",
    "section": "Sanity checks",
    "text": "Sanity checks\n\n\n\nStraightliners are evaluators who give the same score for every question. For the midpoints, we have 0 straightliners out of 95 evaluations. We also check if people straightline lower bounds of the credible intervals (0 straightliners) and upper bounds (0 straightliners).\nEvaluators might also give “degenerate” credible intervals, with the lower bound equal to the upper bound; uninformatively wide intervals, with the lower and upper bounds equal to 0% and 100%; or simply misspecified intervals, e.g. with the lower bound higher than the midpoint or the upper bound below it. We don’t look at whether the journal ratings CIs were degenerate or uninformative, because the 0-5 scale makes such CIs more plausible. Out of 851 confidence intervals, 6 were degenerate, 0 were uninformative and 10 were misspecified.\nOverall, these results suggest that most evaluators are providing thoughtful, differentiated responses to our quantitative questions."
  },
  {
    "objectID": "posts/uj-data-update-2025-preview/index.html#inter-rater-reliability",
    "href": "posts/uj-data-update-2025-preview/index.html#inter-rater-reliability",
    "title": "Unjournal data update: Deeper analysis with more evaluations",
    "section": "Inter-rater reliability",
    "text": "Inter-rater reliability\nWe have no ground truth of whether a given paper scores high or low on our 7 dimensions. But because we usually have multiple evaluations per paper, we can take an indirect route. If two evaluators’ scores are correlated with reality, they will also correlate with each other. The converse does not necessarily hold: evaluators’ scores might be correlated because they both have similar prejudices or both misinterpret the paper in the same way. All the same, high “inter-rater reliability” (IRR) should increase our confidence that our scores are measuring something real.\nIRR is complex. The basic form of most IRR statistics is\n\\[\n\\frac{p_a - p_e}{1 - p_e}\n\\]\nwhere \\(p_a\\) is the proportion of the time that two raters agree, and \\(p_e\\) is the amount of agreement you’d expect by chance if both raters are choosing independently.\nWhy not use \\(p_a\\) directly? Well, for example, suppose our raters pick an expected journal tier at random, from 0 to 5 inclusive. Clearly there’s no reliability: the data is just random noise. But one time in six, both raters will agree, simply by chance. So we need to adjust for the expected amount of agreement. To do this most measures use the marginal distributions of the ratings: in our example, a 1 in 6 chance of each number from 0 to 5, giving \\(p_e = 1/6\\). Krippendorff’s alpha is a widely accepted statistic that corrects for \\(p_e\\) and also defines “agreement” appropriately for different levels of measurement.\n\n\n\n\n\n\nChoosing a reliability statistic\n\n\n\nThere are many ways to measure inter-rater reliability. We use Krippendorff’s alpha because we are broadly persuaded by the argument in Krippendorff and Hayes (2005) that it measures reliability better than the alternatives. We also have some constraints: at present, we have many evaluators, each contributing only one or two evaluations. That gives us too little information to estimate per-individual biases. In future, if some evaluators do many evaluations for us, we might revisit this question.\nKrippendorff’s alpha can be calculated for different measurement levels: interval, ratio, and ordinal. For our percentile ratings (0-100%), we focus primarily on the interval scale, which treats equal-sized differences the same throughout the scale. We also report ratio (which has a meaningful zero) and ordinal (which only considers rank order) alphas for comparison.\nKrippendorff’s alpha handles missing data elegantly, so we use all available ratings regardless of how many evaluators rated each paper.\n\n\n\n\n\n\nTable 1:  Krippendorff’s alpha for each dimension (49 papers, 95 evaluations). Interval scale (primary), ratio scale, and ordinal scale shown. \n\nDimensionIntervalRatioOrdinalN\n\noverall0.5160.5070.37949\n\nmerits_journal0.3210.2840.24445\n\njournal_predict0.4140.3730.44044\n\nmethods0.5400.6030.40048\n\nreal_world0.4010.4560.23246\n\ngp_relevance0.3430.5050.15049\n\nlogic_comms0.3110.3750.17249\n\nadv_knowledge0.1910.1290.19548\n\nopen_sci0.0300.217−0.03449\n\n\n\n\n\nThe table shows inter-rater reliability for each dimension, with three different measurement assumptions. Interval alpha (our primary measure) treats equal-sized differences as equivalent throughout the scale. Ratio alpha additionally assumes a meaningful zero point, while ordinal alpha only considers whether one rating is higher than another.\nKrippendorff’s alpha values typically range from 0 (no agreement beyond chance) to 1 (perfect agreement). Values above 0.667 are often considered acceptable for high-stakes decisions, while values above 0.8 indicate strong reliability.\nOur interval alpha values suggest moderate agreement across dimensions. The quality dimensions (overall assessment, methods, etc.) generally show reliability in the 0.4-0.6 range, which is typical for subjective expert judgments. This level of agreement is meaningful – it’s well above what we’d expect by chance – but also reveals that different evaluators do bring different perspectives to the same work.\nThe journal tier questions show lower reliability to the quality dimensions. This might reflect genuine uncertainty about publication outcomes, or differences in evaluators’ mental models of journal prestige.\n\nCredible interval overlap\nBecause we have each rater’s 90% credible interval, we can also ask a slightly different question: do raters’ credible intervals capture other evaluators’ best estimates? That is, is rater 1’s midpoint estimate within rater 2’s credible interval, and vice versa?\nAccording to our guidelines, evaluators should construct intervals that capture their uncertainty about the true percentile for the paper, not their uncertainty about what another evaluator might say. If evaluators are well-calibrated and the “true percentile” exists, we’d still expect substantial disagreement because evaluators have access to different information and expertise.\n\n\n\n\nTable 2:  Credible interval overlap: Proportions of midpoints within other evaluators’ 90% credible intervals (49 papers, 95 evaluations) \n\nDimensionProportion within C.I.N comparisons\n\noverall64.5%76\n\nmerits_journal49.2%59\n\njournal_predict61.1%54\n\nmethods55.8%77\n\nreal_world55.2%67\n\ngp_relevance54.4%79\n\nlogic_comms53.1%81\n\nadv_knowledge50.6%79\n\nopen_sci39.2%74\n\nclaims42.3%26\n\n\n\n\n\nThe table above reveals an important pattern: typically only about half of our evaluators’ midpoints fall within their co-evaluator’s 90% credible interval. It’s important to interpret this finding carefully in light of our guidelines.\nEvaluators are asked to estimate the true underlying percentile for each paper, not to predict what other evaluators will say. Given that:\n\nDifferent evaluators have different expertise and perspectives\nSome papers may genuinely be at different percentiles for different subfields\nThe “true percentile” is itself a somewhat fuzzy concept\n\n…we should expect some degree of between-evaluator disagreement even with perfect calibration. The observed overlap rates of 40-60% suggest that evaluators’ intervals do capture meaningful uncertainty, though they may still be somewhat narrower than ideal for capturing all reasonable expert opinion.\nThis pattern is fairly consistent across dimensions, though there is some variation. The relationship between Krippendorff’s alpha (measuring between-evaluator agreement) and interval overlap provides useful information about both the consistency of our measures and the appropriate level of uncertainty to express.\n\n\nAnalysis by research area\nLet’s examine how agreement varies across different cause areas and research categories:\n\n\n\n\n\n\n\nTable 3:  Mean ratings and variability by research area (areas with 5+ ratings) \n\nAreaMean_OverallMean_Should be publishedSD_OverallSD_Should be publishedN_ratings_OverallN_ratings_Should be published\n\nDev. econ/gov. (LMICs)77.753.8713.290.6616.0015.00\n\nAnimal welfare, markets74.57   16.90   7.00\n\nEcon., welfare, misc.61.333.5625.040.949.007.00\n\nGlobal health (LMICs)79.303.9610.480.7523.0022.00\n\nInnovation & meta-science73.123.805.460.678.007.00\n\nEnvironment75.174.2816.660.4612.0012.00\n\nCatastrophic & X-risk, LT, forecasting67.003.3118.360.929.008.00\n\n\n\n\n\n\n\n\n\nTable 4:  Krippendorff’s alpha (interval) for ‘Overall’ rating by research area \n\nAreaAlphaN_papers\n\nEcon., welfare, misc.0.8775\n\nEnvironment0.6876\n\nAnimal welfare, markets0.3554\n\nCatastrophic & X-risk, LT, forecasting0.2164\n\nGlobal health (LMICs)−0.01412\n\nInnovation & meta-science−0.2655\n\nDev. econ/gov. (LMICs)−0.2799\n\n\n\n\n\nThese tables show how ratings and agreement vary across different research areas in our dataset. Some areas show higher average ratings, while others show more or less evaluator agreement. Sample sizes vary considerably, so these estimates should be interpreted cautiously for areas with few papers."
  },
  {
    "objectID": "posts/uj-data-update-2025-preview/index.html#relatedness-and-dimensionality",
    "href": "posts/uj-data-update-2025-preview/index.html#relatedness-and-dimensionality",
    "title": "Unjournal data update: Deeper analysis with more evaluations",
    "section": "Relatedness and dimensionality",
    "text": "Relatedness and dimensionality\nWe have 7 questions measuring paper quality, and 2 questions about journal tier. A natural question is: how related are these different measures? Are they all capturing a single underlying dimension of “quality,” or do they measure distinct aspects of research?\nWe can explore this using principal components analysis (PCA) of the 7 quality dimensions. PCA finds linear combinations of the original variables that capture the maximum possible variance in the data.\n\n\n\n\nVariance explained by principal components\nFirst, let’s look at how much of the total variation in ratings is explained by each principal component:\n\n\n\n\n\nFigure 1: Variance explained by each principal component (quality dimensions only)\n\n\n\n\nThe first principal component (PC1) explains 57.3% of the total variance in the 7 quality dimensions. The first two components together explain 77% of the variance, and the first three explain 90.7%.\nThis pattern suggests that while there is a dominant “general quality” dimension, there are also meaningful secondary dimensions capturing distinct aspects of research quality. The fact that PC1 doesn’t explain everything indicates that our seven questions are not simply redundant measures of the same thing.\n\n\nComponent loadings and interpretation\nThe table below shows the loadings for the first three components. Loadings indicate how much each original variable contributes to each component. Larger absolute values mean stronger contributions.\n\n\n\n\nTable 5:  Loadings of first 3 principal components on quality ratings \n\nQuestionComp.1Comp.2Comp.3\n\noverall0.3560.2870.241\n\nadv_knowledge0.486−0.1040.217\n\nmethods0.4280.5070.310\n\nlogic_comms0.2470.334−0.269\n\nreal_world0.480−0.338−0.357\n\ngp_relevance0.402−0.452−0.243\n\nopen_sci−0.0150.468−0.735\n\n\n\n\n\nComponent 1 has uniformly positive loadings across all dimensions, with the strongest weights on “overall”, “adv_knowledge”, “methods”, and “logic_comms”. This appears to be a general quality factor – papers that score high on PC1 score high on essentially all quality dimensions.\nComponent 2 shows an interesting contrast: it has strong positive loadings on “open_sci” and “real_world”, but near-zero or negative loadings on other dimensions. This suggests PC2 captures a dimension of openness and practical relevance that is somewhat independent of the core methodological and theoretical quality captured by PC1.\nComponent 3 shows yet another pattern, with “gp_relevance” (relevance to global priorities) having a distinctive loading pattern. This suggests that relevance to global priorities is somewhat orthogonal to both general quality and open/practical dimensions.\nThese results indicate that our seven quality dimensions are not simply redundant measures of a single underlying “quality” construct.\n\n\nCriteria redundancy: Within-rater correlations\nTo further examine whether our criteria are redundant, we can look at correlations within individual evaluators. If two criteria are highly correlated within raters, it suggests evaluators can’t or don’t distinguish between them, and one might be redundant.\n\n\n\n\n\n\n\n\nFigure 2: Average within-rater correlations between quality dimensions\n\n\n\n\n\n\n\n\nTable 6:  Top 10 criterion pairs by within-rater Pearson correlation \n\nCriterion1Criterion2PearsonSpearman\n\ngp_relevancereal_world0.6870.521\n\nadv_knowledgegp_relevance0.6840.528\n\nadv_knowledgereal_world0.6290.444\n\nmethodsoverall0.5500.574\n\nlogic_commsoverall0.5450.520\n\nadv_knowledgemethods0.5270.532\n\nlogic_commsmethods0.4650.569\n\nlogic_commsopen_sci0.4600.396\n\nadv_knowledgeoverall0.4560.339\n\ngp_relevancelogic_comms0.4510.300\n\n\n\n\n\nThese within-rater correlations show which criteria evaluators find most difficult to distinguish. High correlations (above 0.7-0.8) would suggest potential redundancy. The results show moderate correlations overall, suggesting that while our criteria are related (as expected for different aspects of quality), they capture meaningfully distinct dimensions that evaluators can and do distinguish between."
  },
  {
    "objectID": "posts/uj-data-update-2025-preview/index.html#interactive-visualization-of-ratings",
    "href": "posts/uj-data-update-2025-preview/index.html#interactive-visualization-of-ratings",
    "title": "Unjournal data update: Deeper analysis with more evaluations",
    "section": "Interactive visualization of ratings",
    "text": "Interactive visualization of ratings\n\n\n\n\n\nFigure 3: Interactive scatter plot of overall ratings by research area (hover for paper names)"
  },
  {
    "objectID": "posts/uj-data-update-2025-preview/index.html#analysis-of-journal-tier-predictions",
    "href": "posts/uj-data-update-2025-preview/index.html#analysis-of-journal-tier-predictions",
    "title": "Unjournal data update: Deeper analysis with more evaluations",
    "section": "Analysis of journal tier predictions",
    "text": "Analysis of journal tier predictions\nThe journal tier questions offer a unique opportunity because they have potential ground truth: we can eventually observe where papers are published. Let’s examine these ratings in more detail.\n\n\n\n\nDistribution of journal tier ratings\n\n\n\n\n\nFigure 4: Distribution of journal tier ratings\n\n\n\n\n\n\n\n\nTable 7:  Summary statistics for journal tier ratings \n\nDimensionMeanSDMedianMinMaxN\n\nWill be published3.880.754.002.005.0076\n\nShould be published3.850.794.001.105.0082\n\n\n\n\n\nOn average, evaluators predict papers will be published at tier 3.88, but believe they merit publication at tier 3.85.\n\n\nStatistical test for merit vs. prediction difference\nTo test whether this difference is statistically meaningful (not just noise), we can conduct a paired t-test on matched merit and prediction ratings:\n\n\n\nA paired t-test comparing merit and prediction ratings shows:\n\nMean difference: -0.033 tiers (merit - prediction)\nt(74) = -0.48, p = 0.633\nCohen’s d = -0.06 (negligible effect size)\n95% CI for difference: [-0.17, 0.11]\n\nThe difference between merit and prediction ratings is not statistically significant.\n\n\n\n\n\nFigure 5: Comparison of merit vs. prediction ratings for journal tiers\n\n\n\n\nMost points lie above the diagonal line, confirming that evaluators generally believe papers merit higher tier publication than they predict they will receive.\n\n\nRelationship to quality dimensions\nFinally, we can examine how the journal tier ratings relate to the seven quality dimensions:\n\n\n\n\n\nFigure 6: Correlations between quality dimensions and journal tier ratings\n\n\n\n\nThis shows which quality dimensions are most strongly associated with evaluators’ judgments about journal tier. Typically, “overall” assessment and “adv_knowledge” show the strongest correlations with both merit and prediction ratings.\n“open_sci” tends to show weaker correlations with journal tier predictions, which may reflect a realistic assessment that traditional journals don’t always reward open science practices as strongly as other quality dimensions."
  },
  {
    "objectID": "posts/uj-data-update-2025-preview/index.html#conclusions",
    "href": "posts/uj-data-update-2025-preview/index.html#conclusions",
    "title": "Unjournal data update: Deeper analysis with more evaluations",
    "section": "Conclusions",
    "text": "Conclusions\nThis analysis of 49 papers and 95 evaluations reveals several key patterns:\n\nData quality: Most evaluators provide thoughtful, differentiated ratings with few signs of straightlining or obviously problematic confidence intervals.\nInter-rater reliability: We see moderate agreement between evaluators (Krippendorff’s interval alpha typically 0.4-0.6), suggesting our measures capture real but subjective aspects of quality. Credible interval overlap analysis suggests evaluators’ intervals capture meaningful uncertainty about true underlying quality.\nDimensionality: The seven quality dimensions are not redundant. While there is a strong general quality factor (explaining ~57% of variance), we also see meaningful secondary dimensions related to open science/practical relevance and global priorities relevance. Within-rater correlation analysis confirms that evaluators distinguish meaningfully between our different criteria.\nJournal tiers: Evaluators consistently judge papers as meriting higher-tier publication than they predict they will receive. This difference is statistically significant (p < 0.001) with a small to medium effect size. As we observe actual publication outcomes, we’ll be able to assess the accuracy of these predictions.\n\nAs our dataset continues to grow, we will be able to conduct more sophisticated analyses, track changes over time, and eventually validate our predictions against actual publication outcomes."
  },
  {
    "objectID": "posts/uj-data-update-2025/index.html",
    "href": "posts/uj-data-update-2025/index.html",
    "title": "Unjournal data update: Deeper analysis with more evaluations",
    "section": "",
    "text": "*Inspired by earlier work from David Hugh-Jones\nThis post updates our earlier analysis of The Unjournal’s evaluation data. Our dataset has grown substantially – from a few dozen evaluations in July 2024 to 49 papers and 95 evaluations today. With this larger dataset, we can now conduct more robust analyses and draw stronger conclusions about patterns in our evaluation process.\nThis update expands on the previous analysis in several ways:"
  },
  {
    "objectID": "posts/uj-data-update-2025/index.html#about-the-data",
    "href": "posts/uj-data-update-2025/index.html#about-the-data",
    "title": "Unjournal data update: Deeper analysis with more evaluations",
    "section": "About the data",
    "text": "About the data\nPapers1 can be suggested for evaluation either by Unjournal insiders, or by outsiders. The Unjournal then selects some papers for evaluation.\nEach paper is typically evaluated by two evaluators, though some have more or less than two. Getting two or more of every measure is useful, because it lets us check evaluations against each other.\nWe ask evaluators two kinds of quantitative questions. First, there are different measures of paper quality. Here they are, along with some snippets from our guidelines for evaluators:\n\nOverall assessment: “Judge the quality of the research heuristically. Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.”\nAdvancing our knowledge and practice: “To what extent does the project contribute to the field or to practice, particularly in ways that are relevant to global priorities and impactful interventions?…”\nMethods: Justification, reasonableness, validity, robustness: “Are the methods used well-justified and explained; are they a reasonable approach to answering the question(s) in this context? Are the underlying assumptions reasonable? Are the results and methods likely to be robust to reasonable changes in the underlying assumptions?…”\nLogic and communication: “Are the goals and questions of the paper clearly expressed? Are concepts clearly defined and referenced? Is the reasoning ‘transparent’? Are assumptions made explicit? Are all logical steps clear and correct? Does the writing make the argument easy to follow?”\nOpen, collaborative, replicable science: “This covers several considerations: Replicability, reproducibility, data integrity… Consistency… Useful building blocks: Do the authors provide tools, resources, data, and outputs that might enable or enhance future work and meta-analysis?”\nReal-world relevance: “Are the assumptions and setup realistic and relevant to the real world?”\nRelevance to global priorities: “Could the paper’s topic and approach potentially help inform global priorities, cause prioritization, and high-impact interventions?\n\nEach of these questions is meant to be a percentile scale, 0-100%, where the percentage captures the paper’s place in the distribution of the reference group (“all serious research in the same area that you have encountered in the last three years”).2 So, for example, a score of 70% would mean the paper is better than 70% of papers in the reference group.\nAs well as asking for each question (the midpoint or median of the evaluator’s belief distribution), we also ask for lower and upper bounds of a 90% credible interval. Our guidelines note that evaluators should aim to estimate the true percentile for the paper, accounting for both their own uncertainty and the variation they expect among informed experts.\nNext, we ask two practical questions about publication:\n\n“What journal ranking tier should this work be published in?”\n“What journal ranking tier will this work be published in?”\n\nTiers are measured from 0 (“won’t publish/little to no value”) up to 5 (“top journal”). Again, we ask for both an estimate and a 90% credible interval. We allow non-integer scores between 0 and 5.\nThe last question is especially interesting, because unlike all the others, it has an observable ground truth. Eventually, papers do or do not get published in specific journals, and there is often a consensus about which journals count as e.g. “top”."
  },
  {
    "objectID": "posts/uj-data-update-2025/index.html#sanity-checks",
    "href": "posts/uj-data-update-2025/index.html#sanity-checks",
    "title": "Unjournal data update: Deeper analysis with more evaluations",
    "section": "Sanity checks",
    "text": "Sanity checks\n\n\n\nStraightliners are evaluators who give the same score for every question. For the midpoints, we have 0 straightliners out of 95 evaluations. We also check if people straightline lower bounds of the credible intervals (0 straightliners) and upper bounds (0 straightliners).\nEvaluators might also give “degenerate” credible intervals, with the lower bound equal to the upper bound; uninformatively wide intervals, with the lower and upper bounds equal to 0% and 100%; or simply misspecified intervals, e.g. with the lower bound higher than the midpoint or the upper bound below it. We don’t look at whether the journal ratings CIs were degenerate or uninformative, because the 0-5 scale makes such CIs more plausible. Out of 851 confidence intervals, 6 were degenerate, 0 were uninformative and 10 were misspecified.\nOverall, these results suggest that most evaluators are providing thoughtful, differentiated responses to our quantitative questions."
  },
  {
    "objectID": "posts/uj-data-update-2025/index.html#inter-rater-reliability",
    "href": "posts/uj-data-update-2025/index.html#inter-rater-reliability",
    "title": "Unjournal data update: Deeper analysis with more evaluations",
    "section": "Inter-rater reliability",
    "text": "Inter-rater reliability\nWe have no ground truth of whether a given paper scores high or low on our 7 dimensions. But because we usually have multiple evaluations per paper, we can take an indirect route. If two evaluators’ scores are correlated with reality, they will also correlate with each other. The converse does not necessarily hold: evaluators’ scores might be correlated because they both have similar prejudices or both misinterpret the paper in the same way. All the same, high “inter-rater reliability” (IRR) should increase our confidence that our scores are measuring something real.\nIRR is complex. The basic form of most IRR statistics is\n\\[\n\\frac{p_a - p_e}{1 - p_e}\n\\]\nwhere \\(p_a\\) is the proportion of the time that two raters agree, and \\(p_e\\) is the amount of agreement you’d expect by chance if both raters are choosing independently.\nWhy not use \\(p_a\\) directly? Well, for example, suppose our raters pick an expected journal tier at random, from 0 to 5 inclusive. Clearly there’s no reliability: the data is just random noise. But one time in six, both raters will agree, simply by chance. So we need to adjust for the expected amount of agreement. To do this most measures use the marginal distributions of the ratings: in our example, a 1 in 6 chance of each number from 0 to 5, giving \\(p_e = 1/6\\). Krippendorff’s alpha is a widely accepted statistic that corrects for \\(p_e\\) and also defines “agreement” appropriately for different levels of measurement.\n\n\n\n\n\n\nChoosing a reliability statistic\n\n\n\nThere are many ways to measure inter-rater reliability. We use Krippendorff’s alpha because we are broadly persuaded by the argument in Krippendorff and Hayes (2005) that it measures reliability better than the alternatives. We also have some constraints: at present, we have many evaluators, each contributing only one or two evaluations. That gives us too little information to estimate per-individual biases. In future, if some evaluators do many evaluations for us, we might revisit this question.\nKrippendorff’s alpha can be calculated for different measurement levels: interval, ratio, and ordinal. For our percentile ratings (0-100%), we focus primarily on the interval scale, which treats equal-sized differences the same throughout the scale. We also report ratio (which has a meaningful zero) and ordinal (which only considers rank order) alphas for comparison.\nKrippendorff’s alpha handles missing data elegantly, so we use all available ratings regardless of how many evaluators rated each paper.\n\n\n\n\n\n\nTable 1:  Krippendorff’s alpha for each dimension (49 papers, 95 evaluations). Interval scale (primary), ratio scale, and ordinal scale shown. \n\nDimensionIntervalRatioOrdinalN\n\noverall0.5160.5070.37949\n\nmerits_journal0.3210.2840.24445\n\njournal_predict0.4140.3730.44044\n\nmethods0.5400.6030.40048\n\nreal_world0.4010.4560.23246\n\ngp_relevance0.3430.5050.15049\n\nlogic_comms0.3110.3750.17249\n\nadv_knowledge0.1910.1290.19548\n\nopen_sci0.0300.217−0.03449\n\n\n\n\n\nThe table shows inter-rater reliability for each dimension, with three different measurement assumptions. Interval alpha (our primary measure) treats equal-sized differences as equivalent throughout the scale. Ratio alpha additionally assumes a meaningful zero point, while ordinal alpha only considers whether one rating is higher than another.\nKrippendorff’s alpha values typically range from 0 (no agreement beyond chance) to 1 (perfect agreement). Values above 0.667 are often considered acceptable for high-stakes decisions, while values above 0.8 indicate strong reliability.\nOur interval alpha values suggest moderate agreement across dimensions. The quality dimensions (overall assessment, methods, etc.) generally show reliability in the 0.4-0.6 range, which is typical for subjective expert judgments. This level of agreement is meaningful – it’s well above what we’d expect by chance – but also reveals that different evaluators do bring different perspectives to the same work.\nThe journal tier questions show lower reliability to the quality dimensions. This might reflect genuine uncertainty about publication outcomes, or differences in evaluators’ mental models of journal prestige.\n\nCredible interval overlap\nBecause we have each rater’s 90% credible interval, we can also ask a slightly different question: do raters’ credible intervals capture other evaluators’ best estimates? That is, is rater 1’s midpoint estimate within rater 2’s credible interval, and vice versa?\nAccording to our guidelines, evaluators should construct intervals that capture their uncertainty about the true percentile for the paper, not their uncertainty about what another evaluator might say. If evaluators are well-calibrated and the “true percentile” exists, we’d still expect substantial disagreement because evaluators have access to different information and expertise.\n\n\n\n\nTable 2:  Credible interval overlap: Proportions of midpoints within other evaluators’ 90% credible intervals (49 papers, 95 evaluations) \n\nDimensionProportion within C.I.N comparisons\n\noverall64.5%76\n\nmerits_journal49.2%59\n\njournal_predict61.1%54\n\nmethods55.8%77\n\nreal_world55.2%67\n\ngp_relevance54.4%79\n\nlogic_comms53.1%81\n\nadv_knowledge50.6%79\n\nopen_sci39.2%74\n\nclaims42.3%26\n\n\n\n\n\nThe table above reveals an important pattern: typically only about half of our evaluators’ midpoints fall within their co-evaluator’s 90% credible interval. It’s important to interpret this finding carefully in light of our guidelines.\nEvaluators are asked to estimate the true underlying percentile for each paper, not to predict what other evaluators will say. Given that:\n\nDifferent evaluators have different expertise and perspectives\nSome papers may genuinely be at different percentiles for different subfields\nThe “true percentile” is itself a somewhat fuzzy concept\n\n…we should expect some degree of between-evaluator disagreement even with perfect calibration. The observed overlap rates of 40-60% suggest that evaluators’ intervals do capture meaningful uncertainty, though they may still be somewhat narrower than ideal for capturing all reasonable expert opinion.\nThis pattern is fairly consistent across dimensions, though there is some variation. The relationship between Krippendorff’s alpha (measuring between-evaluator agreement) and interval overlap provides useful information about both the consistency of our measures and the appropriate level of uncertainty to express.\n\n\nAnalysis by research area\nLet’s examine how agreement varies across different cause areas and research categories:\n\n\n\n\n\n\n\nTable 3:  Mean ratings and variability by research area (areas with 5+ ratings) \n\nAreaMean_OverallMean_Should be publishedSD_OverallSD_Should be publishedN_ratings_OverallN_ratings_Should be published\n\nDev. econ/gov. (LMICs)77.753.8713.290.6616.0015.00\n\nAnimal welfare, markets74.57   16.90   7.00\n\nEcon., welfare, misc.61.333.5625.040.949.007.00\n\nGlobal health (LMICs)79.303.9610.480.7523.0022.00\n\nInnovation & meta-science73.123.805.460.678.007.00\n\nEnvironment75.174.2816.660.4612.0012.00\n\nCatastrophic & X-risk, LT, forecasting67.003.3118.360.929.008.00\n\n\n\n\n\n\n\n\n\nTable 4:  Krippendorff’s alpha (interval) for ‘Overall’ rating by research area \n\nAreaAlphaN_papers\n\nEcon., welfare, misc.0.8775\n\nEnvironment0.6876\n\nAnimal welfare, markets0.3554\n\nCatastrophic & X-risk, LT, forecasting0.2164\n\nGlobal health (LMICs)−0.01412\n\nInnovation & meta-science−0.2655\n\nDev. econ/gov. (LMICs)−0.2799\n\n\n\n\n\nThese tables show how ratings and agreement vary across different research areas in our dataset. Some areas show higher average ratings, while others show more or less evaluator agreement. Sample sizes vary considerably, so these estimates should be interpreted cautiously for areas with few papers."
  },
  {
    "objectID": "posts/uj-data-update-2025/index.html#relatedness-and-dimensionality",
    "href": "posts/uj-data-update-2025/index.html#relatedness-and-dimensionality",
    "title": "Unjournal data update: Deeper analysis with more evaluations",
    "section": "Relatedness and dimensionality",
    "text": "Relatedness and dimensionality\nWe have 7 questions measuring paper quality, and 2 questions about journal tier. A natural question is: how related are these different measures? Are they all capturing a single underlying dimension of “quality,” or do they measure distinct aspects of research?\nWe can explore this using principal components analysis (PCA) of the 7 quality dimensions. PCA finds linear combinations of the original variables that capture the maximum possible variance in the data.\n\n\n\n\nVariance explained by principal components\nFirst, let’s look at how much of the total variation in ratings is explained by each principal component:\n\n\n\n\n\nFigure 1: Variance explained by each principal component (quality dimensions only)\n\n\n\n\nThe first principal component (PC1) explains 57.3% of the total variance in the 7 quality dimensions. The first two components together explain 77% of the variance, and the first three explain 90.7%.\nThis pattern suggests that while there is a dominant “general quality” dimension, there are also meaningful secondary dimensions capturing distinct aspects of research quality. The fact that PC1 doesn’t explain everything indicates that our seven questions are not simply redundant measures of the same thing.\n\n\nComponent loadings and interpretation\nThe table below shows the loadings for the first three components. Loadings indicate how much each original variable contributes to each component. Larger absolute values mean stronger contributions.\n\n\n\n\nTable 5:  Loadings of first 3 principal components on quality ratings \n\nQuestionComp.1Comp.2Comp.3\n\noverall0.3560.2870.241\n\nadv_knowledge0.486−0.1040.217\n\nmethods0.4280.5070.310\n\nlogic_comms0.2470.334−0.269\n\nreal_world0.480−0.338−0.357\n\ngp_relevance0.402−0.452−0.243\n\nopen_sci−0.0150.468−0.735\n\n\n\n\n\nComponent 1 has uniformly positive loadings across all dimensions, with the strongest weights on “overall”, “adv_knowledge”, “methods”, and “logic_comms”. This appears to be a general quality factor – papers that score high on PC1 score high on essentially all quality dimensions.\nComponent 2 shows an interesting contrast: it has strong positive loadings on “open_sci” and “real_world”, but near-zero or negative loadings on other dimensions. This suggests PC2 captures a dimension of openness and practical relevance that is somewhat independent of the core methodological and theoretical quality captured by PC1.\nComponent 3 shows yet another pattern, with “gp_relevance” (relevance to global priorities) having a distinctive loading pattern. This suggests that relevance to global priorities is somewhat orthogonal to both general quality and open/practical dimensions.\nThese results indicate that our seven quality dimensions are not simply redundant measures of a single underlying “quality” construct.\n\n\nCriteria redundancy: Within-rater correlations\nTo further examine whether our criteria are redundant, we can look at correlations within individual evaluators. If two criteria are highly correlated within raters, it suggests evaluators can’t or don’t distinguish between them, and one might be redundant.\n\n\n\n\n\n\n\n\nFigure 2: Average within-rater correlations between quality dimensions\n\n\n\n\n\n\n\n\nTable 6:  Top 10 criterion pairs by within-rater Pearson correlation \n\nCriterion1Criterion2PearsonSpearman\n\ngp_relevancereal_world0.6870.521\n\nadv_knowledgegp_relevance0.6840.528\n\nadv_knowledgereal_world0.6290.444\n\nmethodsoverall0.5500.574\n\nlogic_commsoverall0.5450.520\n\nadv_knowledgemethods0.5270.532\n\nlogic_commsmethods0.4650.569\n\nlogic_commsopen_sci0.4600.396\n\nadv_knowledgeoverall0.4560.339\n\ngp_relevancelogic_comms0.4510.300\n\n\n\n\n\nThese within-rater correlations show which criteria evaluators find most difficult to distinguish. High correlations (above 0.7-0.8) would suggest potential redundancy. The results show moderate correlations overall, suggesting that while our criteria are related (as expected for different aspects of quality), they capture meaningfully distinct dimensions that evaluators can and do distinguish between."
  },
  {
    "objectID": "posts/uj-data-update-2025/index.html#interactive-visualization-of-ratings",
    "href": "posts/uj-data-update-2025/index.html#interactive-visualization-of-ratings",
    "title": "Unjournal data update: Deeper analysis with more evaluations",
    "section": "Interactive visualization of ratings",
    "text": "Interactive visualization of ratings\n\n\n\n\n\nFigure 3: Interactive scatter plot of overall ratings by research area (hover for paper names)"
  },
  {
    "objectID": "posts/uj-data-update-2025/index.html#analysis-of-journal-tier-predictions",
    "href": "posts/uj-data-update-2025/index.html#analysis-of-journal-tier-predictions",
    "title": "Unjournal data update: Deeper analysis with more evaluations",
    "section": "Analysis of journal tier predictions",
    "text": "Analysis of journal tier predictions\nThe journal tier questions offer a unique opportunity because they have potential ground truth: we can eventually observe where papers are published. Let’s examine these ratings in more detail.\n\n\n\n\nDistribution of journal tier ratings\n\n\n\n\n\nFigure 4: Distribution of journal tier ratings\n\n\n\n\n\n\n\n\nTable 7:  Summary statistics for journal tier ratings \n\nDimensionMeanSDMedianMinMaxN\n\nWill be published3.880.754.002.005.0076\n\nShould be published3.850.794.001.105.0082\n\n\n\n\n\nOn average, evaluators predict papers will be published at tier 3.88, but believe they merit publication at tier 3.85.\n\n\nStatistical test for merit vs. prediction difference\nTo test whether this difference is statistically meaningful (not just noise), we can conduct a paired t-test on matched merit and prediction ratings:\n\n\n\nA paired t-test comparing merit and prediction ratings shows:\n\nMean difference: -0.033 tiers (merit - prediction)\nt(74) = -0.48, p = 0.633\nCohen’s d = -0.06 (negligible effect size)\n95% CI for difference: [-0.17, 0.11]\n\nThe difference between merit and prediction ratings is not statistically significant.\n\n\n\n\n\nFigure 5: Comparison of merit vs. prediction ratings for journal tiers\n\n\n\n\nMost points lie above the diagonal line, confirming that evaluators generally believe papers merit higher tier publication than they predict they will receive.\n\n\nRelationship to quality dimensions\nFinally, we can examine how the journal tier ratings relate to the seven quality dimensions:\n\n\n\n\n\nFigure 6: Correlations between quality dimensions and journal tier ratings\n\n\n\n\nThis shows which quality dimensions are most strongly associated with evaluators’ judgments about journal tier. Typically, “overall” assessment and “adv_knowledge” show the strongest correlations with both merit and prediction ratings.\n“open_sci” tends to show weaker correlations with journal tier predictions, which may reflect a realistic assessment that traditional journals don’t always reward open science practices as strongly as other quality dimensions."
  },
  {
    "objectID": "posts/uj-data-update-2025/index.html#conclusions",
    "href": "posts/uj-data-update-2025/index.html#conclusions",
    "title": "Unjournal data update: Deeper analysis with more evaluations",
    "section": "Conclusions",
    "text": "Conclusions\nThis analysis of 49 papers and 95 evaluations reveals several key patterns:\n\nData quality: Most evaluators provide thoughtful, differentiated ratings with few signs of straightlining or obviously problematic confidence intervals.\nInter-rater reliability: We see moderate agreement between evaluators (Krippendorff’s interval alpha typically 0.4-0.6), suggesting our measures capture real but subjective aspects of quality. Credible interval overlap analysis suggests evaluators’ intervals capture meaningful uncertainty about true underlying quality.\nDimensionality: The seven quality dimensions are not redundant. While there is a strong general quality factor (explaining ~57% of variance), we also see meaningful secondary dimensions related to open science/practical relevance and global priorities relevance. Within-rater correlation analysis confirms that evaluators distinguish meaningfully between our different criteria.\nJournal tiers: Evaluators consistently judge papers as meriting higher-tier publication than they predict they will receive. This difference is statistically significant (p < 0.001) with a small to medium effect size. As we observe actual publication outcomes, we’ll be able to assess the accuracy of these predictions.\n\nAs our dataset continues to grow, we will be able to conduct more sophisticated analyses, track changes over time, and eventually validate our predictions against actual publication outcomes."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Unjournal data blog",
    "section": "",
    "text": "This site hosts the Unjournal data blog. Our main web site, unjournal.org, and our knowledge base explain and present our vision, procedures and progress. The ‘output’ evaluations, including feedback and discussion, can be found on our PubPub page, and are indexed in scholarly archives.\nWe also have an interactive dashboard.\nThis site and the dashboard present data and analysis on The Unjournal’s pipeline and evaluation output. We use them to:\n\nKeep track of what we are covering, when and how.\nPresent the quantitative evaluations in useful ways.\nBenchmark, check, and aggregate the expert judgment of our evaluators as reflected in their ratings and predictions.\n\nWe may expand this analysis further in the future, e.g., to include\n\nFurther analysis of the relevant research contexts (e.g., ‘how many papers are coming out by field’)\nConnections to replications and prediction initiatives.\nComparing and benchmarking our evaluations against ‘traditional publication outcomes’ for the evaluated work, such as journal tiers and citations.\n\nThis resource aims to be:\n\nDynamic: Regularly updated to reflect our progress\nTransparent and replicable: sharing data and code to permit checking and ‘forked’ analyses\nInteractive: presenting a dashboard-style interface, allowing readers to choose their analyses of interest.\n\n20 May 2024: David Reinstein, Julia Bottesini, and David Hugh-Jones have done most of the analysis here and in the accompanying dashboards.\n\nData dashboard\n::: column-body-outset"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog Posts",
    "section": "",
    "text": "data\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 15, 2025\n\n\nOrchestrated by David Reinstein with Claude Code assistance*\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 15, 2025\n\n\nOrchestrated by David Reinstein with Claude Code assistance*\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nJul 25, 2024\n\n\nDavid Hugh-Jones\n\n\n\n\n\n\nNo matching items"
  }
]